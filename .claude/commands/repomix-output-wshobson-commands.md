This file is a merged representation of a subset of the codebase, containing files not matching ignore patterns, combined into a single document by Repomix.
The content has been processed where content has been compressed (code blocks are separated by ‚ãÆ---- delimiter), security check has been disabled.

# File Summary

## Purpose

This file contains a packed representation of a subset of the repository's contents that is considered the most important context.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

## File Format

The content is organized as follows:

1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  a. A header with the file path (## File: path/to/file)
  b. The full contents of the file in a code block

## Usage Guidelines

- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

## Notes

- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching these patterns are excluded: .github
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Content has been compressed - code blocks are separated by ‚ãÆ---- delimiter
- Security check has been disabled - content may contain sensitive information
- Files are sorted by Git change count (files with more changes are at the bottom)

# Directory Structure

```
tools/
  accessibility-audit.md
  ai-assistant.md
  ai-review.md
  api-mock.md
  api-scaffold.md
  code-explain.md
  code-migrate.md
  compliance-check.md
  config-validate.md
  context-restore.md
  context-save.md
  cost-optimize.md
  data-pipeline.md
  data-validation.md
  db-migrate.md
  debug-trace.md
  deploy-checklist.md
  deps-audit.md
  deps-upgrade.md
  doc-generate.md
  docker-optimize.md
  error-analysis.md
  error-trace.md
  issue.md
  k8s-manifest.md
  langchain-agent.md
  monitor-setup.md
  multi-agent-optimize.md
  multi-agent-review.md
  onboard.md
  pr-enhance.md
  prompt-optimize.md
  refactor-clean.md
  security-scan.md
  slo-implement.md
  smart-debug.md
  standup-notes.md
  tech-debt.md
  test-harness.md
workflows/
  data-driven-feature.md
  feature-development.md
  full-review.md
  full-stack-feature.md
  git-workflow.md
  improve-agent.md
  incident-response.md
  legacy-modernize.md
  ml-pipeline.md
  multi-platform.md
  performance-optimization.md
  security-hardening.md
  smart-fix.md
  workflow-automate.md
.gitignore
LICENSE
README.md
```

# Files

## File: tools/accessibility-audit.md

````markdown
# Accessibility Audit and Testing

You are an accessibility expert specializing in WCAG compliance, inclusive design, and assistive technology compatibility. Conduct comprehensive audits, identify barriers, provide remediation guidance, and ensure digital products are accessible to all users.

## Context
The user needs to audit and improve accessibility to ensure compliance with WCAG standards and provide an inclusive experience for users with disabilities. Focus on automated testing, manual verification, remediation strategies, and establishing ongoing accessibility practices.

## Requirements
$ARGUMENTS

## Instructions

### 1. Automated Accessibility Testing

Implement comprehensive automated testing:

**Accessibility Test Suite**
```javascript
// accessibility-test-suite.js
const { AxePuppeteer } = require('@axe-core/puppeteer');
const puppeteer = require('puppeteer');
const pa11y = require('pa11y');
const htmlValidator = require('html-validator');

class AccessibilityAuditor {
    constructor(options = {}) {
        this.wcagLevel = options.wcagLevel || 'AA';
        this.viewport = options.viewport || { width: 1920, height: 1080 };
        this.results = [];
    }
    
    async runFullAudit(url) {
        console.log(`üîç Starting accessibility audit for ${url}`);
        
        const results = {
            url,
            timestamp: new Date().toISOString(),
            summary: {},
            violations: [],
            passes: [],
            incomplete: [],
            inapplicable: []
        };
        
        // Run multiple testing tools
        const [axeResults, pa11yResults, htmlResults] = await Promise.all([
            this.runAxeCore(url),
            this.runPa11y(url),
            this.validateHTML(url)
        ]);
        
        // Combine results
        results.violations = this.mergeViolations([
            ...axeResults.violations,
            ...pa11yResults.violations
        ]);
        
        results.htmlErrors = htmlResults.errors;
        results.summary = this.generateSummary(results);
        
        return results;
    }
    
    async runAxeCore(url) {
        const browser = await puppeteer.launch();
        const page = await browser.newPage();
        await page.setViewport(this.viewport);
        await page.goto(url, { waitUntil: 'networkidle2' });
        
        // Configure axe
        const axeBuilder = new AxePuppeteer(page)
            .withTags(['wcag2a', 'wcag2aa', 'wcag21a', 'wcag21aa'])
            .disableRules(['color-contrast']) // Will test separately
            .exclude('.no-a11y-check');
        
        const results = await axeBuilder.analyze();
        await browser.close();
        
        return this.formatAxeResults(results);
    }
    
    async runPa11y(url) {
        const results = await pa11y(url, {
            standard: 'WCAG2AA',
            runners: ['axe', 'htmlcs'],
            includeWarnings: true,
            viewport: this.viewport,
            actions: [
                'wait for element .main-content to be visible'
            ]
        });
        
        return this.formatPa11yResults(results);
    }
    
    formatAxeResults(results) {
        return {
            violations: results.violations.map(violation => ({
                id: violation.id,
                impact: violation.impact,
                description: violation.description,
                help: violation.help,
                helpUrl: violation.helpUrl,
                nodes: violation.nodes.map(node => ({
                    html: node.html,
                    target: node.target,
                    failureSummary: node.failureSummary
                }))
            })),
            passes: results.passes.length,
            incomplete: results.incomplete.length
        };
    }
    
    generateSummary(results) {
        const violationsByImpact = {
            critical: 0,
            serious: 0,
            moderate: 0,
            minor: 0
        };
        
        results.violations.forEach(violation => {
            if (violationsByImpact.hasOwnProperty(violation.impact)) {
                violationsByImpact[violation.impact]++;
            }
        });
        
        return {
            totalViolations: results.violations.length,
            violationsByImpact,
            score: this.calculateAccessibilityScore(results),
            wcagCompliance: this.assessWCAGCompliance(results)
        };
    }
    
    calculateAccessibilityScore(results) {
        // Simple scoring algorithm
        const weights = {
            critical: 10,
            serious: 5,
            moderate: 2,
            minor: 1
        };
        
        let totalWeight = 0;
        results.violations.forEach(violation => {
            totalWeight += weights[violation.impact] || 0;
        });
        
        // Score from 0-100
        return Math.max(0, 100 - totalWeight);
    }
}

// Component-level testing
import { render } from '@testing-library/react';
import { axe, toHaveNoViolations } from 'jest-axe';

expect.extend(toHaveNoViolations);

describe('Accessibility Tests', () => {
    it('should have no accessibility violations', async () => {
        const { container } = render(<MyComponent />);
        const results = await axe(container);
        expect(results).toHaveNoViolations();
    });
    
    it('should have proper ARIA labels', async () => {
        const { container } = render(<Form />);
        const results = await axe(container, {
            rules: {
                'label': { enabled: true },
                'aria-valid-attr': { enabled: true },
                'aria-roles': { enabled: true }
            }
        });
        expect(results).toHaveNoViolations();
    });
});
```

### 2. Color Contrast Analysis

Implement comprehensive color contrast testing:

**Color Contrast Checker**
```javascript
// color-contrast-analyzer.js
class ColorContrastAnalyzer {
    constructor() {
        this.wcagLevels = {
            'AA': { normal: 4.5, large: 3 },
            'AAA': { normal: 7, large: 4.5 }
        };
    }
    
    async analyzePageContrast(page) {
        const contrastIssues = [];
        
        // Extract all text elements with their styles
        const elements = await page.evaluate(() => {
            const allElements = document.querySelectorAll('*');
            const textElements = [];
            
            allElements.forEach(el => {
                if (el.innerText && el.innerText.trim()) {
                    const styles = window.getComputedStyle(el);
                    const rect = el.getBoundingClientRect();
                    
                    textElements.push({
                        text: el.innerText.trim(),
                        selector: el.tagName.toLowerCase() + 
                                 (el.id ? `#${el.id}` : '') +
                                 (el.className ? `.${el.className.split(' ').join('.')}` : ''),
                        color: styles.color,
                        backgroundColor: styles.backgroundColor,
                        fontSize: parseFloat(styles.fontSize),
                        fontWeight: styles.fontWeight,
                        position: { x: rect.x, y: rect.y },
                        isVisible: rect.width > 0 && rect.height > 0
                    });
                }
            });
            
            return textElements;
        });
        
        // Check contrast for each element
        for (const element of elements) {
            if (!element.isVisible) continue;
            
            const contrast = this.calculateContrast(
                element.color,
                element.backgroundColor
            );
            
            const isLargeText = this.isLargeText(
                element.fontSize,
                element.fontWeight
            );
            
            const requiredContrast = isLargeText ? 
                this.wcagLevels.AA.large : 
                this.wcagLevels.AA.normal;
            
            if (contrast < requiredContrast) {
                contrastIssues.push({
                    selector: element.selector,
                    text: element.text.substring(0, 50) + '...',
                    currentContrast: contrast.toFixed(2),
                    requiredContrast,
                    foreground: element.color,
                    background: element.backgroundColor,
                    recommendation: this.generateColorRecommendation(
                        element.color,
                        element.backgroundColor,
                        requiredContrast
                    )
                });
            }
        }
        
        return contrastIssues;
    }
    
    calculateContrast(foreground, background) {
        const rgb1 = this.parseColor(foreground);
        const rgb2 = this.parseColor(background);
        
        const l1 = this.relativeLuminance(rgb1);
        const l2 = this.relativeLuminance(rgb2);
        
        const lighter = Math.max(l1, l2);
        const darker = Math.min(l1, l2);
        
        return (lighter + 0.05) / (darker + 0.05);
    }
    
    relativeLuminance(rgb) {
        const [r, g, b] = rgb.map(val => {
            val = val / 255;
            return val <= 0.03928 ? 
                val / 12.92 : 
                Math.pow((val + 0.055) / 1.055, 2.4);
        });
        
        return 0.2126 * r + 0.7152 * g + 0.0722 * b;
    }
    
    generateColorRecommendation(foreground, background, targetRatio) {
        // Suggest adjusted colors that meet contrast requirements
        const suggestions = [];
        
        // Try darkening foreground
        const darkerFg = this.adjustColorForContrast(
            foreground,
            background,
            targetRatio,
            'darken'
        );
        if (darkerFg) {
            suggestions.push({
                type: 'darken-foreground',
                color: darkerFg,
                contrast: this.calculateContrast(darkerFg, background)
            });
        }
        
        // Try lightening background
        const lighterBg = this.adjustColorForContrast(
            background,
            foreground,
            targetRatio,
            'lighten'
        );
        if (lighterBg) {
            suggestions.push({
                type: 'lighten-background',
                color: lighterBg,
                contrast: this.calculateContrast(foreground, lighterBg)
            });
        }
        
        return suggestions;
    }
}

// CSS for high contrast mode
const highContrastStyles = `
@media (prefers-contrast: high) {
    :root {
        --text-primary: #000;
        --text-secondary: #333;
        --bg-primary: #fff;
        --bg-secondary: #f0f0f0;
        --border-color: #000;
    }
    
    * {
        border-color: var(--border-color) !important;
    }
    
    a {
        text-decoration: underline !important;
        text-decoration-thickness: 2px !important;
    }
    
    button, input, select, textarea {
        border: 2px solid var(--border-color) !important;
    }
}

@media (prefers-color-scheme: dark) and (prefers-contrast: high) {
    :root {
        --text-primary: #fff;
        --text-secondary: #ccc;
        --bg-primary: #000;
        --bg-secondary: #1a1a1a;
        --border-color: #fff;
    }
}
`;
```

### 3. Keyboard Navigation Testing

Test keyboard accessibility:

**Keyboard Navigation Tester**
```javascript
// keyboard-navigation-test.js
class KeyboardNavigationTester {
    async testKeyboardNavigation(page) {
        const results = {
            focusableElements: [],
            tabOrder: [],
            keyboardTraps: [],
            missingFocusIndicators: [],
            inaccessibleInteractive: []
        };
        
        // Get all focusable elements
        const focusableElements = await page.evaluate(() => {
            const selector = 'a[href], button, input, select, textarea, [tabindex]:not([tabindex="-1"])';
            const elements = document.querySelectorAll(selector);
            
            return Array.from(elements).map((el, index) => ({
                tagName: el.tagName.toLowerCase(),
                type: el.type || null,
                text: el.innerText || el.value || el.placeholder || '',
                tabIndex: el.tabIndex,
                hasAriaLabel: !!el.getAttribute('aria-label'),
                hasAriaLabelledBy: !!el.getAttribute('aria-labelledby'),
                selector: el.tagName.toLowerCase() + 
                         (el.id ? `#${el.id}` : '') +
                         (el.className ? `.${el.className.split(' ').join('.')}` : '')
            }));
        });
        
        results.focusableElements = focusableElements;
        
        // Test tab order
        for (let i = 0; i < focusableElements.length; i++) {
            await page.keyboard.press('Tab');
            
            const focusedElement = await page.evaluate(() => {
                const el = document.activeElement;
                return {
                    tagName: el.tagName.toLowerCase(),
                    selector: el.tagName.toLowerCase() + 
                             (el.id ? `#${el.id}` : '') +
                             (el.className ? `.${el.className.split(' ').join('.')}` : ''),
                    hasFocusIndicator: window.getComputedStyle(el).outline !== 'none'
                };
            });
            
            results.tabOrder.push(focusedElement);
            
            if (!focusedElement.hasFocusIndicator) {
                results.missingFocusIndicators.push(focusedElement);
            }
        }
        
        // Test for keyboard traps
        await this.detectKeyboardTraps(page, results);
        
        // Test interactive elements
        await this.testInteractiveElements(page, results);
        
        return results;
    }
    
    async detectKeyboardTraps(page, results) {
        // Test common trap patterns
        const trapSelectors = [
            'div[role="dialog"]',
            '.modal',
            '.dropdown-menu',
            '[role="menu"]'
        ];
        
        for (const selector of trapSelectors) {
            const elements = await page.$$(selector);
            
            for (const element of elements) {
                const canEscape = await this.testEscapeability(page, element);
                if (!canEscape) {
                    results.keyboardTraps.push({
                        selector,
                        issue: 'Cannot escape with keyboard'
                    });
                }
            }
        }
    }
    
    async testInteractiveElements(page, results) {
        // Find elements with click handlers but no keyboard support
        const clickableElements = await page.evaluate(() => {
            const elements = document.querySelectorAll('*');
            const clickable = [];
            
            elements.forEach(el => {
                const hasClickHandler = 
                    el.onclick || 
                    el.getAttribute('onclick') ||
                    (window.getEventListeners && 
                     window.getEventListeners(el).click);
                
                const isNotNativelyClickable = 
                    !['a', 'button', 'input', 'select', 'textarea'].includes(
                        el.tagName.toLowerCase()
                    );
                
                if (hasClickHandler && isNotNativelyClickable) {
                    const hasKeyboardSupport = 
                        el.getAttribute('tabindex') !== null ||
                        el.getAttribute('role') === 'button' ||
                        el.onkeydown || 
                        el.onkeyup;
                    
                    if (!hasKeyboardSupport) {
                        clickable.push({
                            selector: el.tagName.toLowerCase() + 
                                     (el.id ? `#${el.id}` : ''),
                            issue: 'Click handler without keyboard support'
                        });
                    }
                }
            });
            
            return clickable;
        });
        
        results.inaccessibleInteractive = clickableElements;
    }
}

// Keyboard navigation enhancement
function enhanceKeyboardNavigation() {
    // Skip to main content link
    const skipLink = document.createElement('a');
    skipLink.href = '#main-content';
    skipLink.className = 'skip-link';
    skipLink.textContent = 'Skip to main content';
    document.body.insertBefore(skipLink, document.body.firstChild);
    
    // Add keyboard event handlers
    document.addEventListener('keydown', (e) => {
        // Escape key closes modals
        if (e.key === 'Escape') {
            const modal = document.querySelector('.modal.open');
            if (modal) {
                closeModal(modal);
            }
        }
        
        // Arrow key navigation for menus
        if (e.key.startsWith('Arrow')) {
            const menu = document.activeElement.closest('[role="menu"]');
            if (menu) {
                navigateMenu(menu, e.key);
                e.preventDefault();
            }
        }
    });
    
    // Ensure all interactive elements are keyboard accessible
    document.querySelectorAll('[onclick]').forEach(el => {
        if (!el.hasAttribute('tabindex') && 
            !['a', 'button', 'input'].includes(el.tagName.toLowerCase())) {
            el.setAttribute('tabindex', '0');
            el.setAttribute('role', 'button');
            
            el.addEventListener('keydown', (e) => {
                if (e.key === 'Enter' || e.key === ' ') {
                    el.click();
                    e.preventDefault();
                }
            });
        }
    });
}
```

### 4. Screen Reader Testing

Implement screen reader compatibility testing:

**Screen Reader Test Suite**
```javascript
// screen-reader-test.js
class ScreenReaderTester {
    async testScreenReaderCompatibility(page) {
        const results = {
            landmarks: await this.testLandmarks(page),
            headings: await this.testHeadingStructure(page),
            images: await this.testImageAccessibility(page),
            forms: await this.testFormAccessibility(page),
            tables: await this.testTableAccessibility(page),
            liveRegions: await this.testLiveRegions(page),
            semantics: await this.testSemanticHTML(page)
        };
        
        return results;
    }
    
    async testLandmarks(page) {
        const landmarks = await page.evaluate(() => {
            const landmarkRoles = [
                'banner', 'navigation', 'main', 'complementary', 
                'contentinfo', 'search', 'form', 'region'
            ];
            
            const found = [];
            
            // Check ARIA landmarks
            landmarkRoles.forEach(role => {
                const elements = document.querySelectorAll(`[role="${role}"]`);
                elements.forEach(el => {
                    found.push({
                        type: role,
                        hasLabel: !!(el.getAttribute('aria-label') || 
                                   el.getAttribute('aria-labelledby')),
                        selector: this.getSelector(el)
                    });
                });
            });
            
            // Check HTML5 landmarks
            const html5Landmarks = {
                'header': 'banner',
                'nav': 'navigation',
                'main': 'main',
                'aside': 'complementary',
                'footer': 'contentinfo'
            };
            
            Object.entries(html5Landmarks).forEach(([tag, role]) => {
                const elements = document.querySelectorAll(tag);
                elements.forEach(el => {
                    if (!el.closest('[role]')) {
                        found.push({
                            type: role,
                            hasLabel: !!(el.getAttribute('aria-label') || 
                                       el.getAttribute('aria-labelledby')),
                            selector: tag
                        });
                    }
                });
            });
            
            return found;
        });
        
        return {
            landmarks,
            issues: this.analyzeLandmarkIssues(landmarks)
        };
    }
    
    async testHeadingStructure(page) {
        const headings = await page.evaluate(() => {
            const allHeadings = document.querySelectorAll('h1, h2, h3, h4, h5, h6');
            const structure = [];
            
            allHeadings.forEach(heading => {
                structure.push({
                    level: parseInt(heading.tagName[1]),
                    text: heading.textContent.trim(),
                    hasAriaLevel: !!heading.getAttribute('aria-level'),
                    isEmpty: !heading.textContent.trim()
                });
            });
            
            return structure;
        });
        
        // Analyze heading structure
        const issues = [];
        let previousLevel = 0;
        
        headings.forEach((heading, index) => {
            // Check for skipped levels
            if (heading.level > previousLevel + 1 && previousLevel !== 0) {
                issues.push({
                    type: 'skipped-level',
                    message: `Heading level ${heading.level} skips from level ${previousLevel}`,
                    heading: heading.text
                });
            }
            
            // Check for empty headings
            if (heading.isEmpty) {
                issues.push({
                    type: 'empty-heading',
                    message: `Empty h${heading.level} element`,
                    index
                });
            }
            
            previousLevel = heading.level;
        });
        
        // Check for missing h1
        if (!headings.some(h => h.level === 1)) {
            issues.push({
                type: 'missing-h1',
                message: 'Page is missing an h1 element'
            });
        }
        
        return { headings, issues };
    }
    
    async testFormAccessibility(page) {
        const forms = await page.evaluate(() => {
            const formElements = document.querySelectorAll('form');
            const results = [];
            
            formElements.forEach(form => {
                const inputs = form.querySelectorAll('input, textarea, select');
                const formData = {
                    hasFieldset: !!form.querySelector('fieldset'),
                    hasLegend: !!form.querySelector('legend'),
                    fields: []
                };
                
                inputs.forEach(input => {
                    const field = {
                        type: input.type || input.tagName.toLowerCase(),
                        name: input.name,
                        id: input.id,
                        hasLabel: false,
                        hasAriaLabel: !!input.getAttribute('aria-label'),
                        hasAriaDescribedBy: !!input.getAttribute('aria-describedby'),
                        hasPlaceholder: !!input.placeholder,
                        required: input.required,
                        hasErrorMessage: false
                    };
                    
                    // Check for associated label
                    if (input.id) {
                        field.hasLabel = !!document.querySelector(`label[for="${input.id}"]`);
                    }
                    
                    // Check if wrapped in label
                    if (!field.hasLabel) {
                        field.hasLabel = !!input.closest('label');
                    }
                    
                    formData.fields.push(field);
                });
                
                results.push(formData);
            });
            
            return results;
        });
        
        // Analyze form accessibility
        const issues = [];
        forms.forEach((form, formIndex) => {
            form.fields.forEach((field, fieldIndex) => {
                if (!field.hasLabel && !field.hasAriaLabel) {
                    issues.push({
                        type: 'missing-label',
                        form: formIndex,
                        field: fieldIndex,
                        fieldType: field.type
                    });
                }
                
                if (field.required && !field.hasErrorMessage) {
                    issues.push({
                        type: 'missing-error-message',
                        form: formIndex,
                        field: fieldIndex,
                        fieldType: field.type
                    });
                }
            });
        });
        
        return { forms, issues };
    }
}

// ARIA implementation patterns
const ariaPatterns = {
    // Accessible modal
    modal: `
<div role="dialog" 
     aria-labelledby="modal-title" 
     aria-describedby="modal-description"
     aria-modal="true">
    <h2 id="modal-title">Modal Title</h2>
    <p id="modal-description">Modal description text</p>
    <button aria-label="Close modal">√ó</button>
</div>
    `,
    
    // Accessible tabs
    tabs: `
<div role="tablist" aria-label="Section navigation">
    <button role="tab" 
            aria-selected="true" 
            aria-controls="panel-1" 
            id="tab-1">
        Tab 1
    </button>
    <button role="tab" 
            aria-selected="false" 
            aria-controls="panel-2" 
            id="tab-2">
        Tab 2
    </button>
</div>
<div role="tabpanel" 
     id="panel-1" 
     aria-labelledby="tab-1">
    Panel 1 content
</div>
    `,
    
    // Accessible form
    form: `
<form>
    <fieldset>
        <legend>User Information</legend>
        
        <label for="name">
            Name
            <span aria-label="required">*</span>
        </label>
        <input id="name" 
               type="text" 
               required 
               aria-required="true"
               aria-describedby="name-error">
        <span id="name-error" 
              role="alert" 
              aria-live="polite"></span>
    </fieldset>
</form>
    `
};
```

### 5. Manual Testing Checklist

Create comprehensive manual testing guides:

**Manual Accessibility Checklist**
```markdown
## Manual Accessibility Testing Checklist

### 1. Keyboard Navigation
- [ ] Can access all interactive elements using Tab key
- [ ] Can activate buttons with Enter/Space
- [ ] Can navigate dropdowns with arrow keys
- [ ] Can escape modals with Esc key
- [ ] Focus indicator is always visible
- [ ] No keyboard traps exist
- [ ] Skip links work correctly
- [ ] Tab order is logical

### 2. Screen Reader Testing
- [ ] Page title is descriptive
- [ ] Headings create logical outline
- [ ] All images have appropriate alt text
- [ ] Form fields have labels
- [ ] Error messages are announced
- [ ] Dynamic content updates are announced
- [ ] Tables have proper headers
- [ ] Lists use semantic markup

### 3. Visual Testing
- [ ] Text can be resized to 200% without loss of functionality
- [ ] Color is not the only means of conveying information
- [ ] Focus indicators have sufficient contrast
- [ ] Content reflows at 320px width
- [ ] No horizontal scrolling at 320px
- [ ] Animations can be paused/stopped
- [ ] No content flashes more than 3 times per second

### 4. Cognitive Accessibility
- [ ] Instructions are clear and simple
- [ ] Error messages are helpful
- [ ] Forms can be completed without time limits
- [ ] Content is organized logically
- [ ] Navigation is consistent
- [ ] Important actions are reversible
- [ ] Help is available when needed

### 5. Mobile Accessibility
- [ ] Touch targets are at least 44x44 pixels
- [ ] Gestures have alternatives
- [ ] Device orientation works in both modes
- [ ] Virtual keyboard doesn't obscure inputs
- [ ] Pinch zoom is not disabled
```

### 6. Remediation Strategies

Provide fixes for common issues:

**Accessibility Fixes**
```javascript
// accessibility-fixes.js
class AccessibilityRemediator {
    applyFixes(violations) {
        violations.forEach(violation => {
            switch(violation.id) {
                case 'image-alt':
                    this.fixMissingAltText(violation.nodes);
                    break;
                case 'label':
                    this.fixMissingLabels(violation.nodes);
                    break;
                case 'color-contrast':
                    this.fixColorContrast(violation.nodes);
                    break;
                case 'heading-order':
                    this.fixHeadingOrder(violation.nodes);
                    break;
                case 'landmark-one-main':
                    this.fixLandmarks(violation.nodes);
                    break;
                default:
                    console.warn(`No automatic fix for: ${violation.id}`);
            }
        });
    }
    
    fixMissingAltText(nodes) {
        nodes.forEach(node => {
            const element = document.querySelector(node.target[0]);
            if (element && element.tagName === 'IMG') {
                // Decorative image
                if (this.isDecorativeImage(element)) {
                    element.setAttribute('alt', '');
                    element.setAttribute('role', 'presentation');
                } else {
                    // Generate meaningful alt text
                    const altText = this.generateAltText(element);
                    element.setAttribute('alt', altText);
                }
            }
        });
    }
    
    fixMissingLabels(nodes) {
        nodes.forEach(node => {
            const element = document.querySelector(node.target[0]);
            if (element && ['INPUT', 'SELECT', 'TEXTAREA'].includes(element.tagName)) {
                // Try to find nearby text
                const nearbyText = this.findNearbyLabelText(element);
                if (nearbyText) {
                    const label = document.createElement('label');
                    label.textContent = nearbyText;
                    label.setAttribute('for', element.id || this.generateId());
                    element.id = element.id || label.getAttribute('for');
                    element.parentNode.insertBefore(label, element);
                } else {
                    // Use placeholder as aria-label
                    if (element.placeholder) {
                        element.setAttribute('aria-label', element.placeholder);
                    }
                }
            }
        });
    }
    
    fixColorContrast(nodes) {
        nodes.forEach(node => {
            const element = document.querySelector(node.target[0]);
            if (element) {
                const styles = window.getComputedStyle(element);
                const foreground = styles.color;
                const background = this.getBackgroundColor(element);
                
                // Apply high contrast fixes
                element.style.setProperty('color', 'var(--high-contrast-text, #000)', 'important');
                element.style.setProperty('background-color', 'var(--high-contrast-bg, #fff)', 'important');
            }
        });
    }
    
    generateAltText(img) {
        // Use various strategies to generate alt text
        const strategies = [
            () => img.title,
            () => img.getAttribute('data-alt'),
            () => this.extractFromFilename(img.src),
            () => this.extractFromSurroundingText(img),
            () => 'Image'
        ];
        
        for (const strategy of strategies) {
            const text = strategy();
            if (text && text.trim()) {
                return text.trim();
            }
        }
        
        return 'Image';
    }
}

// React accessibility components
import React from 'react';

// Accessible button component
const AccessibleButton = ({ 
    children, 
    onClick, 
    ariaLabel, 
    ariaPressed,
    disabled,
    ...props 
}) => {
    return (
        <button
            onClick={onClick}
            aria-label={ariaLabel}
            aria-pressed={ariaPressed}
            disabled={disabled}
            className="accessible-button"
            {...props}
        >
            {children}
        </button>
    );
};

// Live region for announcements
const LiveRegion = ({ message, politeness = 'polite' }) => {
    return (
        <div
            role="status"
            aria-live={politeness}
            aria-atomic="true"
            className="sr-only"
        >
            {message}
        </div>
    );
};

// Skip navigation component
const SkipNav = () => {
    return (
        <a href="#main-content" className="skip-nav">
            Skip to main content
        </a>
    );
};
```

### 7. CI/CD Integration

Integrate accessibility testing into pipelines:

**CI/CD Accessibility Pipeline**
```yaml
# .github/workflows/accessibility.yml
name: Accessibility Tests

on: [push, pull_request]

jobs:
  a11y-tests:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Setup Node.js
      uses: actions/setup-node@v3
      with:
        node-version: '18'
    
    - name: Install dependencies
      run: npm ci
    
    - name: Build application
      run: npm run build
    
    - name: Start server
      run: |
        npm start &
        npx wait-on http://localhost:3000
    
    - name: Run axe accessibility tests
      run: npm run test:a11y
    
    - name: Run pa11y tests
      run: |
        npx pa11y http://localhost:3000 \
          --reporter cli \
          --standard WCAG2AA \
          --threshold 0
    
    - name: Run Lighthouse CI
      run: |
        npm install -g @lhci/cli
        lhci autorun --config=lighthouserc.json
    
    - name: Upload accessibility report
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: accessibility-report
        path: |
          a11y-report.html
          lighthouse-report.html
```

**Pre-commit Hook**
```bash
#!/bin/bash
# .husky/pre-commit

# Run accessibility tests on changed components
CHANGED_FILES=$(git diff --cached --name-only --diff-filter=ACM | grep -E '\.(jsx?|tsx?)$')

if [ -n "$CHANGED_FILES" ]; then
    echo "Running accessibility tests on changed files..."
    npm run test:a11y -- $CHANGED_FILES
    
    if [ $? -ne 0 ]; then
        echo "‚ùå Accessibility tests failed. Please fix issues before committing."
        exit 1
    fi
fi
```

### 8. Accessibility Reporting

Generate comprehensive reports:

**Report Generator**
```javascript
// accessibility-report-generator.js
class AccessibilityReportGenerator {
    generateHTMLReport(auditResults) {
        const html = `
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Accessibility Audit Report</title>
    <style>
        body { font-family: Arial, sans-serif; margin: 20px; }
        .summary { background: #f0f0f0; padding: 20px; border-radius: 8px; }
        .score { font-size: 48px; font-weight: bold; }
        .score.good { color: #0f0; }
        .score.warning { color: #fa0; }
        .score.poor { color: #f00; }
        .violation { margin: 20px 0; padding: 15px; border: 1px solid #ddd; }
        .violation.critical { border-color: #f00; background: #fee; }
        .violation.serious { border-color: #fa0; background: #ffe; }
        .code { background: #f5f5f5; padding: 10px; font-family: monospace; }
        table { border-collapse: collapse; width: 100%; }
        th, td { border: 1px solid #ddd; padding: 8px; text-align: left; }
    </style>
</head>
<body>
    <h1>Accessibility Audit Report</h1>
    <p>Generated: ${new Date().toLocaleString()}</p>
    
    <div class="summary">
        <h2>Summary</h2>
        <div class="score ${this.getScoreClass(auditResults.summary.score)}">
            Score: ${auditResults.summary.score}/100
        </div>
        <p>WCAG ${auditResults.summary.wcagCompliance} Compliance</p>
        
        <h3>Violations by Impact</h3>
        <table>
            <tr>
                <th>Impact</th>
                <th>Count</th>
            </tr>
            ${Object.entries(auditResults.summary.violationsByImpact)
                .map(([impact, count]) => `
                    <tr>
                        <td>${impact}</td>
                        <td>${count}</td>
                    </tr>
                `).join('')}
        </table>
    </div>
    
    <h2>Detailed Violations</h2>
    ${auditResults.violations.map(violation => `
        <div class="violation ${violation.impact}">
            <h3>${violation.help}</h3>
            <p><strong>Rule:</strong> ${violation.id}</p>
            <p><strong>Impact:</strong> ${violation.impact}</p>
            <p>${violation.description}</p>
            
            <h4>Affected Elements (${violation.nodes.length})</h4>
            ${violation.nodes.map(node => `
                <div class="code">
                    <strong>Element:</strong> ${this.escapeHtml(node.html)}<br>
                    <strong>Selector:</strong> ${node.target.join(' ')}<br>
                    <strong>Fix:</strong> ${node.failureSummary}
                </div>
            `).join('')}
            
            <p><a href="${violation.helpUrl}" target="_blank">Learn more</a></p>
        </div>
    `).join('')}
    
    <h2>Manual Testing Required</h2>
    <ul>
        <li>Test with screen readers (NVDA, JAWS, VoiceOver)</li>
        <li>Test keyboard navigation thoroughly</li>
        <li>Test with browser zoom at 200%</li>
        <li>Test with Windows High Contrast mode</li>
        <li>Review content for plain language</li>
    </ul>
</body>
</html>
        `;
        
        return html;
    }
    
    generateJSONReport(auditResults) {
        return {
            metadata: {
                timestamp: new Date().toISOString(),
                url: auditResults.url,
                wcagVersion: '2.1',
                level: 'AA'
            },
            summary: auditResults.summary,
            violations: auditResults.violations.map(v => ({
                id: v.id,
                impact: v.impact,
                help: v.help,
                count: v.nodes.length,
                elements: v.nodes.map(n => ({
                    target: n.target.join(' '),
                    html: n.html
                }))
            })),
            passes: auditResults.passes,
            incomplete: auditResults.incomplete
        };
    }
}
```

## Output Format

1. **Accessibility Score**: Overall compliance score with WCAG levels
2. **Violation Report**: Detailed list of issues with severity and fixes
3. **Test Results**: Automated and manual test outcomes
4. **Remediation Guide**: Step-by-step fixes for each issue
5. **Code Examples**: Accessible component implementations
6. **Testing Scripts**: Reusable test suites for CI/CD
7. **Checklist**: Manual testing checklist for QA
8. **Progress Tracking**: Accessibility improvement metrics

Focus on creating inclusive experiences that work for all users, regardless of their abilities or assistive technologies.
````

## File: tools/ai-assistant.md

````markdown
# AI Assistant Development

You are an AI assistant development expert specializing in creating intelligent conversational interfaces, chatbots, and AI-powered applications. Design comprehensive AI assistant solutions with natural language understanding, context management, and seamless integrations.

## Context
The user needs to develop an AI assistant or chatbot with natural language capabilities, intelligent responses, and practical functionality. Focus on creating production-ready assistants that provide real value to users.

## Requirements
$ARGUMENTS

## Instructions

### 1. AI Assistant Architecture

Design comprehensive assistant architecture:

**Assistant Architecture Framework**
```python
from typing import Dict, List, Optional, Any
from dataclasses import dataclass
from abc import ABC, abstractmethod
import asyncio

@dataclass
class ConversationContext:
    """Maintains conversation state and context"""
    user_id: str
    session_id: str
    messages: List[Dict[str, Any]]
    user_profile: Dict[str, Any]
    conversation_state: Dict[str, Any]
    metadata: Dict[str, Any]

class AIAssistantArchitecture:
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.components = self._initialize_components()
        
    def design_architecture(self):
        """Design comprehensive AI assistant architecture"""
        return {
            'core_components': {
                'nlu': self._design_nlu_component(),
                'dialog_manager': self._design_dialog_manager(),
                'response_generator': self._design_response_generator(),
                'context_manager': self._design_context_manager(),
                'integration_layer': self._design_integration_layer()
            },
            'data_flow': self._design_data_flow(),
            'deployment': self._design_deployment_architecture(),
            'scalability': self._design_scalability_features()
        }
    
    def _design_nlu_component(self):
        """Natural Language Understanding component"""
        return {
            'intent_recognition': {
                'model': 'transformer-based classifier',
                'features': [
                    'Multi-intent detection',
                    'Confidence scoring',
                    'Fallback handling'
                ],
                'implementation': '''
class IntentClassifier:
    def __init__(self, model_path: str):
        self.model = self.load_model(model_path)
        self.intents = self.load_intent_schema()
    
    async def classify(self, text: str) -> Dict[str, Any]:
        # Preprocess text
        processed = self.preprocess(text)
        
        # Get model predictions
        predictions = await self.model.predict(processed)
        
        # Extract intents with confidence
        intents = []
        for intent, confidence in predictions:
            if confidence > self.config['threshold']:
                intents.append({
                    'name': intent,
                    'confidence': confidence,
                    'parameters': self.extract_parameters(text, intent)
                })
        
        return {
            'intents': intents,
            'primary_intent': intents[0] if intents else None,
            'requires_clarification': len(intents) > 1
        }
'''
            },
            'entity_extraction': {
                'model': 'NER with custom entities',
                'features': [
                    'Domain-specific entities',
                    'Contextual extraction',
                    'Entity resolution'
                ]
            },
            'sentiment_analysis': {
                'model': 'Fine-tuned sentiment classifier',
                'features': [
                    'Emotion detection',
                    'Urgency classification',
                    'User satisfaction tracking'
                ]
            }
        }
    
    def _design_dialog_manager(self):
        """Dialog management system"""
        return '''
class DialogManager:
    """Manages conversation flow and state"""
    
    def __init__(self):
        self.state_machine = ConversationStateMachine()
        self.policy_network = DialogPolicy()
        
    async def process_turn(self, 
                          context: ConversationContext, 
                          nlu_result: Dict[str, Any]) -> Dict[str, Any]:
        # Determine current state
        current_state = self.state_machine.get_state(context)
        
        # Apply dialog policy
        action = await self.policy_network.select_action(
            current_state, 
            nlu_result, 
            context
        )
        
        # Execute action
        result = await self.execute_action(action, context)
        
        # Update state
        new_state = self.state_machine.transition(
            current_state, 
            action, 
            result
        )
        
        return {
            'action': action,
            'new_state': new_state,
            'response_data': result
        }
    
    async def execute_action(self, action: str, context: ConversationContext):
        """Execute dialog action"""
        action_handlers = {
            'greet': self.handle_greeting,
            'provide_info': self.handle_information_request,
            'clarify': self.handle_clarification,
            'confirm': self.handle_confirmation,
            'execute_task': self.handle_task_execution,
            'end_conversation': self.handle_conversation_end
        }
        
        handler = action_handlers.get(action, self.handle_unknown)
        return await handler(context)
'''
```

### 2. Natural Language Processing

Implement advanced NLP capabilities:

**NLP Pipeline Implementation**
```python
class NLPPipeline:
    def __init__(self):
        self.tokenizer = self._initialize_tokenizer()
        self.embedder = self._initialize_embedder()
        self.models = self._load_models()
    
    async def process_message(self, message: str, context: ConversationContext):
        """Process user message through NLP pipeline"""
        # Tokenization and preprocessing
        tokens = self.tokenizer.tokenize(message)
        
        # Generate embeddings
        embeddings = await self.embedder.embed(tokens)
        
        # Parallel processing of NLP tasks
        tasks = [
            self.detect_intent(embeddings),
            self.extract_entities(tokens, embeddings),
            self.analyze_sentiment(embeddings),
            self.detect_language(tokens),
            self.check_spelling(tokens)
        ]
        
        results = await asyncio.gather(*tasks)
        
        return {
            'intent': results[0],
            'entities': results[1],
            'sentiment': results[2],
            'language': results[3],
            'corrections': results[4],
            'original_message': message,
            'processed_tokens': tokens
        }
    
    async def detect_intent(self, embeddings):
        """Advanced intent detection"""
        # Multi-label classification
        intent_scores = await self.models['intent_classifier'].predict(embeddings)
        
        # Hierarchical intent detection
        primary_intent = self.get_primary_intent(intent_scores)
        sub_intents = self.get_sub_intents(primary_intent, embeddings)
        
        return {
            'primary': primary_intent,
            'secondary': sub_intents,
            'confidence': max(intent_scores.values()),
            'all_scores': intent_scores
        }
    
    def extract_entities(self, tokens, embeddings):
        """Extract and resolve entities"""
        # Named Entity Recognition
        entities = self.models['ner'].extract(tokens, embeddings)
        
        # Entity linking and resolution
        resolved_entities = []
        for entity in entities:
            resolved = self.resolve_entity(entity)
            resolved_entities.append({
                'text': entity['text'],
                'type': entity['type'],
                'resolved_value': resolved['value'],
                'confidence': resolved['confidence'],
                'alternatives': resolved.get('alternatives', [])
            })
        
        return resolved_entities
    
    def build_semantic_understanding(self, nlu_result, context):
        """Build semantic representation of user intent"""
        return {
            'user_goal': self.infer_user_goal(nlu_result, context),
            'required_information': self.identify_missing_info(nlu_result),
            'constraints': self.extract_constraints(nlu_result),
            'preferences': self.extract_preferences(nlu_result, context)
        }
```

### 3. Conversation Flow Design

Design intelligent conversation flows:

**Conversation Flow Engine**
```python
class ConversationFlowEngine:
    def __init__(self):
        self.flows = self._load_conversation_flows()
        self.state_tracker = StateTracker()
        
    def design_conversation_flow(self):
        """Design multi-turn conversation flows"""
        return {
            'greeting_flow': {
                'triggers': ['hello', 'hi', 'greetings'],
                'nodes': [
                    {
                        'id': 'greet_user',
                        'type': 'response',
                        'content': self.personalized_greeting,
                        'next': 'ask_how_to_help'
                    },
                    {
                        'id': 'ask_how_to_help',
                        'type': 'question',
                        'content': "How can I assist you today?",
                        'expected_intents': ['request_help', 'ask_question'],
                        'timeout': 30,
                        'timeout_action': 'offer_suggestions'
                    }
                ]
            },
            'task_completion_flow': {
                'triggers': ['task_request'],
                'nodes': [
                    {
                        'id': 'understand_task',
                        'type': 'nlu_processing',
                        'extract': ['task_type', 'parameters'],
                        'next': 'check_requirements'
                    },
                    {
                        'id': 'check_requirements',
                        'type': 'validation',
                        'validate': self.validate_task_requirements,
                        'on_success': 'confirm_task',
                        'on_missing': 'request_missing_info'
                    },
                    {
                        'id': 'request_missing_info',
                        'type': 'slot_filling',
                        'slots': self.get_required_slots,
                        'prompts': self.get_slot_prompts,
                        'next': 'confirm_task'
                    },
                    {
                        'id': 'confirm_task',
                        'type': 'confirmation',
                        'content': self.generate_task_summary,
                        'on_confirm': 'execute_task',
                        'on_deny': 'clarify_task'
                    }
                ]
            }
        }
    
    async def execute_flow(self, flow_id: str, context: ConversationContext):
        """Execute a conversation flow"""
        flow = self.flows[flow_id]
        current_node = flow['nodes'][0]
        
        while current_node:
            result = await self.execute_node(current_node, context)
            
            # Determine next node
            if result.get('user_input'):
                next_node_id = self.determine_next_node(
                    current_node, 
                    result['user_input'],
                    context
                )
            else:
                next_node_id = current_node.get('next')
            
            current_node = self.get_node(flow, next_node_id)
            
            # Update context
            context.conversation_state.update(result.get('state_updates', {}))
        
        return context
```

### 4. Response Generation

Create intelligent response generation:

**Response Generator**
```python
class ResponseGenerator:
    def __init__(self, llm_client=None):
        self.llm = llm_client
        self.templates = self._load_response_templates()
        self.personality = self._load_personality_config()
        
    async def generate_response(self, 
                               intent: str, 
                               context: ConversationContext,
                               data: Dict[str, Any]) -> str:
        """Generate contextual responses"""
        
        # Select response strategy
        if self.should_use_template(intent):
            response = self.generate_from_template(intent, data)
        elif self.should_use_llm(intent, context):
            response = await self.generate_with_llm(intent, context, data)
        else:
            response = self.generate_hybrid_response(intent, context, data)
        
        # Apply personality and tone
        response = self.apply_personality(response, context)
        
        # Ensure response appropriateness
        response = self.validate_response(response, context)
        
        return response
    
    async def generate_with_llm(self, intent, context, data):
        """Generate response using LLM"""
        # Construct prompt
        prompt = self.build_llm_prompt(intent, context, data)
        
        # Set generation parameters
        params = {
            'temperature': self.get_temperature(intent),
            'max_tokens': 150,
            'stop_sequences': ['\n\n', 'User:', 'Human:']
        }
        
        # Generate response
        response = await self.llm.generate(prompt, **params)
        
        # Post-process response
        return self.post_process_llm_response(response)
    
    def build_llm_prompt(self, intent, context, data):
        """Build context-aware prompt for LLM"""
        return f"""
You are a helpful AI assistant with the following characteristics:
{self.personality.description}

Conversation history:
{self.format_conversation_history(context.messages[-5:])}

User intent: {intent}
Relevant data: {json.dumps(data, indent=2)}

Generate a helpful, concise response that:
1. Addresses the user's intent
2. Uses the provided data appropriately
3. Maintains conversation continuity
4. Follows the personality guidelines

Response:"""
    
    def generate_from_template(self, intent, data):
        """Generate response from templates"""
        template = self.templates.get(intent)
        if not template:
            return self.get_fallback_response()
        
        # Select template variant
        variant = self.select_template_variant(template, data)
        
        # Fill template slots
        response = variant
        for key, value in data.items():
            response = response.replace(f"{{{key}}}", str(value))
        
        return response
    
    def apply_personality(self, response, context):
        """Apply personality traits to response"""
        # Add personality markers
        if self.personality.get('friendly'):
            response = self.add_friendly_markers(response)
        
        if self.personality.get('professional'):
            response = self.ensure_professional_tone(response)
        
        # Adjust based on user preferences
        if context.user_profile.get('prefers_brief'):
            response = self.make_concise(response)
        
        return response
```

### 5. Context Management

Implement sophisticated context management:

**Context Management System**
```python
class ContextManager:
    def __init__(self):
        self.short_term_memory = ShortTermMemory()
        self.long_term_memory = LongTermMemory()
        self.working_memory = WorkingMemory()
        
    async def manage_context(self, 
                            new_input: Dict[str, Any],
                            current_context: ConversationContext) -> ConversationContext:
        """Manage conversation context"""
        
        # Update conversation history
        current_context.messages.append({
            'role': 'user',
            'content': new_input['message'],
            'timestamp': datetime.now(),
            'metadata': new_input.get('metadata', {})
        })
        
        # Resolve references
        resolved_input = await self.resolve_references(new_input, current_context)
        
        # Update working memory
        self.working_memory.update(resolved_input, current_context)
        
        # Detect topic changes
        topic_shift = self.detect_topic_shift(resolved_input, current_context)
        if topic_shift:
            current_context = self.handle_topic_shift(topic_shift, current_context)
        
        # Maintain entity state
        current_context = self.update_entity_state(resolved_input, current_context)
        
        # Prune old context if needed
        if len(current_context.messages) > self.config['max_context_length']:
            current_context = self.prune_context(current_context)
        
        return current_context
    
    async def resolve_references(self, input_data, context):
        """Resolve pronouns and references"""
        text = input_data['message']
        
        # Pronoun resolution
        pronouns = self.extract_pronouns(text)
        for pronoun in pronouns:
            referent = self.find_referent(pronoun, context)
            if referent:
                text = text.replace(pronoun['text'], referent['resolved'])
        
        # Temporal reference resolution
        temporal_refs = self.extract_temporal_references(text)
        for ref in temporal_refs:
            resolved_time = self.resolve_temporal_reference(ref, context)
            text = text.replace(ref['text'], str(resolved_time))
        
        input_data['resolved_message'] = text
        return input_data
    
    def maintain_entity_state(self):
        """Track entity states across conversation"""
        return '''
class EntityStateTracker:
    def __init__(self):
        self.entities = {}
        
    def update_entity(self, entity_id: str, updates: Dict[str, Any]):
        """Update entity state"""
        if entity_id not in self.entities:
            self.entities[entity_id] = {
                'id': entity_id,
                'type': updates.get('type'),
                'attributes': {},
                'history': []
            }
        
        # Record history
        self.entities[entity_id]['history'].append({
            'timestamp': datetime.now(),
            'updates': updates
        })
        
        # Apply updates
        self.entities[entity_id]['attributes'].update(updates)
    
    def get_entity_state(self, entity_id: str) -> Optional[Dict[str, Any]]:
        """Get current entity state"""
        return self.entities.get(entity_id)
    
    def query_entities(self, entity_type: str = None, **filters):
        """Query entities by type and attributes"""
        results = []
        for entity in self.entities.values():
            if entity_type and entity['type'] != entity_type:
                continue
            
            matches = True
            for key, value in filters.items():
                if entity['attributes'].get(key) != value:
                    matches = False
                    break
            
            if matches:
                results.append(entity)
        
        return results
'''
```

### 6. Integration with LLMs

Integrate with various LLM providers:

**LLM Integration Layer**
```python
class LLMIntegrationLayer:
    def __init__(self):
        self.providers = {
            'openai': OpenAIProvider(),
            'anthropic': AnthropicProvider(),
            'local': LocalLLMProvider()
        }
        self.current_provider = None
        
    async def setup_llm_integration(self, provider: str, config: Dict[str, Any]):
        """Setup LLM integration"""
        self.current_provider = self.providers[provider]
        await self.current_provider.initialize(config)
        
        return {
            'provider': provider,
            'capabilities': self.current_provider.get_capabilities(),
            'rate_limits': self.current_provider.get_rate_limits()
        }
    
    async def generate_completion(self, 
                                 prompt: str,
                                 system_prompt: str = None,
                                 **kwargs):
        """Generate completion with fallback handling"""
        try:
            # Primary attempt
            response = await self.current_provider.complete(
                prompt=prompt,
                system_prompt=system_prompt,
                **kwargs
            )
            
            # Validate response
            if self.is_valid_response(response):
                return response
            else:
                return await self.handle_invalid_response(prompt, response)
                
        except RateLimitError:
            # Switch to fallback provider
            return await self.use_fallback_provider(prompt, system_prompt, **kwargs)
        except Exception as e:
            # Log error and use cached response if available
            return self.get_cached_response(prompt) or self.get_default_response()
    
    def create_function_calling_interface(self):
        """Create function calling interface for LLMs"""
        return '''
class FunctionCallingInterface:
    def __init__(self):
        self.functions = {}
        
    def register_function(self, 
                         name: str,
                         func: callable,
                         description: str,
                         parameters: Dict[str, Any]):
        """Register a function for LLM to call"""
        self.functions[name] = {
            'function': func,
            'description': description,
            'parameters': parameters
        }
    
    async def process_function_call(self, llm_response):
        """Process function calls from LLM"""
        if 'function_call' not in llm_response:
            return llm_response
        
        function_name = llm_response['function_call']['name']
        arguments = llm_response['function_call']['arguments']
        
        if function_name not in self.functions:
            return {'error': f'Unknown function: {function_name}'}
        
        # Validate arguments
        validated_args = self.validate_arguments(
            function_name, 
            arguments
        )
        
        # Execute function
        result = await self.functions[function_name]['function'](**validated_args)
        
        # Return result for LLM to process
        return {
            'function_result': result,
            'function_name': function_name
        }
'''
```

### 7. Testing Conversational AI

Implement comprehensive testing:

**Conversation Testing Framework**
```python
class ConversationTestFramework:
    def __init__(self):
        self.test_suites = []
        self.metrics = ConversationMetrics()
        
    def create_test_suite(self):
        """Create comprehensive test suite"""
        return {
            'unit_tests': self._create_unit_tests(),
            'integration_tests': self._create_integration_tests(),
            'conversation_tests': self._create_conversation_tests(),
            'performance_tests': self._create_performance_tests(),
            'user_simulation': self._create_user_simulation()
        }
    
    def _create_conversation_tests(self):
        """Test multi-turn conversations"""
        return '''
class ConversationTest:
    async def test_multi_turn_conversation(self):
        """Test complete conversation flow"""
        assistant = AIAssistant()
        context = ConversationContext(user_id="test_user")
        
        # Conversation script
        conversation = [
            {
                'user': "Hello, I need help with my order",
                'expected_intent': 'order_help',
                'expected_action': 'ask_order_details'
            },
            {
                'user': "My order number is 12345",
                'expected_entities': [{'type': 'order_id', 'value': '12345'}],
                'expected_action': 'retrieve_order'
            },
            {
                'user': "When will it arrive?",
                'expected_intent': 'delivery_inquiry',
                'should_use_context': True
            }
        ]
        
        for turn in conversation:
            # Send user message
            response = await assistant.process_message(
                turn['user'], 
                context
            )
            
            # Validate intent detection
            if 'expected_intent' in turn:
                assert response['intent'] == turn['expected_intent']
            
            # Validate entity extraction
            if 'expected_entities' in turn:
                self.validate_entities(
                    response['entities'], 
                    turn['expected_entities']
                )
            
            # Validate context usage
            if turn.get('should_use_context'):
                assert 'order_id' in response['context_used']
    
    def test_error_handling(self):
        """Test error scenarios"""
        error_cases = [
            {
                'input': "askdjfkajsdf",
                'expected_behavior': 'fallback_response'
            },
            {
                'input': "I want to [REDACTED]",
                'expected_behavior': 'safety_response'
            },
            {
                'input': "Tell me about " + "x" * 1000,
                'expected_behavior': 'length_limit_response'
            }
        ]
        
        for case in error_cases:
            response = assistant.process_message(case['input'])
            assert response['behavior'] == case['expected_behavior']
'''
    
    def create_automated_testing(self):
        """Automated conversation testing"""
        return '''
class AutomatedConversationTester:
    def __init__(self):
        self.test_generator = TestCaseGenerator()
        self.evaluator = ResponseEvaluator()
        
    async def run_automated_tests(self, num_tests: int = 100):
        """Run automated conversation tests"""
        results = {
            'total_tests': num_tests,
            'passed': 0,
            'failed': 0,
            'metrics': {}
        }
        
        for i in range(num_tests):
            # Generate test case
            test_case = self.test_generator.generate()
            
            # Run conversation
            conversation_log = await self.run_conversation(test_case)
            
            # Evaluate results
            evaluation = self.evaluator.evaluate(
                conversation_log,
                test_case['expectations']
            )
            
            if evaluation['passed']:
                results['passed'] += 1
            else:
                results['failed'] += 1
                
            # Collect metrics
            self.update_metrics(results['metrics'], evaluation['metrics'])
        
        return results
    
    def generate_adversarial_tests(self):
        """Generate adversarial test cases"""
        return [
            # Ambiguous inputs
            "I want that thing we discussed",
            
            # Context switching
            "Actually, forget that. Tell me about the weather",
            
            # Multiple intents
            "Cancel my order and also update my address",
            
            # Incomplete information
            "Book a flight",
            
            # Contradictions
            "I want a vegetarian meal with bacon"
        ]
'''
```

### 8. Deployment and Scaling

Deploy and scale AI assistants:

**Deployment Architecture**
```python
class AssistantDeployment:
    def create_deployment_architecture(self):
        """Create scalable deployment architecture"""
        return {
            'containerization': '''
# Dockerfile for AI Assistant
FROM python:3.11-slim

WORKDIR /app

# Install dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy application
COPY . .

# Load models at build time
RUN python -m app.model_loader

# Expose port
EXPOSE 8080

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
  CMD python -m app.health_check

# Run application
CMD ["gunicorn", "--worker-class", "uvicorn.workers.UvicornWorker", \
     "--workers", "4", "--bind", "0.0.0.0:8080", "app.main:app"]
''',
            'kubernetes_deployment': '''
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ai-assistant
spec:
  replicas: 3
  selector:
    matchLabels:
      app: ai-assistant
  template:
    metadata:
      labels:
        app: ai-assistant
    spec:
      containers:
      - name: assistant
        image: ai-assistant:latest
        ports:
        - containerPort: 8080
        resources:
          requests:
            memory: "2Gi"
            cpu: "1000m"
          limits:
            memory: "4Gi"
            cpu: "2000m"
        env:
        - name: MODEL_CACHE_SIZE
          value: "1000"
        - name: MAX_CONCURRENT_SESSIONS
          value: "100"
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /ready
            port: 8080
          periodSeconds: 5
---
apiVersion: v1
kind: Service
metadata:
  name: ai-assistant-service
spec:
  selector:
    app: ai-assistant
  ports:
  - port: 80
    targetPort: 8080
  type: LoadBalancer
---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: ai-assistant-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: ai-assistant
  minReplicas: 3
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
''',
            'caching_strategy': self._design_caching_strategy(),
            'load_balancing': self._design_load_balancing()
        }
    
    def _design_caching_strategy(self):
        """Design caching for performance"""
        return '''
class AssistantCache:
    def __init__(self):
        self.response_cache = ResponseCache()
        self.model_cache = ModelCache()
        self.context_cache = ContextCache()
        
    async def get_cached_response(self, 
                                 message: str, 
                                 context_hash: str) -> Optional[str]:
        """Get cached response if available"""
        cache_key = self.generate_cache_key(message, context_hash)
        
        # Check response cache
        cached = await self.response_cache.get(cache_key)
        if cached and not self.is_expired(cached):
            return cached['response']
        
        return None
    
    def cache_response(self, 
                      message: str,
                      context_hash: str,
                      response: str,
                      ttl: int = 3600):
        """Cache response with TTL"""
        cache_key = self.generate_cache_key(message, context_hash)
        
        self.response_cache.set(
            cache_key,
            {
                'response': response,
                'timestamp': datetime.now(),
                'ttl': ttl
            }
        )
    
    def preload_model_cache(self):
        """Preload frequently used models"""
        models_to_cache = [
            'intent_classifier',
            'entity_extractor',
            'response_generator'
        ]
        
        for model_name in models_to_cache:
            model = load_model(model_name)
            self.model_cache.store(model_name, model)
'''
```

### 9. Monitoring and Analytics

Monitor assistant performance:

**Assistant Analytics System**
```python
class AssistantAnalytics:
    def __init__(self):
        self.metrics_collector = MetricsCollector()
        self.analytics_engine = AnalyticsEngine()
        
    def create_monitoring_dashboard(self):
        """Create monitoring dashboard configuration"""
        return {
            'real_time_metrics': {
                'active_sessions': 'gauge',
                'messages_per_second': 'counter',
                'response_time_p95': 'histogram',
                'intent_accuracy': 'gauge',
                'fallback_rate': 'gauge'
            },
            'conversation_metrics': {
                'avg_conversation_length': 'gauge',
                'completion_rate': 'gauge',
                'user_satisfaction': 'gauge',
                'escalation_rate': 'gauge'
            },
            'system_metrics': {
                'model_inference_time': 'histogram',
                'cache_hit_rate': 'gauge',
                'error_rate': 'counter',
                'resource_utilization': 'gauge'
            },
            'alerts': [
                {
                    'name': 'high_fallback_rate',
                    'condition': 'fallback_rate > 0.2',
                    'severity': 'warning'
                },
                {
                    'name': 'slow_response_time',
                    'condition': 'response_time_p95 > 2000',
                    'severity': 'critical'
                }
            ]
        }
    
    def analyze_conversation_quality(self):
        """Analyze conversation quality metrics"""
        return '''
class ConversationQualityAnalyzer:
    def analyze_conversations(self, time_range: str):
        """Analyze conversation quality"""
        conversations = self.fetch_conversations(time_range)
        
        metrics = {
            'intent_recognition': self.analyze_intent_accuracy(conversations),
            'response_relevance': self.analyze_response_relevance(conversations),
            'conversation_flow': self.analyze_conversation_flow(conversations),
            'user_satisfaction': self.analyze_satisfaction(conversations),
            'error_patterns': self.identify_error_patterns(conversations)
        }
        
        return self.generate_quality_report(metrics)
    
    def identify_improvement_areas(self, analysis):
        """Identify areas for improvement"""
        improvements = []
        
        # Low intent accuracy
        if analysis['intent_recognition']['accuracy'] < 0.85:
            improvements.append({
                'area': 'Intent Recognition',
                'issue': 'Low accuracy in intent detection',
                'recommendation': 'Retrain intent classifier with more examples',
                'priority': 'high'
            })
        
        # High fallback rate
        if analysis['conversation_flow']['fallback_rate'] > 0.15:
            improvements.append({
                'area': 'Coverage',
                'issue': 'High fallback rate',
                'recommendation': 'Expand training data for uncovered intents',
                'priority': 'medium'
            })
        
        return improvements
'''
```

### 10. Continuous Improvement

Implement continuous improvement cycle:

**Improvement Pipeline**
```python
class ContinuousImprovement:
    def create_improvement_pipeline(self):
        """Create continuous improvement pipeline"""
        return {
            'data_collection': '''
class ConversationDataCollector:
    async def collect_feedback(self, session_id: str):
        """Collect user feedback"""
        feedback_prompt = {
            'satisfaction': 'How satisfied were you with this conversation? (1-5)',
            'resolved': 'Was your issue resolved?',
            'improvements': 'How could we improve?'
        }
        
        feedback = await self.prompt_user_feedback(
            session_id, 
            feedback_prompt
        )
        
        # Store feedback
        await self.store_feedback({
            'session_id': session_id,
            'timestamp': datetime.now(),
            'feedback': feedback,
            'conversation_metadata': self.get_session_metadata(session_id)
        })
        
        return feedback
    
    def identify_training_opportunities(self):
        """Identify conversations for training"""
        # Find low-confidence interactions
        low_confidence = self.find_low_confidence_interactions()
        
        # Find failed conversations
        failed = self.find_failed_conversations()
        
        # Find highly-rated conversations
        exemplary = self.find_exemplary_conversations()
        
        return {
            'needs_improvement': low_confidence + failed,
            'good_examples': exemplary
        }
''',
            'model_retraining': '''
class ModelRetrainer:
    async def retrain_models(self, new_data):
        """Retrain models with new data"""
        # Prepare training data
        training_data = self.prepare_training_data(new_data)
        
        # Validate data quality
        validation_result = self.validate_training_data(training_data)
        if not validation_result['passed']:
            return {'error': 'Data quality check failed', 'issues': validation_result['issues']}
        
        # Retrain models
        models_to_retrain = ['intent_classifier', 'entity_extractor']
        
        for model_name in models_to_retrain:
            # Load current model
            current_model = self.load_model(model_name)
            
            # Create new version
            new_model = await self.train_model(
                model_name,
                training_data,
                base_model=current_model
            )
            
            # Evaluate new model
            evaluation = await self.evaluate_model(
                new_model,
                self.get_test_set()
            )
            
            # Deploy if improved
            if evaluation['performance'] > current_model.performance:
                await self.deploy_model(new_model, model_name)
        
        return {'status': 'completed', 'models_updated': models_to_retrain}
''',
            'a_b_testing': '''
class ABTestingFramework:
    def create_ab_test(self, 
                      test_name: str,
                      variants: List[Dict[str, Any]],
                      metrics: List[str]):
        """Create A/B test for assistant improvements"""
        test = {
            'id': generate_test_id(),
            'name': test_name,
            'variants': variants,
            'metrics': metrics,
            'allocation': self.calculate_traffic_allocation(variants),
            'duration': self.estimate_test_duration(metrics)
        }
        
        # Deploy test
        self.deploy_test(test)
        
        return test
    
    async def analyze_test_results(self, test_id: str):
        """Analyze A/B test results"""
        data = await self.collect_test_data(test_id)
        
        results = {}
        for metric in data['metrics']:
            # Statistical analysis
            analysis = self.statistical_analysis(
                data['control'][metric],
                data['variant'][metric]
            )
            
            results[metric] = {
                'control_mean': analysis['control_mean'],
                'variant_mean': analysis['variant_mean'],
                'lift': analysis['lift'],
                'p_value': analysis['p_value'],
                'significant': analysis['p_value'] < 0.05
            }
        
        return results
'''
        }
```

## Output Format

1. **Architecture Design**: Complete AI assistant architecture with components
2. **NLP Implementation**: Natural language processing pipeline and models
3. **Conversation Flows**: Dialog management and flow design
4. **Response Generation**: Intelligent response creation with LLM integration
5. **Context Management**: Sophisticated context and state management
6. **Testing Framework**: Comprehensive testing for conversational AI
7. **Deployment Guide**: Scalable deployment architecture
8. **Monitoring Setup**: Analytics and performance monitoring
9. **Improvement Pipeline**: Continuous improvement processes

Focus on creating production-ready AI assistants that provide real value through natural conversations, intelligent responses, and continuous learning from user interactions.
````

## File: tools/ai-review.md

````markdown
# AI/ML Code Review

Perform a specialized AI/ML code review for: $ARGUMENTS

Conduct comprehensive review focusing on:

1. **Model Code Quality**:
   - Reproducibility checks
   - Random seed management
   - Data leakage detection
   - Train/test split validation
   - Feature engineering clarity

2. **AI Best Practices**:
   - Prompt injection prevention
   - Token limit handling
   - Cost optimization
   - Fallback strategies
   - Timeout management

3. **Data Handling**:
   - Privacy compliance (PII handling)
   - Data versioning
   - Preprocessing consistency
   - Batch processing efficiency
   - Memory optimization

4. **Model Management**:
   - Version control for models
   - A/B testing setup
   - Rollback capabilities
   - Performance benchmarks
   - Drift detection

5. **LLM-Specific Checks**:
   - Context window management
   - Prompt template security
   - Response validation
   - Streaming implementation
   - Rate limit handling

6. **Vector Database Review**:
   - Embedding consistency
   - Index optimization
   - Query performance
   - Metadata management
   - Backup strategies

7. **Production Readiness**:
   - GPU/CPU optimization
   - Batching strategies
   - Caching implementation
   - Monitoring hooks
   - Error recovery

8. **Testing Coverage**:
   - Unit tests for preprocessing
   - Integration tests for pipelines
   - Model performance tests
   - Edge case handling
   - Mocked LLM responses

Provide specific recommendations with severity levels (Critical/High/Medium/Low). Include code examples for improvements and links to relevant best practices.
````

## File: tools/api-mock.md

````markdown
# API Mocking Framework

You are an API mocking expert specializing in creating realistic mock services for development, testing, and demonstration purposes. Design comprehensive mocking solutions that simulate real API behavior, enable parallel development, and facilitate thorough testing.

## Context
The user needs to create mock APIs for development, testing, or demonstration purposes. Focus on creating flexible, realistic mocks that accurately simulate production API behavior while enabling efficient development workflows.

## Requirements
$ARGUMENTS

## Instructions

### 1. Mock Server Setup

Create comprehensive mock server infrastructure:

**Mock Server Framework**
```python
from typing import Dict, List, Any, Optional
import json
import asyncio
from datetime import datetime
from fastapi import FastAPI, Request, Response
import uvicorn

class MockAPIServer:
    def __init__(self, config: Dict[str, Any]):
        self.app = FastAPI(title="Mock API Server")
        self.routes = {}
        self.middleware = []
        self.state_manager = StateManager()
        self.scenario_manager = ScenarioManager()
        
    def setup_mock_server(self):
        """Setup comprehensive mock server"""
        # Configure middleware
        self._setup_middleware()
        
        # Load mock definitions
        self._load_mock_definitions()
        
        # Setup dynamic routes
        self._setup_dynamic_routes()
        
        # Initialize scenarios
        self._initialize_scenarios()
        
        return self.app
    
    def _setup_middleware(self):
        """Configure server middleware"""
        @self.app.middleware("http")
        async def add_mock_headers(request: Request, call_next):
            response = await call_next(request)
            response.headers["X-Mock-Server"] = "true"
            response.headers["X-Mock-Scenario"] = self.scenario_manager.current_scenario
            return response
        
        @self.app.middleware("http")
        async def simulate_latency(request: Request, call_next):
            # Simulate network latency
            latency = self._calculate_latency(request.url.path)
            await asyncio.sleep(latency / 1000)  # Convert to seconds
            response = await call_next(request)
            return response
        
        @self.app.middleware("http")
        async def track_requests(request: Request, call_next):
            # Track request for verification
            self.state_manager.track_request({
                'method': request.method,
                'path': str(request.url.path),
                'headers': dict(request.headers),
                'timestamp': datetime.now()
            })
            response = await call_next(request)
            return response
    
    def _setup_dynamic_routes(self):
        """Setup dynamic route handling"""
        @self.app.api_route("/{path:path}", methods=["GET", "POST", "PUT", "DELETE", "PATCH"])
        async def handle_mock_request(path: str, request: Request):
            # Find matching mock
            mock = self._find_matching_mock(request.method, path, request)
            
            if not mock:
                return Response(
                    content=json.dumps({"error": "No mock found for this endpoint"}),
                    status_code=404,
                    media_type="application/json"
                )
            
            # Process mock response
            response_data = await self._process_mock_response(mock, request)
            
            return Response(
                content=json.dumps(response_data['body']),
                status_code=response_data['status'],
                headers=response_data['headers'],
                media_type="application/json"
            )
    
    async def _process_mock_response(self, mock: Dict[str, Any], request: Request):
        """Process and generate mock response"""
        # Check for conditional responses
        if mock.get('conditions'):
            for condition in mock['conditions']:
                if self._evaluate_condition(condition, request):
                    return await self._generate_response(condition['response'], request)
        
        # Use default response
        return await self._generate_response(mock['response'], request)
    
    def _generate_response(self, response_template: Dict[str, Any], request: Request):
        """Generate response from template"""
        response = {
            'status': response_template.get('status', 200),
            'headers': response_template.get('headers', {}),
            'body': self._process_response_body(response_template['body'], request)
        }
        
        # Apply response transformations
        if response_template.get('transformations'):
            response = self._apply_transformations(response, response_template['transformations'])
        
        return response
```

### 2. Request/Response Stubbing

Implement flexible stubbing system:

**Stubbing Engine**
```python
class StubbingEngine:
    def __init__(self):
        self.stubs = {}
        self.matchers = self._initialize_matchers()
        
    def create_stub(self, method: str, path: str, **kwargs):
        """Create a new stub"""
        stub_id = self._generate_stub_id()
        
        stub = {
            'id': stub_id,
            'method': method,
            'path': path,
            'matchers': self._build_matchers(kwargs),
            'response': kwargs.get('response', {}),
            'priority': kwargs.get('priority', 0),
            'times': kwargs.get('times', -1),  # -1 for unlimited
            'delay': kwargs.get('delay', 0),
            'scenario': kwargs.get('scenario', 'default')
        }
        
        self.stubs[stub_id] = stub
        return stub_id
    
    def _build_matchers(self, kwargs):
        """Build request matchers"""
        matchers = []
        
        # Path parameter matching
        if 'path_params' in kwargs:
            matchers.append({
                'type': 'path_params',
                'params': kwargs['path_params']
            })
        
        # Query parameter matching
        if 'query_params' in kwargs:
            matchers.append({
                'type': 'query_params',
                'params': kwargs['query_params']
            })
        
        # Header matching
        if 'headers' in kwargs:
            matchers.append({
                'type': 'headers',
                'headers': kwargs['headers']
            })
        
        # Body matching
        if 'body' in kwargs:
            matchers.append({
                'type': 'body',
                'body': kwargs['body'],
                'match_type': kwargs.get('body_match_type', 'exact')
            })
        
        return matchers
    
    def match_request(self, request: Dict[str, Any]):
        """Find matching stub for request"""
        candidates = []
        
        for stub in self.stubs.values():
            if self._matches_stub(request, stub):
                candidates.append(stub)
        
        # Sort by priority and return best match
        if candidates:
            return sorted(candidates, key=lambda x: x['priority'], reverse=True)[0]
        
        return None
    
    def _matches_stub(self, request: Dict[str, Any], stub: Dict[str, Any]):
        """Check if request matches stub"""
        # Check method
        if request['method'] != stub['method']:
            return False
        
        # Check path
        if not self._matches_path(request['path'], stub['path']):
            return False
        
        # Check all matchers
        for matcher in stub['matchers']:
            if not self._evaluate_matcher(request, matcher):
                return False
        
        # Check if stub is still valid
        if stub['times'] == 0:
            return False
        
        return True
    
    def create_dynamic_stub(self):
        """Create dynamic stub with callbacks"""
        return '''
class DynamicStub:
    def __init__(self, path_pattern: str):
        self.path_pattern = path_pattern
        self.response_generator = None
        self.state_modifier = None
        
    def with_response_generator(self, generator):
        """Set dynamic response generator"""
        self.response_generator = generator
        return self
    
    def with_state_modifier(self, modifier):
        """Set state modification callback"""
        self.state_modifier = modifier
        return self
    
    async def process_request(self, request: Request, state: Dict[str, Any]):
        """Process request dynamically"""
        # Extract request data
        request_data = {
            'method': request.method,
            'path': request.url.path,
            'headers': dict(request.headers),
            'query_params': dict(request.query_params),
            'body': await request.json() if request.method in ['POST', 'PUT'] else None
        }
        
        # Modify state if needed
        if self.state_modifier:
            state = self.state_modifier(state, request_data)
        
        # Generate response
        if self.response_generator:
            response = self.response_generator(request_data, state)
        else:
            response = {'status': 200, 'body': {}}
        
        return response, state

# Usage example
dynamic_stub = DynamicStub('/api/users/{user_id}')
dynamic_stub.with_response_generator(lambda req, state: {
    'status': 200,
    'body': {
        'id': req['path_params']['user_id'],
        'name': state.get('users', {}).get(req['path_params']['user_id'], 'Unknown'),
        'request_count': state.get('request_count', 0)
    }
}).with_state_modifier(lambda state, req: {
    **state,
    'request_count': state.get('request_count', 0) + 1
})
'''
```

### 3. Dynamic Data Generation

Generate realistic mock data:

**Mock Data Generator**
```python
from faker import Faker
import random
from datetime import datetime, timedelta

class MockDataGenerator:
    def __init__(self):
        self.faker = Faker()
        self.templates = {}
        self.generators = self._init_generators()
        
    def generate_data(self, schema: Dict[str, Any]):
        """Generate data based on schema"""
        if isinstance(schema, dict):
            if '$ref' in schema:
                # Reference to another schema
                return self.generate_data(self.resolve_ref(schema['$ref']))
            
            result = {}
            for key, value in schema.items():
                if key.startswith('$'):
                    continue
                result[key] = self._generate_field(value)
            return result
        
        elif isinstance(schema, list):
            # Generate array
            count = random.randint(1, 10)
            return [self.generate_data(schema[0]) for _ in range(count)]
        
        else:
            return schema
    
    def _generate_field(self, field_schema: Dict[str, Any]):
        """Generate field value based on schema"""
        field_type = field_schema.get('type', 'string')
        
        # Check for custom generator
        if 'generator' in field_schema:
            return self._use_custom_generator(field_schema['generator'])
        
        # Check for enum
        if 'enum' in field_schema:
            return random.choice(field_schema['enum'])
        
        # Generate based on type
        generators = {
            'string': self._generate_string,
            'number': self._generate_number,
            'integer': self._generate_integer,
            'boolean': self._generate_boolean,
            'array': self._generate_array,
            'object': lambda s: self.generate_data(s)
        }
        
        generator = generators.get(field_type, self._generate_string)
        return generator(field_schema)
    
    def _generate_string(self, schema: Dict[str, Any]):
        """Generate string value"""
        # Check for format
        format_type = schema.get('format', '')
        
        format_generators = {
            'email': self.faker.email,
            'name': self.faker.name,
            'first_name': self.faker.first_name,
            'last_name': self.faker.last_name,
            'phone': self.faker.phone_number,
            'address': self.faker.address,
            'url': self.faker.url,
            'uuid': self.faker.uuid4,
            'date': lambda: self.faker.date().isoformat(),
            'datetime': lambda: self.faker.date_time().isoformat(),
            'password': lambda: self.faker.password()
        }
        
        if format_type in format_generators:
            return format_generators[format_type]()
        
        # Check for pattern
        if 'pattern' in schema:
            return self._generate_from_pattern(schema['pattern'])
        
        # Default string generation
        min_length = schema.get('minLength', 5)
        max_length = schema.get('maxLength', 20)
        return self.faker.text(max_nb_chars=random.randint(min_length, max_length))
    
    def create_data_templates(self):
        """Create reusable data templates"""
        return {
            'user': {
                'id': {'type': 'string', 'format': 'uuid'},
                'username': {'type': 'string', 'generator': 'username'},
                'email': {'type': 'string', 'format': 'email'},
                'profile': {
                    'type': 'object',
                    'properties': {
                        'firstName': {'type': 'string', 'format': 'first_name'},
                        'lastName': {'type': 'string', 'format': 'last_name'},
                        'avatar': {'type': 'string', 'format': 'url'},
                        'bio': {'type': 'string', 'maxLength': 200}
                    }
                },
                'createdAt': {'type': 'string', 'format': 'datetime'},
                'status': {'type': 'string', 'enum': ['active', 'inactive', 'suspended']}
            },
            'product': {
                'id': {'type': 'string', 'format': 'uuid'},
                'name': {'type': 'string', 'generator': 'product_name'},
                'description': {'type': 'string', 'maxLength': 500},
                'price': {'type': 'number', 'minimum': 0.01, 'maximum': 9999.99},
                'category': {'type': 'string', 'enum': ['electronics', 'clothing', 'food', 'books']},
                'inStock': {'type': 'boolean'},
                'rating': {'type': 'number', 'minimum': 0, 'maximum': 5}
            }
        }
    
    def generate_relational_data(self):
        """Generate data with relationships"""
        return '''
class RelationalDataGenerator:
    def generate_related_entities(self, schema: Dict[str, Any], count: int):
        """Generate related entities maintaining referential integrity"""
        entities = {}
        
        # First pass: generate primary entities
        for entity_name, entity_schema in schema['entities'].items():
            entities[entity_name] = []
            for i in range(count):
                entity = self.generate_entity(entity_schema)
                entity['id'] = f"{entity_name}_{i}"
                entities[entity_name].append(entity)
        
        # Second pass: establish relationships
        for relationship in schema.get('relationships', []):
            self.establish_relationship(entities, relationship)
        
        return entities
    
    def establish_relationship(self, entities: Dict[str, List], relationship: Dict):
        """Establish relationships between entities"""
        source = relationship['source']
        target = relationship['target']
        rel_type = relationship['type']
        
        if rel_type == 'one-to-many':
            for source_entity in entities[source['entity']]:
                # Select random targets
                num_targets = random.randint(1, 5)
                target_refs = random.sample(
                    entities[target['entity']], 
                    min(num_targets, len(entities[target['entity']]))
                )
                source_entity[source['field']] = [t['id'] for t in target_refs]
        
        elif rel_type == 'many-to-one':
            for target_entity in entities[target['entity']]:
                # Select one source
                source_ref = random.choice(entities[source['entity']])
                target_entity[target['field']] = source_ref['id']
'''
```

### 4. Mock Scenarios

Implement scenario-based mocking:

**Scenario Manager**
```python
class ScenarioManager:
    def __init__(self):
        self.scenarios = {}
        self.current_scenario = 'default'
        self.scenario_states = {}
        
    def define_scenario(self, name: str, definition: Dict[str, Any]):
        """Define a mock scenario"""
        self.scenarios[name] = {
            'name': name,
            'description': definition.get('description', ''),
            'initial_state': definition.get('initial_state', {}),
            'stubs': definition.get('stubs', []),
            'sequences': definition.get('sequences', []),
            'conditions': definition.get('conditions', [])
        }
    
    def create_test_scenarios(self):
        """Create common test scenarios"""
        return {
            'happy_path': {
                'description': 'All operations succeed',
                'stubs': [
                    {
                        'path': '/api/auth/login',
                        'response': {
                            'status': 200,
                            'body': {
                                'token': 'valid_token',
                                'user': {'id': '123', 'name': 'Test User'}
                            }
                        }
                    },
                    {
                        'path': '/api/users/{id}',
                        'response': {
                            'status': 200,
                            'body': {
                                'id': '{id}',
                                'name': 'Test User',
                                'email': 'test@example.com'
                            }
                        }
                    }
                ]
            },
            'error_scenario': {
                'description': 'Various error conditions',
                'sequences': [
                    {
                        'name': 'rate_limiting',
                        'steps': [
                            {'repeat': 5, 'response': {'status': 200}},
                            {'repeat': 10, 'response': {'status': 429, 'body': {'error': 'Rate limit exceeded'}}}
                        ]
                    }
                ],
                'stubs': [
                    {
                        'path': '/api/auth/login',
                        'conditions': [
                            {
                                'match': {'body': {'username': 'locked_user'}},
                                'response': {'status': 423, 'body': {'error': 'Account locked'}}
                            }
                        ]
                    }
                ]
            },
            'degraded_performance': {
                'description': 'Slow responses and timeouts',
                'stubs': [
                    {
                        'path': '/api/*',
                        'delay': 5000,  # 5 second delay
                        'response': {'status': 200}
                    }
                ]
            }
        }
    
    def execute_scenario_sequence(self):
        """Execute scenario sequences"""
        return '''
class SequenceExecutor:
    def __init__(self):
        self.sequence_states = {}
        
    def get_sequence_response(self, sequence_name: str, request: Dict):
        """Get response based on sequence state"""
        if sequence_name not in self.sequence_states:
            self.sequence_states[sequence_name] = {'step': 0, 'count': 0}
        
        state = self.sequence_states[sequence_name]
        sequence = self.get_sequence_definition(sequence_name)
        
        # Get current step
        current_step = sequence['steps'][state['step']]
        
        # Check if we should advance to next step
        state['count'] += 1
        if state['count'] >= current_step.get('repeat', 1):
            state['step'] = (state['step'] + 1) % len(sequence['steps'])
            state['count'] = 0
        
        return current_step['response']
    
    def create_stateful_scenario(self):
        """Create scenario with stateful behavior"""
        return {
            'shopping_cart': {
                'initial_state': {
                    'cart': {},
                    'total': 0
                },
                'stubs': [
                    {
                        'method': 'POST',
                        'path': '/api/cart/items',
                        'handler': 'add_to_cart',
                        'modifies_state': True
                    },
                    {
                        'method': 'GET',
                        'path': '/api/cart',
                        'handler': 'get_cart',
                        'uses_state': True
                    }
                ],
                'handlers': {
                    'add_to_cart': lambda state, request: {
                        'state': {
                            **state,
                            'cart': {
                                **state['cart'],
                                request['body']['product_id']: request['body']['quantity']
                            },
                            'total': state['total'] + request['body']['price']
                        },
                        'response': {
                            'status': 201,
                            'body': {'message': 'Item added to cart'}
                        }
                    },
                    'get_cart': lambda state, request: {
                        'response': {
                            'status': 200,
                            'body': {
                                'items': state['cart'],
                                'total': state['total']
                            }
                        }
                    }
                }
            }
        }
'''
```

### 5. Contract Testing

Implement contract-based mocking:

**Contract Testing Framework**
```python
class ContractMockServer:
    def __init__(self):
        self.contracts = {}
        self.validators = self._init_validators()
        
    def load_contract(self, contract_path: str):
        """Load API contract (OpenAPI, AsyncAPI, etc.)"""
        with open(contract_path, 'r') as f:
            contract = yaml.safe_load(f)
        
        # Parse contract
        self.contracts[contract['info']['title']] = {
            'spec': contract,
            'endpoints': self._parse_endpoints(contract),
            'schemas': self._parse_schemas(contract)
        }
    
    def generate_mocks_from_contract(self, contract_name: str):
        """Generate mocks from contract specification"""
        contract = self.contracts[contract_name]
        mocks = []
        
        for path, methods in contract['endpoints'].items():
            for method, spec in methods.items():
                mock = self._create_mock_from_spec(path, method, spec)
                mocks.append(mock)
        
        return mocks
    
    def _create_mock_from_spec(self, path: str, method: str, spec: Dict):
        """Create mock from endpoint specification"""
        mock = {
            'method': method.upper(),
            'path': self._convert_path_to_pattern(path),
            'responses': {}
        }
        
        # Generate responses for each status code
        for status_code, response_spec in spec.get('responses', {}).items():
            mock['responses'][status_code] = {
                'status': int(status_code),
                'headers': self._get_response_headers(response_spec),
                'body': self._generate_response_body(response_spec)
            }
        
        # Add request validation
        if 'requestBody' in spec:
            mock['request_validation'] = self._create_request_validator(spec['requestBody'])
        
        return mock
    
    def validate_against_contract(self):
        """Validate mock responses against contract"""
        return '''
class ContractValidator:
    def validate_response(self, contract_spec, actual_response):
        """Validate response against contract"""
        validation_results = {
            'valid': True,
            'errors': []
        }
        
        # Find response spec for status code
        response_spec = contract_spec['responses'].get(
            str(actual_response['status']),
            contract_spec['responses'].get('default')
        )
        
        if not response_spec:
            validation_results['errors'].append({
                'type': 'unexpected_status',
                'message': f"Status {actual_response['status']} not defined in contract"
            })
            validation_results['valid'] = False
            return validation_results
        
        # Validate headers
        if 'headers' in response_spec:
            header_errors = self.validate_headers(
                response_spec['headers'],
                actual_response['headers']
            )
            validation_results['errors'].extend(header_errors)
        
        # Validate body schema
        if 'content' in response_spec:
            body_errors = self.validate_body(
                response_spec['content'],
                actual_response['body']
            )
            validation_results['errors'].extend(body_errors)
        
        validation_results['valid'] = len(validation_results['errors']) == 0
        return validation_results
    
    def validate_body(self, content_spec, actual_body):
        """Validate response body against schema"""
        errors = []
        
        # Get schema for content type
        schema = content_spec.get('application/json', {}).get('schema')
        if not schema:
            return errors
        
        # Validate against JSON schema
        try:
            validate(instance=actual_body, schema=schema)
        except ValidationError as e:
            errors.append({
                'type': 'schema_validation',
                'path': e.json_path,
                'message': e.message
            })
        
        return errors
'''
```

### 6. Performance Testing

Create performance testing mocks:

**Performance Mock Server**
```python
class PerformanceMockServer:
    def __init__(self):
        self.performance_profiles = {}
        self.metrics_collector = MetricsCollector()
        
    def create_performance_profile(self, name: str, config: Dict):
        """Create performance testing profile"""
        self.performance_profiles[name] = {
            'latency': config.get('latency', {'min': 10, 'max': 100}),
            'throughput': config.get('throughput', 1000),  # requests per second
            'error_rate': config.get('error_rate', 0.01),  # 1% errors
            'response_size': config.get('response_size', {'min': 100, 'max': 10000})
        }
    
    async def simulate_performance(self, profile_name: str, request: Request):
        """Simulate performance characteristics"""
        profile = self.performance_profiles[profile_name]
        
        # Simulate latency
        latency = random.uniform(profile['latency']['min'], profile['latency']['max'])
        await asyncio.sleep(latency / 1000)
        
        # Simulate errors
        if random.random() < profile['error_rate']:
            return self._generate_error_response()
        
        # Generate response with specified size
        response_size = random.randint(
            profile['response_size']['min'],
            profile['response_size']['max']
        )
        
        response_data = self._generate_data_of_size(response_size)
        
        # Track metrics
        self.metrics_collector.record({
            'latency': latency,
            'response_size': response_size,
            'timestamp': datetime.now()
        })
        
        return response_data
    
    def create_load_test_scenarios(self):
        """Create load testing scenarios"""
        return {
            'gradual_load': {
                'description': 'Gradually increase load',
                'stages': [
                    {'duration': 60, 'target_rps': 100},
                    {'duration': 120, 'target_rps': 500},
                    {'duration': 180, 'target_rps': 1000},
                    {'duration': 60, 'target_rps': 100}
                ]
            },
            'spike_test': {
                'description': 'Sudden spike in traffic',
                'stages': [
                    {'duration': 60, 'target_rps': 100},
                    {'duration': 10, 'target_rps': 5000},
                    {'duration': 60, 'target_rps': 100}
                ]
            },
            'stress_test': {
                'description': 'Find breaking point',
                'stages': [
                    {'duration': 60, 'target_rps': 100},
                    {'duration': 60, 'target_rps': 500},
                    {'duration': 60, 'target_rps': 1000},
                    {'duration': 60, 'target_rps': 2000},
                    {'duration': 60, 'target_rps': 5000},
                    {'duration': 60, 'target_rps': 10000}
                ]
            }
        }
    
    def implement_throttling(self):
        """Implement request throttling"""
        return '''
class ThrottlingMiddleware:
    def __init__(self, max_rps: int):
        self.max_rps = max_rps
        self.request_times = deque()
        
    async def __call__(self, request: Request, call_next):
        current_time = time.time()
        
        # Remove old requests
        while self.request_times and self.request_times[0] < current_time - 1:
            self.request_times.popleft()
        
        # Check if we're over limit
        if len(self.request_times) >= self.max_rps:
            return Response(
                content=json.dumps({
                    'error': 'Rate limit exceeded',
                    'retry_after': 1
                }),
                status_code=429,
                headers={'Retry-After': '1'}
            )
        
        # Record this request
        self.request_times.append(current_time)
        
        # Process request
        response = await call_next(request)
        return response
'''
```

### 7. Mock Data Management

Manage mock data effectively:

**Mock Data Store**
```python
class MockDataStore:
    def __init__(self):
        self.collections = {}
        self.indexes = {}
        
    def create_collection(self, name: str, schema: Dict = None):
        """Create a new data collection"""
        self.collections[name] = {
            'data': {},
            'schema': schema,
            'counter': 0
        }
        
        # Create default index on 'id'
        self.create_index(name, 'id')
    
    def insert(self, collection: str, data: Dict):
        """Insert data into collection"""
        collection_data = self.collections[collection]
        
        # Validate against schema if exists
        if collection_data['schema']:
            self._validate_data(data, collection_data['schema'])
        
        # Generate ID if not provided
        if 'id' not in data:
            collection_data['counter'] += 1
            data['id'] = str(collection_data['counter'])
        
        # Store data
        collection_data['data'][data['id']] = data
        
        # Update indexes
        self._update_indexes(collection, data)
        
        return data['id']
    
    def query(self, collection: str, filters: Dict = None):
        """Query collection with filters"""
        collection_data = self.collections[collection]['data']
        
        if not filters:
            return list(collection_data.values())
        
        # Use indexes if available
        if self._can_use_index(collection, filters):
            return self._query_with_index(collection, filters)
        
        # Full scan
        results = []
        for item in collection_data.values():
            if self._matches_filters(item, filters):
                results.append(item)
        
        return results
    
    def create_relationships(self):
        """Define relationships between collections"""
        return '''
class RelationshipManager:
    def __init__(self, data_store: MockDataStore):
        self.store = data_store
        self.relationships = {}
        
    def define_relationship(self, 
                          source_collection: str,
                          target_collection: str,
                          relationship_type: str,
                          foreign_key: str):
        """Define relationship between collections"""
        self.relationships[f"{source_collection}->{target_collection}"] = {
            'type': relationship_type,
            'source': source_collection,
            'target': target_collection,
            'foreign_key': foreign_key
        }
    
    def populate_related_data(self, entity: Dict, collection: str, depth: int = 1):
        """Populate related data for entity"""
        if depth <= 0:
            return entity
        
        # Find relationships for this collection
        for rel_key, rel in self.relationships.items():
            if rel['source'] == collection:
                # Get related data
                foreign_id = entity.get(rel['foreign_key'])
                if foreign_id:
                    related = self.store.get(rel['target'], foreign_id)
                    if related:
                        # Recursively populate
                        related = self.populate_related_data(
                            related, 
                            rel['target'], 
                            depth - 1
                        )
                        entity[rel['target']] = related
        
        return entity
    
    def cascade_operations(self, operation: str, collection: str, entity_id: str):
        """Handle cascade operations"""
        if operation == 'delete':
            # Find dependent relationships
            for rel in self.relationships.values():
                if rel['target'] == collection:
                    # Delete dependent entities
                    dependents = self.store.query(
                        rel['source'],
                        {rel['foreign_key']: entity_id}
                    )
                    for dep in dependents:
                        self.store.delete(rel['source'], dep['id'])
'''
```

### 8. Testing Framework Integration

Integrate with popular testing frameworks:

**Testing Integration**
```python
class TestingFrameworkIntegration:
    def create_jest_integration(self):
        """Jest testing integration"""
        return '''
// jest.mock.config.js
import { MockServer } from './mockServer';

const mockServer = new MockServer();

beforeAll(async () => {
    await mockServer.start({ port: 3001 });
    
    // Load mock definitions
    await mockServer.loadMocks('./mocks/*.json');
    
    // Set default scenario
    await mockServer.setScenario('test');
});

afterAll(async () => {
    await mockServer.stop();
});

beforeEach(async () => {
    // Reset mock state
    await mockServer.reset();
});

// Test helper functions
export const setupMock = async (stub) => {
    return await mockServer.addStub(stub);
};

export const verifyRequests = async (matcher) => {
    const requests = await mockServer.getRequests(matcher);
    return requests;
};

// Example test
describe('User API', () => {
    it('should fetch user details', async () => {
        // Setup mock
        await setupMock({
            method: 'GET',
            path: '/api/users/123',
            response: {
                status: 200,
                body: { id: '123', name: 'Test User' }
            }
        });
        
        // Make request
        const response = await fetch('http://localhost:3001/api/users/123');
        const user = await response.json();
        
        // Verify
        expect(user.name).toBe('Test User');
        
        // Verify mock was called
        const requests = await verifyRequests({ path: '/api/users/123' });
        expect(requests).toHaveLength(1);
    });
});
'''
    
    def create_pytest_integration(self):
        """Pytest integration"""
        return '''
# conftest.py
import pytest
from mock_server import MockServer
import asyncio

@pytest.fixture(scope="session")
def event_loop():
    loop = asyncio.get_event_loop_policy().new_event_loop()
    yield loop
    loop.close()

@pytest.fixture(scope="session")
async def mock_server(event_loop):
    server = MockServer()
    await server.start(port=3001)
    yield server
    await server.stop()

@pytest.fixture(autouse=True)
async def reset_mocks(mock_server):
    await mock_server.reset()
    yield
    # Verify no unexpected calls
    unmatched = await mock_server.get_unmatched_requests()
    assert len(unmatched) == 0, f"Unmatched requests: {unmatched}"

# Test utilities
class MockBuilder:
    def __init__(self, mock_server):
        self.server = mock_server
        self.stubs = []
    
    def when(self, method, path):
        self.current_stub = {
            'method': method,
            'path': path
        }
        return self
    
    def with_body(self, body):
        self.current_stub['body'] = body
        return self
    
    def then_return(self, status, body=None, headers=None):
        self.current_stub['response'] = {
            'status': status,
            'body': body,
            'headers': headers or {}
        }
        self.stubs.append(self.current_stub)
        return self
    
    async def setup(self):
        for stub in self.stubs:
            await self.server.add_stub(stub)

# Example test
@pytest.mark.asyncio
async def test_user_creation(mock_server):
    # Setup mocks
    mock = MockBuilder(mock_server)
    mock.when('POST', '/api/users') \
        .with_body({'name': 'New User'}) \
        .then_return(201, {'id': '456', 'name': 'New User'})
    
    await mock.setup()
    
    # Test code here
    response = await create_user({'name': 'New User'})
    assert response['id'] == '456'
'''
```

### 9. Mock Server Deployment

Deploy mock servers:

**Deployment Configuration**
```yaml
# docker-compose.yml for mock services
version: '3.8'

services:
  mock-api:
    build:
      context: .
      dockerfile: Dockerfile.mock
    ports:
      - "3001:3001"
    environment:
      - MOCK_SCENARIO=production
      - MOCK_DATA_PATH=/data/mocks
    volumes:
      - ./mocks:/data/mocks
      - ./scenarios:/data/scenarios
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3001/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  mock-admin:
    build:
      context: .
      dockerfile: Dockerfile.admin
    ports:
      - "3002:3002"
    environment:
      - MOCK_SERVER_URL=http://mock-api:3001
    depends_on:
      - mock-api

# Kubernetes deployment
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mock-server
spec:
  replicas: 2
  selector:
    matchLabels:
      app: mock-server
  template:
    metadata:
      labels:
        app: mock-server
    spec:
      containers:
      - name: mock-server
        image: mock-server:latest
        ports:
        - containerPort: 3001
        env:
        - name: MOCK_SCENARIO
          valueFrom:
            configMapKeyRef:
              name: mock-config
              key: scenario
        volumeMounts:
        - name: mock-definitions
          mountPath: /data/mocks
      volumes:
      - name: mock-definitions
        configMap:
          name: mock-definitions
```

### 10. Mock Documentation

Generate mock API documentation:

**Documentation Generator**
```python
class MockDocumentationGenerator:
    def generate_documentation(self, mock_server):
        """Generate comprehensive mock documentation"""
        return f"""
# Mock API Documentation

## Overview
{self._generate_overview(mock_server)}

## Available Endpoints
{self._generate_endpoints_doc(mock_server)}

## Scenarios
{self._generate_scenarios_doc(mock_server)}

## Data Models
{self._generate_models_doc(mock_server)}

## Usage Examples
{self._generate_examples(mock_server)}

## Configuration
{self._generate_config_doc(mock_server)}
"""
    
    def _generate_endpoints_doc(self, mock_server):
        """Generate endpoint documentation"""
        doc = ""
        for endpoint in mock_server.get_endpoints():
            doc += f"""
### {endpoint['method']} {endpoint['path']}

**Description**: {endpoint.get('description', 'No description')}

**Request**:
```json
{json.dumps(endpoint.get('request_example', {}), indent=2)}
```

**Response**:
```json
{json.dumps(endpoint.get('response_example', {}), indent=2)}
```

**Scenarios**:
{self._format_endpoint_scenarios(endpoint)}
"""
        return doc
    
    def create_interactive_docs(self):
        """Create interactive API documentation"""
        return '''
<!DOCTYPE html>
<html>
<head>
    <title>Mock API Interactive Documentation</title>
    <script src="https://unpkg.com/swagger-ui-dist/swagger-ui-bundle.js"></script>
    <link rel="stylesheet" href="https://unpkg.com/swagger-ui-dist/swagger-ui.css">
</head>
<body>
    <div id="swagger-ui"></div>
    <script>
        window.onload = function() {
            const ui = SwaggerUIBundle({
                url: "/api/mock/openapi.json",
                dom_id: '#swagger-ui',
                presets: [
                    SwaggerUIBundle.presets.apis,
                    SwaggerUIBundle.SwaggerUIStandalonePreset
                ],
                layout: "BaseLayout",
                tryItOutEnabled: true,
                requestInterceptor: (request) => {
                    request.headers['X-Mock-Scenario'] = 
                        document.getElementById('scenario-select').value;
                    return request;
                }
            });
        }
    </script>
    
    <div class="scenario-selector">
        <label>Scenario:</label>
        <select id="scenario-select">
            <option value="default">Default</option>
            <option value="error">Error Conditions</option>
            <option value="slow">Slow Responses</option>
        </select>
    </div>
</body>
</html>
'''
```

## Output Format

1. **Mock Server Setup**: Complete mock server implementation
2. **Stubbing Configuration**: Flexible request/response stubbing
3. **Data Generation**: Realistic mock data generation
4. **Scenario Definitions**: Comprehensive test scenarios
5. **Contract Testing**: Contract-based mock validation
6. **Performance Simulation**: Performance testing capabilities
7. **Data Management**: Mock data storage and relationships
8. **Testing Integration**: Framework integration examples
9. **Deployment Guide**: Mock server deployment configurations
10. **Documentation**: Auto-generated mock API documentation

Focus on creating flexible, realistic mock services that enable efficient development, thorough testing, and reliable API simulation for all stages of the development lifecycle.
````

## File: tools/api-scaffold.md

````markdown
# API Scaffold Generator

You are an API development expert specializing in creating production-ready, scalable REST APIs with modern frameworks. Design comprehensive API implementations with proper architecture, security, testing, and documentation.

## Context
The user needs to create a new API endpoint or service with complete implementation including models, validation, security, testing, and deployment configuration. Focus on production-ready code that follows industry best practices.

## Requirements
$ARGUMENTS

## Instructions

### 1. API Framework Selection

Choose the appropriate framework based on requirements:

**Framework Comparison Matrix**
```python
def select_framework(requirements):
    """Select optimal API framework based on requirements"""
    
    frameworks = {
        'fastapi': {
            'best_for': ['high_performance', 'async_operations', 'type_safety', 'modern_python'],
            'strengths': ['Auto OpenAPI docs', 'Type hints', 'Async support', 'Fast performance'],
            'use_cases': ['Microservices', 'Data APIs', 'ML APIs', 'Real-time systems'],
            'example_stack': 'FastAPI + Pydantic + SQLAlchemy + PostgreSQL'
        },
        'django_rest': {
            'best_for': ['rapid_development', 'orm_integration', 'admin_interface', 'large_teams'],
            'strengths': ['Batteries included', 'ORM', 'Admin panel', 'Mature ecosystem'],
            'use_cases': ['CRUD applications', 'Content management', 'Enterprise systems'],
            'example_stack': 'Django + DRF + PostgreSQL + Redis'
        },
        'express': {
            'best_for': ['node_ecosystem', 'real_time', 'frontend_integration', 'javascript_teams'],
            'strengths': ['NPM ecosystem', 'JSON handling', 'WebSocket support', 'Fast development'],
            'use_cases': ['Real-time apps', 'API gateways', 'Serverless functions'],
            'example_stack': 'Express + TypeScript + Prisma + PostgreSQL'
        },
        'spring_boot': {
            'best_for': ['enterprise', 'java_teams', 'complex_business_logic', 'microservices'],
            'strengths': ['Enterprise features', 'Dependency injection', 'Security', 'Monitoring'],
            'use_cases': ['Enterprise APIs', 'Financial systems', 'Complex microservices'],
            'example_stack': 'Spring Boot + JPA + PostgreSQL + Redis'
        }
    }
    
    # Selection logic based on requirements
    if 'high_performance' in requirements:
        return frameworks['fastapi']
    elif 'enterprise' in requirements:
        return frameworks['spring_boot']
    elif 'rapid_development' in requirements:
        return frameworks['django_rest']
    elif 'real_time' in requirements:
        return frameworks['express']
    
    return frameworks['fastapi']  # Default recommendation
```

### 2. FastAPI Implementation

Complete FastAPI API implementation:

**Project Structure**
```
project/
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ main.py
‚îÇ   ‚îú‚îÄ‚îÄ core/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ config.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ security.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ database.py
‚îÇ   ‚îú‚îÄ‚îÄ api/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ deps.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ v1/
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ endpoints/
‚îÇ   ‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ users.py
‚îÇ   ‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ items.py
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ api.py
‚îÇ   ‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ user.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ item.py
‚îÇ   ‚îú‚îÄ‚îÄ schemas/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ user.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ item.py
‚îÇ   ‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ user_service.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ item_service.py
‚îÇ   ‚îî‚îÄ‚îÄ tests/
‚îÇ       ‚îú‚îÄ‚îÄ conftest.py
‚îÇ       ‚îú‚îÄ‚îÄ test_users.py
‚îÇ       ‚îî‚îÄ‚îÄ test_items.py
‚îú‚îÄ‚îÄ alembic/
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ Dockerfile
‚îî‚îÄ‚îÄ docker-compose.yml
```

**Core Configuration**
```python
# app/core/config.py
from pydantic import BaseSettings, validator
from typing import Optional, Dict, Any
import secrets

class Settings(BaseSettings):
    API_V1_STR: str = "/api/v1"
    SECRET_KEY: str = secrets.token_urlsafe(32)
    ACCESS_TOKEN_EXPIRE_MINUTES: int = 60 * 24 * 8  # 8 days
    SERVER_NAME: str = "localhost"
    SERVER_HOST: str = "0.0.0.0"
    
    # Database
    POSTGRES_SERVER: str = "localhost"
    POSTGRES_USER: str = "postgres"
    POSTGRES_PASSWORD: str = ""
    POSTGRES_DB: str = "app"
    DATABASE_URL: Optional[str] = None
    
    @validator("DATABASE_URL", pre=True)
    def assemble_db_connection(cls, v: Optional[str], values: Dict[str, Any]) -> Any:
        if isinstance(v, str):
            return v
        return f"postgresql://{values.get('POSTGRES_USER')}:{values.get('POSTGRES_PASSWORD')}@{values.get('POSTGRES_SERVER')}/{values.get('POSTGRES_DB')}"
    
    # Redis
    REDIS_URL: str = "redis://localhost:6379"
    
    # Security
    BACKEND_CORS_ORIGINS: list = ["http://localhost:3000", "http://localhost:8000"]
    
    # Rate Limiting
    RATE_LIMIT_REQUESTS: int = 100
    RATE_LIMIT_WINDOW: int = 60
    
    # Monitoring
    SENTRY_DSN: Optional[str] = None
    LOG_LEVEL: str = "INFO"
    
    class Config:
        env_file = ".env"

settings = Settings()
```

**Database Setup**
```python
# app/core/database.py
from sqlalchemy import create_engine
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker
from sqlalchemy.pool import StaticPool
import redis

from app.core.config import settings

# PostgreSQL
engine = create_engine(
    settings.DATABASE_URL,
    poolclass=StaticPool,
    pool_size=20,
    max_overflow=30,
    pool_pre_ping=True,
    echo=settings.LOG_LEVEL == "DEBUG"
)

SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)
Base = declarative_base()

# Redis
redis_client = redis.from_url(settings.REDIS_URL, decode_responses=True)

def get_db():
    """Dependency to get database session"""
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()

def get_redis():
    """Dependency to get Redis connection"""
    return redis_client
```

**Security Implementation**
```python
# app/core/security.py
from datetime import datetime, timedelta
from typing import Optional
import jwt
from passlib.context import CryptContext
from fastapi import HTTPException, status
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials

from app.core.config import settings

pwd_context = CryptContext(schemes=["bcrypt"], deprecated="auto")
security = HTTPBearer()

def create_access_token(data: dict, expires_delta: Optional[timedelta] = None):
    """Create JWT access token"""
    to_encode = data.copy()
    if expires_delta:
        expire = datetime.utcnow() + expires_delta
    else:
        expire = datetime.utcnow() + timedelta(minutes=settings.ACCESS_TOKEN_EXPIRE_MINUTES)
    
    to_encode.update({"exp": expire})
    encoded_jwt = jwt.encode(to_encode, settings.SECRET_KEY, algorithm="HS256")
    return encoded_jwt

def verify_token(credentials: HTTPAuthorizationCredentials) -> dict:
    """Verify JWT token"""
    try:
        payload = jwt.decode(
            credentials.credentials, 
            settings.SECRET_KEY, 
            algorithms=["HS256"]
        )
        username: str = payload.get("sub")
        if username is None:
            raise HTTPException(
                status_code=status.HTTP_401_UNAUTHORIZED,
                detail="Could not validate credentials",
                headers={"WWW-Authenticate": "Bearer"},
            )
        return payload
    except jwt.PyJWTError:
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Could not validate credentials",
            headers={"WWW-Authenticate": "Bearer"},
        )

def verify_password(plain_password: str, hashed_password: str) -> bool:
    """Verify password against hash"""
    return pwd_context.verify(plain_password, hashed_password)

def get_password_hash(password: str) -> str:
    """Generate password hash"""
    return pwd_context.hash(password)
```

**Models Implementation**
```python
# app/models/user.py
from sqlalchemy import Column, Integer, String, Boolean, DateTime, Text
from sqlalchemy.sql import func
from sqlalchemy.orm import relationship

from app.core.database import Base

class User(Base):
    __tablename__ = "users"
    
    id = Column(Integer, primary_key=True, index=True)
    email = Column(String, unique=True, index=True, nullable=False)
    username = Column(String, unique=True, index=True, nullable=False)
    hashed_password = Column(String, nullable=False)
    full_name = Column(String)
    is_active = Column(Boolean, default=True)
    is_superuser = Column(Boolean, default=False)
    created_at = Column(DateTime(timezone=True), server_default=func.now())
    updated_at = Column(DateTime(timezone=True), onupdate=func.now())
    
    # Relationships
    items = relationship("Item", back_populates="owner")

# app/models/item.py
from sqlalchemy import Column, Integer, String, Boolean, DateTime, Text, ForeignKey
from sqlalchemy.sql import func
from sqlalchemy.orm import relationship

from app.core.database import Base

class Item(Base):
    __tablename__ = "items"
    
    id = Column(Integer, primary_key=True, index=True)
    title = Column(String, index=True, nullable=False)
    description = Column(Text)
    is_active = Column(Boolean, default=True)
    owner_id = Column(Integer, ForeignKey("users.id"))
    created_at = Column(DateTime(timezone=True), server_default=func.now())
    updated_at = Column(DateTime(timezone=True), onupdate=func.now())
    
    # Relationships
    owner = relationship("User", back_populates="items")
```

**Pydantic Schemas**
```python
# app/schemas/user.py
from pydantic import BaseModel, EmailStr, validator
from typing import Optional
from datetime import datetime

class UserBase(BaseModel):
    email: EmailStr
    username: str
    full_name: Optional[str] = None

class UserCreate(UserBase):
    password: str
    
    @validator('password')
    def validate_password(cls, v):
        if len(v) < 8:
            raise ValueError('Password must be at least 8 characters')
        return v

class UserUpdate(BaseModel):
    email: Optional[EmailStr] = None
    username: Optional[str] = None
    full_name: Optional[str] = None
    is_active: Optional[bool] = None

class UserInDB(UserBase):
    id: int
    is_active: bool
    is_superuser: bool
    created_at: datetime
    updated_at: Optional[datetime]
    
    class Config:
        orm_mode = True

class User(UserInDB):
    pass

# app/schemas/item.py
from pydantic import BaseModel, validator
from typing import Optional
from datetime import datetime

class ItemBase(BaseModel):
    title: str
    description: Optional[str] = None

class ItemCreate(ItemBase):
    @validator('title')
    def validate_title(cls, v):
        if len(v.strip()) < 3:
            raise ValueError('Title must be at least 3 characters')
        return v.strip()

class ItemUpdate(BaseModel):
    title: Optional[str] = None
    description: Optional[str] = None
    is_active: Optional[bool] = None

class ItemInDB(ItemBase):
    id: int
    is_active: bool
    owner_id: int
    created_at: datetime
    updated_at: Optional[datetime]
    
    class Config:
        orm_mode = True

class Item(ItemInDB):
    pass
```

**Service Layer**
```python
# app/services/user_service.py
from typing import Optional, List
from sqlalchemy.orm import Session
from fastapi import HTTPException, status

from app.models.user import User
from app.schemas.user import UserCreate, UserUpdate
from app.core.security import get_password_hash, verify_password

class UserService:
    def __init__(self, db: Session):
        self.db = db
    
    def get_user(self, user_id: int) -> Optional[User]:
        """Get user by ID"""
        return self.db.query(User).filter(User.id == user_id).first()
    
    def get_user_by_email(self, email: str) -> Optional[User]:
        """Get user by email"""
        return self.db.query(User).filter(User.email == email).first()
    
    def get_users(self, skip: int = 0, limit: int = 100) -> List[User]:
        """Get list of users"""
        return self.db.query(User).offset(skip).limit(limit).all()
    
    def create_user(self, user_create: UserCreate) -> User:
        """Create new user"""
        # Check if user exists
        if self.get_user_by_email(user_create.email):
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="Email already registered"
            )
        
        # Create user
        hashed_password = get_password_hash(user_create.password)
        db_user = User(
            email=user_create.email,
            username=user_create.username,
            full_name=user_create.full_name,
            hashed_password=hashed_password
        )
        self.db.add(db_user)
        self.db.commit()
        self.db.refresh(db_user)
        return db_user
    
    def update_user(self, user_id: int, user_update: UserUpdate) -> User:
        """Update user"""
        db_user = self.get_user(user_id)
        if not db_user:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="User not found"
            )
        
        update_data = user_update.dict(exclude_unset=True)
        for field, value in update_data.items():
            setattr(db_user, field, value)
        
        self.db.commit()
        self.db.refresh(db_user)
        return db_user
    
    def authenticate_user(self, email: str, password: str) -> Optional[User]:
        """Authenticate user"""
        user = self.get_user_by_email(email)
        if not user or not verify_password(password, user.hashed_password):
            return None
        return user
```

**Rate Limiting Middleware**
```python
# app/core/rate_limiting.py
import time
from typing import Callable
from fastapi import Request, HTTPException, status
from fastapi.responses import JSONResponse
import redis

from app.core.config import settings
from app.core.database import get_redis

class RateLimiter:
    def __init__(self, redis_client: redis.Redis):
        self.redis = redis_client
        self.requests = settings.RATE_LIMIT_REQUESTS
        self.window = settings.RATE_LIMIT_WINDOW
    
    async def __call__(self, request: Request, call_next: Callable):
        # Get client identifier
        client_ip = request.client.host
        user_id = getattr(request.state, 'user_id', None)
        key = f"rate_limit:{user_id or client_ip}"
        
        # Check rate limit
        current = self.redis.get(key)
        if current is None:
            # First request in window
            self.redis.setex(key, self.window, 1)
        else:
            current = int(current)
            if current >= self.requests:
                raise HTTPException(
                    status_code=status.HTTP_429_TOO_MANY_REQUESTS,
                    detail=f"Rate limit exceeded. Try again in {self.redis.ttl(key)} seconds",
                    headers={"Retry-After": str(self.redis.ttl(key))}
                )
            self.redis.incr(key)
        
        response = await call_next(request)
        
        # Add rate limit headers
        remaining = max(0, self.requests - int(self.redis.get(key) or 0))
        response.headers["X-RateLimit-Limit"] = str(self.requests)
        response.headers["X-RateLimit-Remaining"] = str(remaining)
        response.headers["X-RateLimit-Reset"] = str(int(time.time()) + self.redis.ttl(key))
        
        return response
```

**API Endpoints**
```python
# app/api/v1/endpoints/users.py
from typing import List
from fastapi import APIRouter, Depends, HTTPException, status
from sqlalchemy.orm import Session

from app.core.database import get_db
from app.core.security import verify_token, security
from app.schemas.user import User, UserCreate, UserUpdate
from app.services.user_service import UserService

router = APIRouter()

@router.post("/", response_model=User, status_code=status.HTTP_201_CREATED)
async def create_user(
    user_create: UserCreate,
    db: Session = Depends(get_db)
):
    """Create new user"""
    service = UserService(db)
    return service.create_user(user_create)

@router.get("/me", response_model=User)
async def get_current_user(
    token: dict = Depends(verify_token),
    db: Session = Depends(get_db)
):
    """Get current user profile"""
    service = UserService(db)
    user = service.get_user_by_email(token["sub"])
    if not user:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="User not found"
        )
    return user

@router.get("/{user_id}", response_model=User)
async def get_user(
    user_id: int,
    db: Session = Depends(get_db),
    token: dict = Depends(verify_token)
):
    """Get user by ID"""
    service = UserService(db)
    user = service.get_user(user_id)
    if not user:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="User not found"
        )
    return user

@router.put("/{user_id}", response_model=User)
async def update_user(
    user_id: int,
    user_update: UserUpdate,
    db: Session = Depends(get_db),
    token: dict = Depends(verify_token)
):
    """Update user"""
    service = UserService(db)
    return service.update_user(user_id, user_update)

@router.get("/", response_model=List[User])
async def list_users(
    skip: int = 0,
    limit: int = 100,
    db: Session = Depends(get_db),
    token: dict = Depends(verify_token)
):
    """List users"""
    service = UserService(db)
    return service.get_users(skip=skip, limit=limit)
```

**Main Application**
```python
# app/main.py
from fastapi import FastAPI, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.middleware.trustedhost import TrustedHostMiddleware
from fastapi.responses import JSONResponse
import logging
import time
import uuid

from app.core.config import settings
from app.core.rate_limiting import RateLimiter
from app.core.database import get_redis
from app.api.v1.api import api_router

# Configure logging
logging.basicConfig(
    level=getattr(logging, settings.LOG_LEVEL),
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

# Create FastAPI app
app = FastAPI(
    title="Production API",
    description="A production-ready API with FastAPI",
    version="1.0.0",
    openapi_url=f"{settings.API_V1_STR}/openapi.json"
)

# Middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=settings.BACKEND_CORS_ORIGINS,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

app.add_middleware(
    TrustedHostMiddleware,
    allowed_hosts=["localhost", "127.0.0.1", settings.SERVER_NAME]
)

# Rate limiting middleware
rate_limiter = RateLimiter(get_redis())
app.middleware("http")(rate_limiter)

@app.middleware("http")
async def add_process_time_header(request: Request, call_next):
    """Add processing time and correlation ID"""
    correlation_id = str(uuid.uuid4())
    request.state.correlation_id = correlation_id
    
    start_time = time.time()
    response = await call_next(request)
    process_time = time.time() - start_time
    
    response.headers["X-Process-Time"] = str(process_time)
    response.headers["X-Correlation-ID"] = correlation_id
    return response

@app.exception_handler(Exception)
async def global_exception_handler(request: Request, exc: Exception):
    """Global exception handler"""
    correlation_id = getattr(request.state, 'correlation_id', 'unknown')
    
    logger.error(
        f"Unhandled exception: {exc}",
        extra={"correlation_id": correlation_id, "path": request.url.path}
    )
    
    return JSONResponse(
        status_code=500,
        content={
            "detail": "Internal server error",
            "correlation_id": correlation_id
        }
    )

# Include routers
app.include_router(api_router, prefix=settings.API_V1_STR)

@app.get("/health")
async def health_check():
    """Health check endpoint"""
    return {"status": "healthy", "timestamp": time.time()}

@app.get("/")
async def root():
    """Root endpoint"""
    return {"message": "Welcome to the Production API"}

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(
        "app.main:app",
        host=settings.SERVER_HOST,
        port=8000,
        reload=True
    )
```

### 3. Express.js Implementation

Complete Express.js TypeScript implementation:

**Project Structure & Setup**
```typescript
// package.json
{
  "name": "express-api",
  "version": "1.0.0",
  "scripts": {
    "dev": "nodemon src/index.ts",
    "build": "tsc",
    "start": "node dist/index.js",
    "test": "jest",
    "test:watch": "jest --watch"
  },
  "dependencies": {
    "express": "^4.18.2",
    "express-rate-limit": "^6.10.0",
    "helmet": "^7.0.0",
    "cors": "^2.8.5",
    "compression": "^1.7.4",
    "morgan": "^1.10.0",
    "joi": "^17.9.2",
    "jsonwebtoken": "^9.0.2",
    "bcryptjs": "^2.4.3",
    "prisma": "^5.1.0",
    "@prisma/client": "^5.1.0",
    "redis": "^4.6.7",
    "winston": "^3.10.0"
  },
  "devDependencies": {
    "@types/express": "^4.17.17",
    "@types/node": "^20.4.5",
    "typescript": "^5.1.6",
    "nodemon": "^3.0.1",
    "jest": "^29.6.1",
    "@types/jest": "^29.5.3",
    "supertest": "^6.3.3"
  }
}

// src/types/index.ts
export interface User {
  id: string;
  email: string;
  username: string;
  fullName?: string;
  isActive: boolean;
  createdAt: Date;
  updatedAt: Date;
}

export interface CreateUserRequest {
  email: string;
  username: string;
  password: string;
  fullName?: string;
}

export interface AuthTokenPayload {
  userId: string;
  email: string;
  iat: number;
  exp: number;
}

// src/config/index.ts
import { config as dotenvConfig } from 'dotenv';

dotenvConfig();

export const config = {
  port: parseInt(process.env.PORT || '3000'),
  jwtSecret: process.env.JWT_SECRET || 'your-secret-key',
  jwtExpiresIn: process.env.JWT_EXPIRES_IN || '7d',
  redisUrl: process.env.REDIS_URL || 'redis://localhost:6379',
  databaseUrl: process.env.DATABASE_URL || 'postgresql://user:pass@localhost:5432/db',
  nodeEnv: process.env.NODE_ENV || 'development',
  corsOrigins: process.env.CORS_ORIGINS?.split(',') || ['http://localhost:3000'],
  rateLimitMax: parseInt(process.env.RATE_LIMIT_MAX || '100'),
  rateLimitWindow: parseInt(process.env.RATE_LIMIT_WINDOW_MS || '900000'), // 15 minutes
};

// src/middleware/validation.ts
import { Request, Response, NextFunction } from 'express';
import Joi from 'joi';

export const validateRequest = (schema: Joi.ObjectSchema) => {
  return (req: Request, res: Response, next: NextFunction) => {
    const { error } = schema.validate(req.body);
    
    if (error) {
      return res.status(400).json({
        success: false,
        message: 'Validation error',
        details: error.details.map(detail => ({
          field: detail.path.join('.'),
          message: detail.message
        }))
      });
    }
    
    next();
  };
};

// Validation schemas
export const userSchemas = {
  create: Joi.object({
    email: Joi.string().email().required(),
    username: Joi.string().alphanum().min(3).max(30).required(),
    password: Joi.string().min(8).required(),
    fullName: Joi.string().max(100).optional()
  }),
  
  update: Joi.object({
    email: Joi.string().email().optional(),
    username: Joi.string().alphanum().min(3).max(30).optional(),
    fullName: Joi.string().max(100).optional(),
    isActive: Joi.boolean().optional()
  })
};

// src/middleware/auth.ts
import { Request, Response, NextFunction } from 'express';
import jwt from 'jsonwebtoken';
import { config } from '../config';
import { AuthTokenPayload } from '../types';

declare global {
  namespace Express {
    interface Request {
      user?: AuthTokenPayload;
    }
  }
}

export const authenticateToken = (req: Request, res: Response, next: NextFunction) => {
  const authHeader = req.headers.authorization;
  const token = authHeader && authHeader.split(' ')[1];

  if (!token) {
    return res.status(401).json({
      success: false,
      message: 'Access token required'
    });
  }

  try {
    const decoded = jwt.verify(token, config.jwtSecret) as AuthTokenPayload;
    req.user = decoded;
    next();
  } catch (error) {
    return res.status(403).json({
      success: false,
      message: 'Invalid or expired token'
    });
  }
};

// src/services/userService.ts
import { PrismaClient } from '@prisma/client';
import bcrypt from 'bcryptjs';
import jwt from 'jsonwebtoken';
import { config } from '../config';
import { CreateUserRequest, User } from '../types';

const prisma = new PrismaClient();

export class UserService {
  async createUser(userData: CreateUserRequest): Promise<User> {
    // Check if user exists
    const existingUser = await prisma.user.findFirst({
      where: {
        OR: [
          { email: userData.email },
          { username: userData.username }
        ]
      }
    });

    if (existingUser) {
      throw new Error('User with this email or username already exists');
    }

    // Hash password
    const hashedPassword = await bcrypt.hash(userData.password, 12);

    // Create user
    const user = await prisma.user.create({
      data: {
        email: userData.email,
        username: userData.username,
        fullName: userData.fullName,
        hashedPassword,
        isActive: true
      }
    });

    // Remove password from response
    const { hashedPassword: _, ...userWithoutPassword } = user;
    return userWithoutPassword as User;
  }

  async getUserById(id: string): Promise<User | null> {
    const user = await prisma.user.findUnique({
      where: { id },
      select: {
        id: true,
        email: true,
        username: true,
        fullName: true,
        isActive: true,
        createdAt: true,
        updatedAt: true
      }
    });

    return user;
  }

  async authenticateUser(email: string, password: string): Promise<string | null> {
    const user = await prisma.user.findUnique({
      where: { email }
    });

    if (!user || !await bcrypt.compare(password, user.hashedPassword)) {
      return null;
    }

    const token = jwt.sign(
      { userId: user.id, email: user.email },
      config.jwtSecret,
      { expiresIn: config.jwtExpiresIn }
    );

    return token;
  }

  async getUsers(skip = 0, take = 10): Promise<User[]> {
    return await prisma.user.findMany({
      skip,
      take,
      select: {
        id: true,
        email: true,
        username: true,
        fullName: true,
        isActive: true,
        createdAt: true,
        updatedAt: true
      }
    });
  }
}

// src/controllers/userController.ts
import { Request, Response } from 'express';
import { UserService } from '../services/userService';
import { logger } from '../utils/logger';

const userService = new UserService();

export class UserController {
  async createUser(req: Request, res: Response) {
    try {
      const user = await userService.createUser(req.body);
      
      logger.info('User created successfully', { userId: user.id });
      
      res.status(201).json({
        success: true,
        message: 'User created successfully',
        data: user
      });
    } catch (error) {
      logger.error('Error creating user', { error: error.message });
      
      res.status(400).json({
        success: false,
        message: error.message || 'Failed to create user'
      });
    }
  }

  async getUser(req: Request, res: Response) {
    try {
      const { id } = req.params;
      const user = await userService.getUserById(id);

      if (!user) {
        return res.status(404).json({
          success: false,
          message: 'User not found'
        });
      }

      res.json({
        success: true,
        data: user
      });
    } catch (error) {
      logger.error('Error fetching user', { error: error.message });
      
      res.status(500).json({
        success: false,
        message: 'Internal server error'
      });
    }
  }

  async getCurrentUser(req: Request, res: Response) {
    try {
      const user = await userService.getUserById(req.user!.userId);

      if (!user) {
        return res.status(404).json({
          success: false,
          message: 'User not found'
        });
      }

      res.json({
        success: true,
        data: user
      });
    } catch (error) {
      logger.error('Error fetching current user', { error: error.message });
      
      res.status(500).json({
        success: false,
        message: 'Internal server error'
      });
    }
  }

  async loginUser(req: Request, res: Response) {
    try {
      const { email, password } = req.body;
      const token = await userService.authenticateUser(email, password);

      if (!token) {
        return res.status(401).json({
          success: false,
          message: 'Invalid email or password'
        });
      }

      res.json({
        success: true,
        message: 'Login successful',
        data: { token }
      });
    } catch (error) {
      logger.error('Error during login', { error: error.message });
      
      res.status(500).json({
        success: false,
        message: 'Internal server error'
      });
    }
  }
}
```

### 4. Testing Implementation

Comprehensive testing setup:

**FastAPI Tests**
```python
# tests/conftest.py
import pytest
from fastapi.testclient import TestClient
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker
from sqlalchemy.pool import StaticPool

from app.main import app
from app.core.database import Base, get_db
from app.core.config import settings

# Test database
SQLALCHEMY_DATABASE_URL = "sqlite:///./test.db"
engine = create_engine(
    SQLALCHEMY_DATABASE_URL,
    connect_args={"check_same_thread": False},
    poolclass=StaticPool,
)
TestingSessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)

def override_get_db():
    try:
        db = TestingSessionLocal()
        yield db
    finally:
        db.close()

app.dependency_overrides[get_db] = override_get_db

@pytest.fixture(scope="session")
def db_engine():
    Base.metadata.create_all(bind=engine)
    yield engine
    Base.metadata.drop_all(bind=engine)

@pytest.fixture(scope="function")
def db_session(db_engine):
    connection = db_engine.connect()
    transaction = connection.begin()
    session = TestingSessionLocal(bind=connection)
    
    yield session
    
    session.close()
    transaction.rollback()
    connection.close()

@pytest.fixture(scope="module")
def client():
    with TestClient(app) as test_client:
        yield test_client

# tests/test_users.py
import pytest
from fastapi.testclient import TestClient

def test_create_user(client: TestClient):
    """Test user creation"""
    user_data = {
        "email": "test@example.com",
        "username": "testuser",
        "password": "testpassword123",
        "full_name": "Test User"
    }
    
    response = client.post("/api/v1/users/", json=user_data)
    assert response.status_code == 201
    
    data = response.json()
    assert data["email"] == user_data["email"]
    assert data["username"] == user_data["username"]
    assert "id" in data
    assert "hashed_password" not in data

def test_create_user_duplicate_email(client: TestClient):
    """Test creating user with duplicate email"""
    user_data = {
        "email": "duplicate@example.com",
        "username": "user1",
        "password": "password123"
    }
    
    # Create first user
    response1 = client.post("/api/v1/users/", json=user_data)
    assert response1.status_code == 201
    
    # Try to create second user with same email
    user_data["username"] = "user2"
    response2 = client.post("/api/v1/users/", json=user_data)
    assert response2.status_code == 400
    assert "already registered" in response2.json()["detail"]

def test_get_user_unauthorized(client: TestClient):
    """Test accessing protected endpoint without token"""
    response = client.get("/api/v1/users/me")
    assert response.status_code == 401

@pytest.mark.asyncio
async def test_user_authentication_flow(client: TestClient):
    """Test complete authentication flow"""
    # Create user
    user_data = {
        "email": "auth@example.com",
        "username": "authuser",
        "password": "authpassword123"
    }
    
    create_response = client.post("/api/v1/users/", json=user_data)
    assert create_response.status_code == 201
    
    # Login
    login_data = {
        "email": user_data["email"],
        "password": user_data["password"]
    }
    login_response = client.post("/api/v1/auth/login", json=login_data)
    assert login_response.status_code == 200
    
    token = login_response.json()["access_token"]
    
    # Access protected endpoint
    headers = {"Authorization": f"Bearer {token}"}
    profile_response = client.get("/api/v1/users/me", headers=headers)
    assert profile_response.status_code == 200
    
    profile_data = profile_response.json()
    assert profile_data["email"] == user_data["email"]
```

### 5. Deployment Configuration

**Docker Configuration**
```dockerfile
# Dockerfile (FastAPI)
FROM python:3.11-slim

WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y \
    gcc \
    && rm -rf /var/lib/apt/lists/*

# Install Python dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy application
COPY . .

# Create non-root user
RUN adduser --disabled-password --gecos '' appuser
RUN chown -R appuser:appuser /app
USER appuser

# Health check
HEALTHCHECK --interval=30s --timeout=30s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:8000/health || exit 1

EXPOSE 8000

CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000"]
```

**Docker Compose**
```yaml
# docker-compose.yml
version: '3.8'

services:
  api:
    build: .
    ports:
      - "8000:8000"
    environment:
      - DATABASE_URL=postgresql://postgres:password@db:5432/appdb
      - REDIS_URL=redis://redis:6379
    depends_on:
      - db
      - redis
    volumes:
      - ./app:/app/app
    restart: unless-stopped

  db:
    image: postgres:15
    environment:
      - POSTGRES_DB=appdb
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=password
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./init.sql:/docker-entrypoint-initdb.d/init.sql
    ports:
      - "5432:5432"
    restart: unless-stopped

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    restart: unless-stopped

  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf
      - ./ssl:/etc/nginx/ssl
    depends_on:
      - api
    restart: unless-stopped

volumes:
  postgres_data:
  redis_data:
```

### 6. CI/CD Pipeline

**GitHub Actions Workflow**
```yaml
# .github/workflows/api.yml
name: API CI/CD

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]

env:
  REGISTRY: ghcr.io
  IMAGE_NAME: ${{ github.repository }}

jobs:
  test:
    runs-on: ubuntu-latest
    
    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: test_db
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
      
      redis:
        image: redis:7
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379

    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-dev.txt
    
    - name: Run linting
      run: |
        flake8 app tests
        black --check app tests
        isort --check-only app tests
    
    - name: Run type checking
      run: mypy app
    
    - name: Run security scan
      run: bandit -r app
    
    - name: Run tests
      env:
        DATABASE_URL: postgresql://postgres:postgres@localhost:5432/test_db
        REDIS_URL: redis://localhost:6379
      run: |
        pytest tests/ -v --cov=app --cov-report=xml
    
    - name: Upload coverage
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml

  build:
    needs: test
    runs-on: ubuntu-latest
    if: github.event_name == 'push'
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3
    
    - name: Log in to Container Registry
      uses: docker/login-action@v3
      with:
        registry: ${{ env.REGISTRY }}
        username: ${{ github.actor }}
        password: ${{ secrets.GITHUB_TOKEN }}
    
    - name: Extract metadata
      id: meta
      uses: docker/metadata-action@v5
      with:
        images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}
        tags: |
          type=ref,event=branch
          type=ref,event=pr
          type=sha
    
    - name: Build and push Docker image
      uses: docker/build-push-action@v5
      with:
        context: .
        push: true
        tags: ${{ steps.meta.outputs.tags }}
        labels: ${{ steps.meta.outputs.labels }}
        cache-from: type=gha
        cache-to: type=gha,mode=max

  deploy:
    needs: build
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    
    steps:
    - name: Deploy to staging
      run: |
        echo "Deploying to staging environment"
        # Add deployment commands here
```

### 7. Monitoring and Observability

**Prometheus Metrics**
```python
# app/core/metrics.py
from prometheus_client import Counter, Histogram, Gauge, generate_latest
from fastapi import Request
import time

# Metrics
REQUEST_COUNT = Counter(
    'http_requests_total',
    'Total HTTP requests',
    ['method', 'endpoint', 'status']
)

REQUEST_DURATION = Histogram(
    'http_request_duration_seconds',
    'HTTP request duration',
    ['method', 'endpoint']
)

ACTIVE_CONNECTIONS = Gauge(
    'active_connections',
    'Active connections'
)

async def record_metrics(request: Request, call_next):
    """Record metrics for each request"""
    start_time = time.time()
    
    ACTIVE_CONNECTIONS.inc()
    
    try:
        response = await call_next(request)
        
        # Record metrics
        REQUEST_COUNT.labels(
            method=request.method,
            endpoint=request.url.path,
            status=response.status_code
        ).inc()
        
        REQUEST_DURATION.labels(
            method=request.method,
            endpoint=request.url.path
        ).observe(time.time() - start_time)
        
        return response
    
    finally:
        ACTIVE_CONNECTIONS.dec()

@app.get("/metrics")
async def metrics():
    """Prometheus metrics endpoint"""
    return Response(
        generate_latest(),
        media_type="text/plain"
    )
```

## Cross-Command Integration

This command integrates seamlessly with other Claude Code commands to create complete development workflows:

### 1. Complete API Development Workflow

**Standard Development Pipeline:**
```bash
# 1. Start with API scaffold
/api-scaffold "User management API with FastAPI, PostgreSQL, and JWT auth"

# 2. Set up comprehensive testing
/test-harness "FastAPI API with unit, integration, and load testing using pytest and locust"

# 3. Security validation
/security-scan "FastAPI application with authentication endpoints"

# 4. Container optimization
/docker-optimize "FastAPI application with PostgreSQL and Redis dependencies"

# 5. Kubernetes deployment
/k8s-manifest "FastAPI microservice with PostgreSQL, Redis, and ingress"

# 6. Frontend integration (if needed)
/frontend-optimize "React application connecting to FastAPI backend"
```

### 2. Database-First Development

**When starting with existing data:**
```bash
# 1. Handle database migrations first
/db-migrate "PostgreSQL schema migration from legacy system to modern structure"

# 2. Generate API based on migrated schema
/api-scaffold "REST API for migrated PostgreSQL schema with auto-generated models"

# 3. Continue with standard pipeline...
```

### 3. Microservices Architecture

**For distributed systems:**
```bash
# Generate multiple related APIs
/api-scaffold "User service with authentication and profile management"
/api-scaffold "Order service with payment processing and inventory"
/api-scaffold "Notification service with email and push notifications"

# Containerize all services
/docker-optimize "Microservices architecture with service discovery"

# Deploy as distributed system
/k8s-manifest "Microservices deployment with service mesh and monitoring"
```

### 4. Integration with Generated Code

**Test Integration Setup:**
```yaml
# After running /api-scaffold, use this with /test-harness
test_config:
  api_base_url: "http://localhost:8000"
  test_database: "postgresql://test:test@localhost:5432/test_db"
  authentication:
    test_user: "test@example.com"
    test_password: "testpassword123"
  
  endpoints_to_test:
    - POST /api/v1/users/
    - POST /api/v1/auth/login
    - GET /api/v1/users/me
    - GET /api/v1/users/{id}
```

**Security Scan Configuration:**
```yaml
# Configuration for /security-scan after API scaffold
security_scan:
  target: "localhost:8000"
  authentication_endpoints:
    - "/api/v1/auth/login"
    - "/api/v1/auth/refresh"
  protected_endpoints:
    - "/api/v1/users/me"
    - "/api/v1/users/{id}"
  vulnerability_tests:
    - jwt_token_validation
    - sql_injection
    - xss_prevention
    - rate_limiting
```

**Docker Integration:**
```dockerfile
# Generated Dockerfile can be optimized with /docker-optimize
# Multi-stage build for FastAPI application
FROM python:3.11-slim as builder
WORKDIR /app
COPY requirements.txt .
RUN pip install --user -r requirements.txt

FROM python:3.11-slim as runtime
WORKDIR /app
COPY --from=builder /root/.local /root/.local
COPY . .
ENV PATH=/root/.local/bin:$PATH
CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000"]
```

**Kubernetes Deployment:**
```yaml
# Use this configuration with /k8s-manifest
apiVersion: apps/v1
kind: Deployment
metadata:
  name: api-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: api
  template:
    metadata:
      labels:
        app: api
    spec:
      containers:
      - name: api
        image: api:latest
        ports:
        - containerPort: 8000
        env:
        - name: DATABASE_URL
          valueFrom:
            secretKeyRef:
              name: api-secrets
              key: database-url
        - name: JWT_SECRET
          valueFrom:
            secretKeyRef:
              name: api-secrets
              key: jwt-secret
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /ready
            port: 8000
          initialDelaySeconds: 5
          periodSeconds: 5
```

### 5. CI/CD Pipeline Integration

**Complete pipeline using multiple commands:**
```yaml
name: Full Stack CI/CD

on:
  push:
    branches: [main]

jobs:
  api-test:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4
    
    # Test API (generated by /api-scaffold)
    - name: Run API tests
      run: |
        # Use test configuration from /test-harness
        pytest tests/ -v --cov=app
    
    # Security scan (from /security-scan)
    - name: Security scan
      run: |
        bandit -r app/
        safety check
    
    # Build optimized container (from /docker-optimize)
    - name: Build container
      run: |
        docker build -f Dockerfile.optimized -t api:${{ github.sha }} .
    
    # Deploy to Kubernetes (from /k8s-manifest)
    - name: Deploy to staging
      run: |
        kubectl apply -f k8s/staging/
        kubectl set image deployment/api-deployment api=api:${{ github.sha }}
```

### 6. Frontend-Backend Integration

**When building full-stack applications:**
```bash
# 1. Backend API
/api-scaffold "REST API with user management and data operations"

# 2. Frontend application
/frontend-optimize "React SPA with API integration, authentication, and state management"

# 3. Integration testing
/test-harness "End-to-end testing for React frontend and FastAPI backend"

# 4. Unified deployment
/k8s-manifest "Full-stack deployment with API, frontend, and database"
```

**Frontend API Integration Code:**
```typescript
// Generated API client for frontend
// Use this pattern with /frontend-optimize
export class APIClient {
  private baseURL: string;
  private token: string | null = null;

  constructor(baseURL: string) {
    this.baseURL = baseURL;
  }

  setAuthToken(token: string) {
    this.token = token;
  }

  private async request<T>(
    endpoint: string, 
    options: RequestInit = {}
  ): Promise<T> {
    const url = `${this.baseURL}${endpoint}`;
    const headers = {
      'Content-Type': 'application/json',
      ...(this.token && { Authorization: `Bearer ${this.token}` }),
      ...options.headers,
    };

    const response = await fetch(url, {
      ...options,
      headers,
    });

    if (!response.ok) {
      throw new Error(`API Error: ${response.statusText}`);
    }

    return response.json();
  }

  // User management methods (matching API scaffold)
  async createUser(userData: CreateUserRequest): Promise<User> {
    return this.request<User>('/api/v1/users/', {
      method: 'POST',
      body: JSON.stringify(userData),
    });
  }

  async login(credentials: LoginRequest): Promise<AuthResponse> {
    return this.request<AuthResponse>('/api/v1/auth/login', {
      method: 'POST',
      body: JSON.stringify(credentials),
    });
  }

  async getCurrentUser(): Promise<User> {
    return this.request<User>('/api/v1/users/me');
  }
}
```

### 7. Monitoring and Observability Integration

**Complete observability stack:**
```bash
# After API deployment, add monitoring
/api-scaffold "Monitoring endpoints with Prometheus metrics and health checks"

# Use with Kubernetes monitoring
/k8s-manifest "Kubernetes deployment with Prometheus, Grafana, and alerting"
```

This integrated approach ensures all components work together seamlessly, creating a production-ready system with proper testing, security, deployment, and monitoring.

## Validation Checklist

- [ ] Framework selected based on requirements
- [ ] Project structure follows best practices
- [ ] Authentication and authorization implemented
- [ ] Input validation and sanitization in place
- [ ] Rate limiting configured
- [ ] Error handling comprehensive
- [ ] Logging and monitoring setup
- [ ] Tests written and passing
- [ ] Security measures implemented
- [ ] API documentation generated
- [ ] Deployment configuration ready
- [ ] CI/CD pipeline configured

Focus on creating production-ready APIs with proper architecture, security, testing, and operational concerns addressed from the start.
````

## File: tools/code-explain.md

````markdown
# Code Explanation and Analysis

You are a code education expert specializing in explaining complex code through clear narratives, visual diagrams, and step-by-step breakdowns. Transform difficult concepts into understandable explanations for developers at all levels.

## Context
The user needs help understanding complex code sections, algorithms, design patterns, or system architectures. Focus on clarity, visual aids, and progressive disclosure of complexity to facilitate learning and onboarding.

## Requirements
$ARGUMENTS

## Instructions

### 1. Code Comprehension Analysis

Analyze the code to determine complexity and structure:

**Code Complexity Assessment**
```python
import ast
import re
from typing import Dict, List, Tuple

class CodeAnalyzer:
    def analyze_complexity(self, code: str) -> Dict:
        """
        Analyze code complexity and structure
        """
        analysis = {
            'complexity_score': 0,
            'concepts': [],
            'patterns': [],
            'dependencies': [],
            'difficulty_level': 'beginner'
        }
        
        # Parse code structure
        try:
            tree = ast.parse(code)
            
            # Analyze complexity metrics
            analysis['metrics'] = {
                'lines_of_code': len(code.splitlines()),
                'cyclomatic_complexity': self._calculate_cyclomatic_complexity(tree),
                'nesting_depth': self._calculate_max_nesting(tree),
                'function_count': len([n for n in ast.walk(tree) if isinstance(n, ast.FunctionDef)]),
                'class_count': len([n for n in ast.walk(tree) if isinstance(n, ast.ClassDef)])
            }
            
            # Identify concepts used
            analysis['concepts'] = self._identify_concepts(tree)
            
            # Detect design patterns
            analysis['patterns'] = self._detect_patterns(tree)
            
            # Extract dependencies
            analysis['dependencies'] = self._extract_dependencies(tree)
            
            # Determine difficulty level
            analysis['difficulty_level'] = self._assess_difficulty(analysis)
            
        except SyntaxError as e:
            analysis['parse_error'] = str(e)
            
        return analysis
    
    def _identify_concepts(self, tree) -> List[str]:
        """
        Identify programming concepts used in the code
        """
        concepts = []
        
        for node in ast.walk(tree):
            # Async/await
            if isinstance(node, (ast.AsyncFunctionDef, ast.AsyncWith, ast.AsyncFor)):
                concepts.append('asynchronous programming')
            
            # Decorators
            elif isinstance(node, ast.FunctionDef) and node.decorator_list:
                concepts.append('decorators')
            
            # Context managers
            elif isinstance(node, ast.With):
                concepts.append('context managers')
            
            # Generators
            elif isinstance(node, ast.Yield):
                concepts.append('generators')
            
            # List/Dict/Set comprehensions
            elif isinstance(node, (ast.ListComp, ast.DictComp, ast.SetComp)):
                concepts.append('comprehensions')
            
            # Lambda functions
            elif isinstance(node, ast.Lambda):
                concepts.append('lambda functions')
            
            # Exception handling
            elif isinstance(node, ast.Try):
                concepts.append('exception handling')
                
        return list(set(concepts))
```

### 2. Visual Explanation Generation

Create visual representations of code flow:

**Flow Diagram Generation**
```python
class VisualExplainer:
    def generate_flow_diagram(self, code_structure):
        """
        Generate Mermaid diagram showing code flow
        """
        diagram = "```mermaid\nflowchart TD\n"
        
        # Example: Function call flow
        if code_structure['type'] == 'function_flow':
            nodes = []
            edges = []
            
            for i, func in enumerate(code_structure['functions']):
                node_id = f"F{i}"
                nodes.append(f"    {node_id}[{func['name']}]")
                
                # Add function details
                if func.get('parameters'):
                    nodes.append(f"    {node_id}_params[/{', '.join(func['parameters'])}/]")
                    edges.append(f"    {node_id}_params --> {node_id}")
                
                # Add return value
                if func.get('returns'):
                    nodes.append(f"    {node_id}_return[{func['returns']}]")
                    edges.append(f"    {node_id} --> {node_id}_return")
                
                # Connect to called functions
                for called in func.get('calls', []):
                    called_id = f"F{code_structure['function_map'][called]}"
                    edges.append(f"    {node_id} --> {called_id}")
            
            diagram += "\n".join(nodes) + "\n"
            diagram += "\n".join(edges) + "\n"
            
        diagram += "```"
        return diagram
    
    def generate_class_diagram(self, classes):
        """
        Generate UML-style class diagram
        """
        diagram = "```mermaid\nclassDiagram\n"
        
        for cls in classes:
            # Class definition
            diagram += f"    class {cls['name']} {{\n"
            
            # Attributes
            for attr in cls.get('attributes', []):
                visibility = '+' if attr['public'] else '-'
                diagram += f"        {visibility}{attr['name']} : {attr['type']}\n"
            
            # Methods
            for method in cls.get('methods', []):
                visibility = '+' if method['public'] else '-'
                params = ', '.join(method.get('params', []))
                diagram += f"        {visibility}{method['name']}({params}) : {method['returns']}\n"
            
            diagram += "    }\n"
            
            # Relationships
            if cls.get('inherits'):
                diagram += f"    {cls['inherits']} <|-- {cls['name']}\n"
            
            for composition in cls.get('compositions', []):
                diagram += f"    {cls['name']} *-- {composition}\n"
            
        diagram += "```"
        return diagram
```

### 3. Step-by-Step Explanation

Break down complex code into digestible steps:

**Progressive Explanation**
```python
def generate_step_by_step_explanation(self, code, analysis):
    """
    Create progressive explanation from simple to complex
    """
    explanation = {
        'overview': self._generate_overview(code, analysis),
        'steps': [],
        'deep_dive': [],
        'examples': []
    }
    
    # Level 1: High-level overview
    explanation['overview'] = f"""
## What This Code Does

{self._summarize_purpose(code, analysis)}

**Key Concepts**: {', '.join(analysis['concepts'])}
**Difficulty Level**: {analysis['difficulty_level'].capitalize()}
"""
    
    # Level 2: Step-by-step breakdown
    if analysis.get('functions'):
        for i, func in enumerate(analysis['functions']):
            step = f"""
### Step {i+1}: {func['name']}

**Purpose**: {self._explain_function_purpose(func)}

**How it works**:
"""
            # Break down function logic
            for j, logic_step in enumerate(self._analyze_function_logic(func)):
                step += f"{j+1}. {logic_step}\n"
            
            # Add visual flow if complex
            if func['complexity'] > 5:
                step += f"\n{self._generate_function_flow(func)}\n"
            
            explanation['steps'].append(step)
    
    # Level 3: Deep dive into complex parts
    for concept in analysis['concepts']:
        deep_dive = self._explain_concept(concept, code)
        explanation['deep_dive'].append(deep_dive)
    
    return explanation

def _explain_concept(self, concept, code):
    """
    Explain programming concept with examples
    """
    explanations = {
        'decorators': '''
## Understanding Decorators

Decorators are a way to modify or enhance functions without changing their code directly.

**Simple Analogy**: Think of a decorator like gift wrapping - it adds something extra around the original item.

**How it works**:
```python
# This decorator:
@timer
def slow_function():
    time.sleep(1)

# Is equivalent to:
def slow_function():
    time.sleep(1)
slow_function = timer(slow_function)
```

**In this code**: The decorator is used to {specific_use_in_code}
''',
        'generators': '''
## Understanding Generators

Generators produce values one at a time, saving memory by not creating all values at once.

**Simple Analogy**: Like a ticket dispenser that gives one ticket at a time, rather than printing all tickets upfront.

**How it works**:
```python
# Generator function
def count_up_to(n):
    i = 0
    while i < n:
        yield i  # Produces one value and pauses
        i += 1

# Using the generator
for num in count_up_to(5):
    print(num)  # Prints 0, 1, 2, 3, 4
```

**In this code**: The generator is used to {specific_use_in_code}
'''
    }
    
    return explanations.get(concept, f"Explanation for {concept}")
```

### 4. Algorithm Visualization

Visualize algorithm execution:

**Algorithm Step Visualization**
```python
class AlgorithmVisualizer:
    def visualize_sorting_algorithm(self, algorithm_name, array):
        """
        Create step-by-step visualization of sorting algorithm
        """
        steps = []
        
        if algorithm_name == 'bubble_sort':
            steps.append("""
## Bubble Sort Visualization

**Initial Array**: [5, 2, 8, 1, 9]

### How Bubble Sort Works:
1. Compare adjacent elements
2. Swap if they're in wrong order
3. Repeat until no swaps needed

### Step-by-Step Execution:
""")
            
            # Simulate bubble sort with visualization
            arr = array.copy()
            n = len(arr)
            
            for i in range(n):
                swapped = False
                step_viz = f"\n**Pass {i+1}**:\n"
                
                for j in range(0, n-i-1):
                    # Show comparison
                    step_viz += f"Compare [{arr[j]}] and [{arr[j+1]}]: "
                    
                    if arr[j] > arr[j+1]:
                        arr[j], arr[j+1] = arr[j+1], arr[j]
                        step_viz += f"Swap ‚Üí {arr}\n"
                        swapped = True
                    else:
                        step_viz += "No swap needed\n"
                
                steps.append(step_viz)
                
                if not swapped:
                    steps.append(f"\n‚úÖ Array is sorted: {arr}")
                    break
        
        return '\n'.join(steps)
    
    def visualize_recursion(self, func_name, example_input):
        """
        Visualize recursive function calls
        """
        viz = f"""
## Recursion Visualization: {func_name}

### Call Stack Visualization:
```
{func_name}({example_input})
‚îÇ
‚îú‚îÄ> Base case check: {example_input} == 0? No
‚îú‚îÄ> Recursive call: {func_name}({example_input - 1})
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ> Base case check: {example_input - 1} == 0? No
‚îÇ   ‚îú‚îÄ> Recursive call: {func_name}({example_input - 2})
‚îÇ   ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   ‚îú‚îÄ> Base case check: 1 == 0? No
‚îÇ   ‚îÇ   ‚îú‚îÄ> Recursive call: {func_name}(0)
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ> Base case: Return 1
‚îÇ   ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   ‚îî‚îÄ> Return: 1 * 1 = 1
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ> Return: 2 * 1 = 2
‚îÇ
‚îî‚îÄ> Return: 3 * 2 = 6
```

**Final Result**: {func_name}({example_input}) = 6
"""
        return viz
```

### 5. Interactive Examples

Generate interactive examples for better understanding:

**Code Playground Examples**
```python
def generate_interactive_examples(self, concept):
    """
    Create runnable examples for concepts
    """
    examples = {
        'error_handling': '''
## Try It Yourself: Error Handling

### Example 1: Basic Try-Except
```python
def safe_divide(a, b):
    try:
        result = a / b
        print(f"{a} / {b} = {result}")
        return result
    except ZeroDivisionError:
        print("Error: Cannot divide by zero!")
        return None
    except TypeError:
        print("Error: Please provide numbers only!")
        return None
    finally:
        print("Division attempt completed")

# Test cases - try these:
safe_divide(10, 2)    # Success case
safe_divide(10, 0)    # Division by zero
safe_divide(10, "2")  # Type error
```

### Example 2: Custom Exceptions
```python
class ValidationError(Exception):
    """Custom exception for validation errors"""
    pass

def validate_age(age):
    try:
        age = int(age)
        if age < 0:
            raise ValidationError("Age cannot be negative")
        if age > 150:
            raise ValidationError("Age seems unrealistic")
        return age
    except ValueError:
        raise ValidationError("Age must be a number")

# Try these examples:
try:
    validate_age(25)     # Valid
    validate_age(-5)     # Negative age
    validate_age("abc")  # Not a number
except ValidationError as e:
    print(f"Validation failed: {e}")
```

### Exercise: Implement Your Own
Try implementing a function that:
1. Takes a list of numbers
2. Returns their average
3. Handles empty lists
4. Handles non-numeric values
5. Uses appropriate exception handling
''',
        'async_programming': '''
## Try It Yourself: Async Programming

### Example 1: Basic Async/Await
```python
import asyncio
import time

async def slow_operation(name, duration):
    print(f"{name} started...")
    await asyncio.sleep(duration)
    print(f"{name} completed after {duration}s")
    return f"{name} result"

async def main():
    # Sequential execution (slow)
    start = time.time()
    await slow_operation("Task 1", 2)
    await slow_operation("Task 2", 2)
    print(f"Sequential time: {time.time() - start:.2f}s")
    
    # Concurrent execution (fast)
    start = time.time()
    results = await asyncio.gather(
        slow_operation("Task 3", 2),
        slow_operation("Task 4", 2)
    )
    print(f"Concurrent time: {time.time() - start:.2f}s")
    print(f"Results: {results}")

# Run it:
asyncio.run(main())
```

### Example 2: Real-world Async Pattern
```python
async def fetch_data(url):
    """Simulate API call"""
    await asyncio.sleep(1)  # Simulate network delay
    return f"Data from {url}"

async def process_urls(urls):
    tasks = [fetch_data(url) for url in urls]
    results = await asyncio.gather(*tasks)
    return results

# Try with different URLs:
urls = ["api.example.com/1", "api.example.com/2", "api.example.com/3"]
results = asyncio.run(process_urls(urls))
print(results)
```
'''
    }
    
    return examples.get(concept, "No example available")
```

### 6. Design Pattern Explanation

Explain design patterns found in code:

**Pattern Recognition and Explanation**
```python
class DesignPatternExplainer:
    def explain_pattern(self, pattern_name, code_example):
        """
        Explain design pattern with diagrams and examples
        """
        patterns = {
            'singleton': '''
## Singleton Pattern

### What is it?
The Singleton pattern ensures a class has only one instance and provides global access to it.

### When to use it?
- Database connections
- Configuration managers
- Logging services
- Cache managers

### Visual Representation:
```mermaid
classDiagram
    class Singleton {
        -instance: Singleton
        -__init__()
        +getInstance(): Singleton
    }
    Singleton --> Singleton : returns same instance
```

### Implementation in this code:
{code_analysis}

### Benefits:
‚úÖ Controlled access to single instance
‚úÖ Reduced namespace pollution
‚úÖ Permits refinement of operations

### Drawbacks:
‚ùå Can make unit testing difficult
‚ùå Violates Single Responsibility Principle
‚ùå Can hide dependencies

### Alternative Approaches:
1. Dependency Injection
2. Module-level singleton
3. Borg pattern
''',
            'observer': '''
## Observer Pattern

### What is it?
The Observer pattern defines a one-to-many dependency between objects so that when one object changes state, all dependents are notified.

### When to use it?
- Event handling systems
- Model-View architectures
- Distributed event handling

### Visual Representation:
```mermaid
classDiagram
    class Subject {
        +attach(Observer)
        +detach(Observer)
        +notify()
    }
    class Observer {
        +update()
    }
    class ConcreteSubject {
        -state
        +getState()
        +setState()
    }
    class ConcreteObserver {
        -subject
        +update()
    }
    Subject <|-- ConcreteSubject
    Observer <|-- ConcreteObserver
    ConcreteSubject --> Observer : notifies
    ConcreteObserver --> ConcreteSubject : observes
```

### Implementation in this code:
{code_analysis}

### Real-world Example:
```python
# Newsletter subscription system
class Newsletter:
    def __init__(self):
        self._subscribers = []
        self._latest_article = None
    
    def subscribe(self, subscriber):
        self._subscribers.append(subscriber)
    
    def unsubscribe(self, subscriber):
        self._subscribers.remove(subscriber)
    
    def publish_article(self, article):
        self._latest_article = article
        self._notify_subscribers()
    
    def _notify_subscribers(self):
        for subscriber in self._subscribers:
            subscriber.update(self._latest_article)

class EmailSubscriber:
    def __init__(self, email):
        self.email = email
    
    def update(self, article):
        print(f"Sending email to {self.email}: New article - {article}")
```
'''
        }
        
        return patterns.get(pattern_name, "Pattern explanation not available")
```

### 7. Common Pitfalls and Best Practices

Highlight potential issues and improvements:

**Code Review Insights**
```python
def analyze_common_pitfalls(self, code):
    """
    Identify common mistakes and suggest improvements
    """
    issues = []
    
    # Check for common Python pitfalls
    pitfall_patterns = [
        {
            'pattern': r'except:',
            'issue': 'Bare except clause',
            'severity': 'high',
            'explanation': '''
## ‚ö†Ô∏è Bare Except Clause

**Problem**: `except:` catches ALL exceptions, including system exits and keyboard interrupts.

**Why it's bad**:
- Hides programming errors
- Makes debugging difficult
- Can catch exceptions you didn't intend to handle

**Better approach**:
```python
# Bad
try:
    risky_operation()
except:
    print("Something went wrong")

# Good
try:
    risky_operation()
except (ValueError, TypeError) as e:
    print(f"Expected error: {e}")
except Exception as e:
    logger.error(f"Unexpected error: {e}")
    raise
```
'''
        },
        {
            'pattern': r'def.*\(\s*\):.*global',
            'issue': 'Global variable usage',
            'severity': 'medium',
            'explanation': '''
## ‚ö†Ô∏è Global Variable Usage

**Problem**: Using global variables makes code harder to test and reason about.

**Better approaches**:
1. Pass as parameter
2. Use class attributes
3. Use dependency injection
4. Return values instead

**Example refactor**:
```python
# Bad
count = 0
def increment():
    global count
    count += 1

# Good
class Counter:
    def __init__(self):
        self.count = 0
    
    def increment(self):
        self.count += 1
        return self.count
```
'''
        }
    ]
    
    for pitfall in pitfall_patterns:
        if re.search(pitfall['pattern'], code):
            issues.append(pitfall)
    
    return issues
```

### 8. Learning Path Recommendations

Suggest resources for deeper understanding:

**Personalized Learning Path**
```python
def generate_learning_path(self, analysis):
    """
    Create personalized learning recommendations
    """
    learning_path = {
        'current_level': analysis['difficulty_level'],
        'identified_gaps': [],
        'recommended_topics': [],
        'resources': []
    }
    
    # Identify knowledge gaps
    if 'async' in analysis['concepts'] and analysis['difficulty_level'] == 'beginner':
        learning_path['identified_gaps'].append('Asynchronous programming fundamentals')
        learning_path['recommended_topics'].extend([
            'Event loops',
            'Coroutines vs threads',
            'Async/await syntax',
            'Concurrent programming patterns'
        ])
    
    # Add resources
    learning_path['resources'] = [
        {
            'topic': 'Async Programming',
            'type': 'tutorial',
            'title': 'Async IO in Python: A Complete Walkthrough',
            'url': 'https://realpython.com/async-io-python/',
            'difficulty': 'intermediate',
            'time_estimate': '45 minutes'
        },
        {
            'topic': 'Design Patterns',
            'type': 'book',
            'title': 'Head First Design Patterns',
            'difficulty': 'beginner-friendly',
            'format': 'visual learning'
        }
    ]
    
    # Create structured learning plan
    learning_path['structured_plan'] = f"""
## Your Personalized Learning Path

### Week 1-2: Fundamentals
- Review basic concepts: {', '.join(learning_path['recommended_topics'][:2])}
- Complete exercises on each topic
- Build a small project using these concepts

### Week 3-4: Applied Learning
- Study the patterns in this codebase
- Refactor a simple version yourself
- Compare your approach with the original

### Week 5-6: Advanced Topics
- Explore edge cases and optimizations
- Learn about alternative approaches
- Contribute to open source projects using these patterns

### Practice Projects:
1. **Beginner**: {self._suggest_beginner_project(analysis)}
2. **Intermediate**: {self._suggest_intermediate_project(analysis)}
3. **Advanced**: {self._suggest_advanced_project(analysis)}
"""
    
    return learning_path
```

## Output Format

1. **Complexity Analysis**: Overview of code complexity and concepts used
2. **Visual Diagrams**: Flow charts, class diagrams, and execution visualizations
3. **Step-by-Step Breakdown**: Progressive explanation from simple to complex
4. **Interactive Examples**: Runnable code samples to experiment with
5. **Common Pitfalls**: Issues to avoid with explanations
6. **Best Practices**: Improved approaches and patterns
7. **Learning Resources**: Curated resources for deeper understanding
8. **Practice Exercises**: Hands-on challenges to reinforce learning

Focus on making complex code accessible through clear explanations, visual aids, and practical examples that build understanding progressively.
````

## File: tools/code-migrate.md

````markdown
# Code Migration Assistant

You are a code migration expert specializing in transitioning codebases between frameworks, languages, versions, and platforms. Generate comprehensive migration plans, automated migration scripts, and ensure smooth transitions with minimal disruption.

## Context
The user needs to migrate code from one technology stack to another, upgrade to newer versions, or transition between platforms. Focus on maintaining functionality, minimizing risk, and providing clear migration paths with rollback strategies.

## Requirements
$ARGUMENTS

## Instructions

### 1. Migration Assessment

Analyze the current codebase and migration requirements:

**Migration Analyzer**
```python
import os
import json
import ast
import re
from pathlib import Path
from collections import defaultdict

class MigrationAnalyzer:
    def __init__(self, source_path, target_tech):
        self.source_path = Path(source_path)
        self.target_tech = target_tech
        self.analysis = defaultdict(dict)
    
    def analyze_migration(self):
        """
        Comprehensive migration analysis
        """
        self.analysis['source'] = self._analyze_source()
        self.analysis['complexity'] = self._assess_complexity()
        self.analysis['dependencies'] = self._analyze_dependencies()
        self.analysis['risks'] = self._identify_risks()
        self.analysis['effort'] = self._estimate_effort()
        self.analysis['strategy'] = self._recommend_strategy()
        
        return self.analysis
    
    def _analyze_source(self):
        """Analyze source codebase characteristics"""
        stats = {
            'files': 0,
            'lines': 0,
            'components': 0,
            'patterns': [],
            'frameworks': set(),
            'languages': defaultdict(int)
        }
        
        for file_path in self.source_path.rglob('*'):
            if file_path.is_file() and not self._is_ignored(file_path):
                stats['files'] += 1
                ext = file_path.suffix
                stats['languages'][ext] += 1
                
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    content = f.read()
                    stats['lines'] += len(content.splitlines())
                    
                    # Detect frameworks and patterns
                    self._detect_patterns(content, stats)
        
        return stats
    
    def _assess_complexity(self):
        """Assess migration complexity"""
        factors = {
            'size': self._calculate_size_complexity(),
            'architectural': self._calculate_architectural_complexity(),
            'dependency': self._calculate_dependency_complexity(),
            'business_logic': self._calculate_logic_complexity(),
            'data': self._calculate_data_complexity()
        }
        
        overall = sum(factors.values()) / len(factors)
        
        return {
            'factors': factors,
            'overall': overall,
            'level': self._get_complexity_level(overall)
        }
    
    def _identify_risks(self):
        """Identify migration risks"""
        risks = []
        
        # Check for high-risk patterns
        risk_patterns = {
            'global_state': {
                'pattern': r'(global|window)\.\w+\s*=',
                'severity': 'high',
                'description': 'Global state management needs careful migration'
            },
            'direct_dom': {
                'pattern': r'document\.(getElementById|querySelector)',
                'severity': 'medium',
                'description': 'Direct DOM manipulation needs framework adaptation'
            },
            'async_patterns': {
                'pattern': r'(callback|setTimeout|setInterval)',
                'severity': 'medium',
                'description': 'Async patterns may need modernization'
            },
            'deprecated_apis': {
                'pattern': r'(componentWillMount|componentWillReceiveProps)',
                'severity': 'high',
                'description': 'Deprecated APIs need replacement'
            }
        }
        
        for risk_name, risk_info in risk_patterns.items():
            occurrences = self._count_pattern_occurrences(risk_info['pattern'])
            if occurrences > 0:
                risks.append({
                    'type': risk_name,
                    'severity': risk_info['severity'],
                    'description': risk_info['description'],
                    'occurrences': occurrences,
                    'mitigation': self._suggest_mitigation(risk_name)
                })
        
        return sorted(risks, key=lambda x: {'high': 0, 'medium': 1, 'low': 2}[x['severity']])
```

### 2. Migration Planning

Create detailed migration plans:

**Migration Planner**
```python
class MigrationPlanner:
    def create_migration_plan(self, analysis):
        """
        Create comprehensive migration plan
        """
        plan = {
            'phases': self._define_phases(analysis),
            'timeline': self._estimate_timeline(analysis),
            'resources': self._calculate_resources(analysis),
            'milestones': self._define_milestones(analysis),
            'success_criteria': self._define_success_criteria()
        }
        
        return self._format_plan(plan)
    
    def _define_phases(self, analysis):
        """Define migration phases"""
        complexity = analysis['complexity']['overall']
        
        if complexity < 3:
            # Simple migration
            return [
                {
                    'name': 'Preparation',
                    'duration': '1 week',
                    'tasks': [
                        'Setup new project structure',
                        'Install dependencies',
                        'Configure build tools',
                        'Setup testing framework'
                    ]
                },
                {
                    'name': 'Core Migration',
                    'duration': '2-3 weeks',
                    'tasks': [
                        'Migrate utility functions',
                        'Port components/modules',
                        'Update data models',
                        'Migrate business logic'
                    ]
                },
                {
                    'name': 'Testing & Refinement',
                    'duration': '1 week',
                    'tasks': [
                        'Unit testing',
                        'Integration testing',
                        'Performance testing',
                        'Bug fixes'
                    ]
                }
            ]
        else:
            # Complex migration
            return [
                {
                    'name': 'Phase 0: Foundation',
                    'duration': '2 weeks',
                    'tasks': [
                        'Architecture design',
                        'Proof of concept',
                        'Tool selection',
                        'Team training'
                    ]
                },
                {
                    'name': 'Phase 1: Infrastructure',
                    'duration': '3 weeks',
                    'tasks': [
                        'Setup build pipeline',
                        'Configure development environment',
                        'Implement core abstractions',
                        'Setup automated testing'
                    ]
                },
                {
                    'name': 'Phase 2: Incremental Migration',
                    'duration': '6-8 weeks',
                    'tasks': [
                        'Migrate shared utilities',
                        'Port feature modules',
                        'Implement adapters/bridges',
                        'Maintain dual runtime'
                    ]
                },
                {
                    'name': 'Phase 3: Cutover',
                    'duration': '2 weeks',
                    'tasks': [
                        'Complete remaining migrations',
                        'Remove legacy code',
                        'Performance optimization',
                        'Final testing'
                    ]
                }
            ]
    
    def _format_plan(self, plan):
        """Format migration plan as markdown"""
        output = "# Migration Plan\n\n"
        
        # Executive Summary
        output += "## Executive Summary\n\n"
        output += f"- **Total Duration**: {plan['timeline']['total']}\n"
        output += f"- **Team Size**: {plan['resources']['team_size']}\n"
        output += f"- **Risk Level**: {plan['timeline']['risk_buffer']}\n\n"
        
        # Phases
        output += "## Migration Phases\n\n"
        for i, phase in enumerate(plan['phases']):
            output += f"### {phase['name']}\n"
            output += f"**Duration**: {phase['duration']}\n\n"
            output += "**Tasks**:\n"
            for task in phase['tasks']:
                output += f"- {task}\n"
            output += "\n"
        
        # Milestones
        output += "## Key Milestones\n\n"
        for milestone in plan['milestones']:
            output += f"- **{milestone['name']}**: {milestone['criteria']}\n"
        
        return output
```

### 3. Framework Migrations

Handle specific framework migrations:

**React to Vue Migration**
```javascript
class ReactToVueMigrator {
    migrateComponent(reactComponent) {
        // Parse React component
        const ast = parseReactComponent(reactComponent);
        
        // Extract component structure
        const componentInfo = {
            name: this.extractComponentName(ast),
            props: this.extractProps(ast),
            state: this.extractState(ast),
            methods: this.extractMethods(ast),
            lifecycle: this.extractLifecycle(ast),
            render: this.extractRender(ast)
        };
        
        // Generate Vue component
        return this.generateVueComponent(componentInfo);
    }
    
    generateVueComponent(info) {
        return `
<template>
${this.convertJSXToTemplate(info.render)}
</template>

<script>
export default {
    name: '${info.name}',
    props: ${this.convertProps(info.props)},
    data() {
        return ${this.convertState(info.state)}
    },
    methods: ${this.convertMethods(info.methods)},
    ${this.convertLifecycle(info.lifecycle)}
}
</script>

<style scoped>
/* Component styles */
</style>
`;
    }
    
    convertJSXToTemplate(jsx) {
        // Convert JSX to Vue template syntax
        let template = jsx;
        
        // Convert className to class
        template = template.replace(/className=/g, 'class=');
        
        // Convert onClick to @click
        template = template.replace(/onClick={/g, '@click="');
        template = template.replace(/on(\w+)={this\.(\w+)}/g, '@$1="$2"');
        
        // Convert conditional rendering
        template = template.replace(/{(\w+) && (.+?)}/g, '<template v-if="$1">$2</template>');
        template = template.replace(/{(\w+) \? (.+?) : (.+?)}/g, 
            '<template v-if="$1">$2</template><template v-else>$3</template>');
        
        // Convert map iterations
        template = template.replace(
            /{(\w+)\.map\(\((\w+), (\w+)\) => (.+?)\)}/g,
            '<template v-for="($2, $3) in $1" :key="$3">$4</template>'
        );
        
        return template;
    }
    
    convertLifecycle(lifecycle) {
        const vueLifecycle = {
            'componentDidMount': 'mounted',
            'componentDidUpdate': 'updated',
            'componentWillUnmount': 'beforeDestroy',
            'getDerivedStateFromProps': 'computed'
        };
        
        let result = '';
        for (const [reactHook, vueHook] of Object.entries(vueLifecycle)) {
            if (lifecycle[reactHook]) {
                result += `${vueHook}() ${lifecycle[reactHook].body},\n`;
            }
        }
        
        return result;
    }
}
```

### 4. Language Migrations

Handle language version upgrades:

**Python 2 to 3 Migration**
```python
class Python2to3Migrator:
    def __init__(self):
        self.transformations = {
            'print_statement': self.transform_print,
            'unicode_literals': self.transform_unicode,
            'division': self.transform_division,
            'imports': self.transform_imports,
            'iterators': self.transform_iterators,
            'exceptions': self.transform_exceptions
        }
    
    def migrate_file(self, file_path):
        """Migrate single Python file from 2 to 3"""
        with open(file_path, 'r') as f:
            content = f.read()
        
        # Parse AST
        try:
            tree = ast.parse(content)
        except SyntaxError:
            # Try with 2to3 lib for syntax conversion first
            content = self._basic_syntax_conversion(content)
            tree = ast.parse(content)
        
        # Apply transformations
        transformer = Python3Transformer()
        new_tree = transformer.visit(tree)
        
        # Generate new code
        return astor.to_source(new_tree)
    
    def transform_print(self, content):
        """Transform print statements to functions"""
        # Simple regex for basic cases
        content = re.sub(
            r'print\s+([^(].*?)$',
            r'print(\1)',
            content,
            flags=re.MULTILINE
        )
        
        # Handle print with >>
        content = re.sub(
            r'print\s*>>\s*(\w+),\s*(.+?)$',
            r'print(\2, file=\1)',
            content,
            flags=re.MULTILINE
        )
        
        return content
    
    def transform_unicode(self, content):
        """Handle unicode literals"""
        # Remove u prefix from strings
        content = re.sub(r'u"([^"]*)"', r'"\1"', content)
        content = re.sub(r"u'([^']*)'", r"'\1'", content)
        
        # Convert unicode() to str()
        content = re.sub(r'\bunicode\(', 'str(', content)
        
        return content
    
    def transform_iterators(self, content):
        """Transform iterator methods"""
        replacements = {
            '.iteritems()': '.items()',
            '.iterkeys()': '.keys()',
            '.itervalues()': '.values()',
            'xrange': 'range',
            '.has_key(': ' in '
        }
        
        for old, new in replacements.items():
            content = content.replace(old, new)
        
        return content

class Python3Transformer(ast.NodeTransformer):
    """AST transformer for Python 3 migration"""
    
    def visit_Raise(self, node):
        """Transform raise statements"""
        if node.exc and node.cause:
            # raise Exception, args -> raise Exception(args)
            if isinstance(node.cause, ast.Str):
                node.exc = ast.Call(
                    func=node.exc,
                    args=[node.cause],
                    keywords=[]
                )
                node.cause = None
        
        return node
    
    def visit_ExceptHandler(self, node):
        """Transform except clauses"""
        if node.type and node.name:
            # except Exception, e -> except Exception as e
            if isinstance(node.name, ast.Name):
                node.name = node.name.id
        
        return node
```

### 5. API Migration

Migrate between API paradigms:

**REST to GraphQL Migration**
```javascript
class RESTToGraphQLMigrator {
    constructor(restEndpoints) {
        this.endpoints = restEndpoints;
        this.schema = {
            types: {},
            queries: {},
            mutations: {}
        };
    }
    
    generateGraphQLSchema() {
        // Analyze REST endpoints
        this.analyzeEndpoints();
        
        // Generate type definitions
        const typeDefs = this.generateTypeDefs();
        
        // Generate resolvers
        const resolvers = this.generateResolvers();
        
        return { typeDefs, resolvers };
    }
    
    analyzeEndpoints() {
        for (const endpoint of this.endpoints) {
            const { method, path, response, params } = endpoint;
            
            // Extract resource type
            const resourceType = this.extractResourceType(path);
            
            // Build GraphQL type
            if (!this.schema.types[resourceType]) {
                this.schema.types[resourceType] = this.buildType(response);
            }
            
            // Map to GraphQL operations
            if (method === 'GET') {
                this.addQuery(resourceType, path, params);
            } else if (['POST', 'PUT', 'PATCH'].includes(method)) {
                this.addMutation(resourceType, path, params, method);
            }
        }
    }
    
    generateTypeDefs() {
        let schema = 'type Query {\n';
        
        // Add queries
        for (const [name, query] of Object.entries(this.schema.queries)) {
            schema += `  ${name}${this.generateArgs(query.args)}: ${query.returnType}\n`;
        }
        
        schema += '}\n\ntype Mutation {\n';
        
        // Add mutations
        for (const [name, mutation] of Object.entries(this.schema.mutations)) {
            schema += `  ${name}${this.generateArgs(mutation.args)}: ${mutation.returnType}\n`;
        }
        
        schema += '}\n\n';
        
        // Add types
        for (const [typeName, fields] of Object.entries(this.schema.types)) {
            schema += `type ${typeName} {\n`;
            for (const [fieldName, fieldType] of Object.entries(fields)) {
                schema += `  ${fieldName}: ${fieldType}\n`;
            }
            schema += '}\n\n';
        }
        
        return schema;
    }
    
    generateResolvers() {
        const resolvers = {
            Query: {},
            Mutation: {}
        };
        
        // Generate query resolvers
        for (const [name, query] of Object.entries(this.schema.queries)) {
            resolvers.Query[name] = async (parent, args, context) => {
                // Transform GraphQL args to REST params
                const restParams = this.transformArgs(args, query.paramMapping);
                
                // Call REST endpoint
                const response = await fetch(
                    this.buildUrl(query.endpoint, restParams),
                    { method: 'GET' }
                );
                
                return response.json();
            };
        }
        
        // Generate mutation resolvers
        for (const [name, mutation] of Object.entries(this.schema.mutations)) {
            resolvers.Mutation[name] = async (parent, args, context) => {
                const { input } = args;
                
                const response = await fetch(
                    mutation.endpoint,
                    {
                        method: mutation.method,
                        headers: { 'Content-Type': 'application/json' },
                        body: JSON.stringify(input)
                    }
                );
                
                return response.json();
            };
        }
        
        return resolvers;
    }
}
```

### 6. Database Migration

Migrate between database systems:

**SQL to NoSQL Migration**
```python
class SQLToNoSQLMigrator:
    def __init__(self, source_db, target_db):
        self.source = source_db
        self.target = target_db
        self.schema_mapping = {}
    
    def analyze_schema(self):
        """Analyze SQL schema for NoSQL conversion"""
        tables = self.get_sql_tables()
        
        for table in tables:
            # Get table structure
            columns = self.get_table_columns(table)
            relationships = self.get_table_relationships(table)
            
            # Design document structure
            doc_structure = self.design_document_structure(
                table, columns, relationships
            )
            
            self.schema_mapping[table] = doc_structure
        
        return self.schema_mapping
    
    def design_document_structure(self, table, columns, relationships):
        """Design NoSQL document structure from SQL table"""
        structure = {
            'collection': self.to_collection_name(table),
            'fields': {},
            'embedded': [],
            'references': []
        }
        
        # Map columns to fields
        for col in columns:
            structure['fields'][col['name']] = {
                'type': self.map_sql_type_to_nosql(col['type']),
                'required': not col['nullable'],
                'indexed': col.get('is_indexed', False)
            }
        
        # Handle relationships
        for rel in relationships:
            if rel['type'] == 'one-to-one' or self.should_embed(rel):
                structure['embedded'].append({
                    'field': rel['field'],
                    'collection': rel['related_table']
                })
            else:
                structure['references'].append({
                    'field': rel['field'],
                    'collection': rel['related_table'],
                    'type': rel['type']
                })
        
        return structure
    
    def generate_migration_script(self):
        """Generate migration script"""
        script = """
import asyncio
from datetime import datetime

class DatabaseMigrator:
    def __init__(self, sql_conn, nosql_conn):
        self.sql = sql_conn
        self.nosql = nosql_conn
        self.batch_size = 1000
        
    async def migrate(self):
        start_time = datetime.now()
        
        # Create indexes
        await self.create_indexes()
        
        # Migrate data
        for table, mapping in schema_mapping.items():
            await self.migrate_table(table, mapping)
        
        # Verify migration
        await self.verify_migration()
        
        elapsed = datetime.now() - start_time
        print(f"Migration completed in {elapsed}")
    
    async def migrate_table(self, table, mapping):
        print(f"Migrating {table}...")
        
        total_rows = await self.get_row_count(table)
        migrated = 0
        
        async for batch in self.read_in_batches(table):
            documents = []
            
            for row in batch:
                doc = self.transform_row_to_document(row, mapping)
                
                # Handle embedded documents
                for embed in mapping['embedded']:
                    related_data = await self.fetch_related(
                        row, embed['field'], embed['collection']
                    )
                    doc[embed['field']] = related_data
                
                documents.append(doc)
            
            # Bulk insert
            await self.nosql[mapping['collection']].insert_many(documents)
            
            migrated += len(batch)
            progress = (migrated / total_rows) * 100
            print(f"  Progress: {progress:.1f}% ({migrated}/{total_rows})")
    
    def transform_row_to_document(self, row, mapping):
        doc = {}
        
        for field, config in mapping['fields'].items():
            value = row.get(field)
            
            # Type conversion
            if value is not None:
                doc[field] = self.convert_value(value, config['type'])
            elif config['required']:
                doc[field] = self.get_default_value(config['type'])
        
        # Add metadata
        doc['_migrated_at'] = datetime.now()
        doc['_source_table'] = mapping['collection']
        
        return doc
"""
        return script
```

### 7. Testing Strategy

Ensure migration correctness:

**Migration Testing Framework**
```python
class MigrationTester:
    def __init__(self, original_app, migrated_app):
        self.original = original_app
        self.migrated = migrated_app
        self.test_results = []
    
    def run_comparison_tests(self):
        """Run side-by-side comparison tests"""
        test_suites = [
            self.test_functionality,
            self.test_performance,
            self.test_data_integrity,
            self.test_api_compatibility,
            self.test_user_flows
        ]
        
        for suite in test_suites:
            results = suite()
            self.test_results.extend(results)
        
        return self.generate_report()
    
    def test_functionality(self):
        """Test functional equivalence"""
        results = []
        
        test_cases = self.generate_test_cases()
        
        for test in test_cases:
            original_result = self.execute_on_original(test)
            migrated_result = self.execute_on_migrated(test)
            
            comparison = self.compare_results(
                original_result, 
                migrated_result
            )
            
            results.append({
                'test': test['name'],
                'status': 'PASS' if comparison['equivalent'] else 'FAIL',
                'details': comparison['details']
            })
        
        return results
    
    def test_performance(self):
        """Compare performance metrics"""
        metrics = ['response_time', 'throughput', 'cpu_usage', 'memory_usage']
        results = []
        
        for metric in metrics:
            original_perf = self.measure_performance(self.original, metric)
            migrated_perf = self.measure_performance(self.migrated, metric)
            
            regression = ((migrated_perf - original_perf) / original_perf) * 100
            
            results.append({
                'metric': metric,
                'original': original_perf,
                'migrated': migrated_perf,
                'regression': regression,
                'acceptable': abs(regression) <= 10  # 10% threshold
            })
        
        return results
```

### 8. Rollback Planning

Implement safe rollback strategies:

```python
class RollbackManager:
    def create_rollback_plan(self, migration_type):
        """Create comprehensive rollback plan"""
        plan = {
            'triggers': self.define_rollback_triggers(),
            'procedures': self.define_rollback_procedures(migration_type),
            'verification': self.define_verification_steps(),
            'communication': self.define_communication_plan()
        }
        
        return self.format_rollback_plan(plan)
    
    def define_rollback_triggers(self):
        """Define conditions that trigger rollback"""
        return [
            {
                'condition': 'Critical functionality broken',
                'threshold': 'Any P0 feature non-functional',
                'detection': 'Automated monitoring + user reports'
            },
            {
                'condition': 'Performance degradation',
                'threshold': '>50% increase in response time',
                'detection': 'APM metrics'
            },
            {
                'condition': 'Data corruption',
                'threshold': 'Any data integrity issues',
                'detection': 'Data validation checks'
            },
            {
                'condition': 'High error rate',
                'threshold': '>5% error rate increase',
                'detection': 'Error tracking system'
            }
        ]
    
    def define_rollback_procedures(self, migration_type):
        """Define step-by-step rollback procedures"""
        if migration_type == 'blue_green':
            return self._blue_green_rollback()
        elif migration_type == 'canary':
            return self._canary_rollback()
        elif migration_type == 'feature_flag':
            return self._feature_flag_rollback()
        else:
            return self._standard_rollback()
    
    def _blue_green_rollback(self):
        return [
            "1. Verify green environment is problematic",
            "2. Update load balancer to route 100% to blue",
            "3. Monitor blue environment stability",
            "4. Notify stakeholders of rollback",
            "5. Begin root cause analysis",
            "6. Keep green environment for debugging"
        ]
```

### 9. Migration Automation

Create automated migration tools:

```python
def create_migration_cli():
    """Generate CLI tool for migration"""
    return '''
#!/usr/bin/env python3
import click
import json
from pathlib import Path

@click.group()
def cli():
    """Code Migration Tool"""
    pass

@cli.command()
@click.option('--source', required=True, help='Source directory')
@click.option('--target', required=True, help='Target technology')
@click.option('--output', default='migration-plan.json', help='Output file')
def analyze(source, target, output):
    """Analyze codebase for migration"""
    analyzer = MigrationAnalyzer(source, target)
    analysis = analyzer.analyze_migration()
    
    with open(output, 'w') as f:
        json.dump(analysis, f, indent=2)
    
    click.echo(f"Analysis complete. Results saved to {output}")

@cli.command()
@click.option('--plan', required=True, help='Migration plan file')
@click.option('--phase', help='Specific phase to execute')
@click.option('--dry-run', is_flag=True, help='Simulate migration')
def migrate(plan, phase, dry_run):
    """Execute migration based on plan"""
    with open(plan) as f:
        migration_plan = json.load(f)
    
    migrator = CodeMigrator(migration_plan)
    
    if dry_run:
        click.echo("Running migration in dry-run mode...")
        results = migrator.dry_run(phase)
    else:
        click.echo("Executing migration...")
        results = migrator.execute(phase)
    
    # Display results
    for result in results:
        status = "‚úì" if result['success'] else "‚úó"
        click.echo(f"{status} {result['task']}: {result['message']}")

@cli.command()
@click.option('--original', required=True, help='Original codebase')
@click.option('--migrated', required=True, help='Migrated codebase')
def test(original, migrated):
    """Test migration results"""
    tester = MigrationTester(original, migrated)
    results = tester.run_comparison_tests()
    
    # Display test results
    passed = sum(1 for r in results if r['status'] == 'PASS')
    total = len(results)
    
    click.echo(f"\\nTest Results: {passed}/{total} passed")
    
    for result in results:
        if result['status'] == 'FAIL':
            click.echo(f"\\n‚ùå {result['test']}")
            click.echo(f"   {result['details']}")

if __name__ == '__main__':
    cli()
'''
```

### 10. Progress Monitoring

Track migration progress:

```python
class MigrationMonitor:
    def __init__(self, migration_id):
        self.migration_id = migration_id
        self.metrics = defaultdict(list)
        self.checkpoints = []
    
    def create_dashboard(self):
        """Create migration monitoring dashboard"""
        return f"""
<!DOCTYPE html>
<html>
<head>
    <title>Migration Dashboard - {self.migration_id}</title>
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    <style>
        .metric-card {{
            background: #f5f5f5;
            padding: 20px;
            margin: 10px;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }}
        .progress-bar {{
            width: 100%;
            height: 30px;
            background: #e0e0e0;
            border-radius: 15px;
            overflow: hidden;
        }}
        .progress-fill {{
            height: 100%;
            background: #4CAF50;
            transition: width 0.5s;
        }}
    </style>
</head>
<body>
    <h1>Migration Progress Dashboard</h1>
    
    <div class="metric-card">
        <h2>Overall Progress</h2>
        <div class="progress-bar">
            <div class="progress-fill" style="width: {self.calculate_progress()}%"></div>
        </div>
        <p>{self.calculate_progress()}% Complete</p>
    </div>
    
    <div class="metric-card">
        <h2>Phase Status</h2>
        <canvas id="phaseChart"></canvas>
    </div>
    
    <div class="metric-card">
        <h2>Migration Metrics</h2>
        <canvas id="metricsChart"></canvas>
    </div>
    
    <div class="metric-card">
        <h2>Recent Activities</h2>
        <ul id="activities">
            {self.format_recent_activities()}
        </ul>
    </div>
    
    <script>
        // Update dashboard every 30 seconds
        setInterval(() => location.reload(), 30000);
        
        // Phase chart
        new Chart(document.getElementById('phaseChart'), {{
            type: 'doughnut',
            data: {self.get_phase_chart_data()}
        }});
        
        // Metrics chart
        new Chart(document.getElementById('metricsChart'), {{
            type: 'line',
            data: {self.get_metrics_chart_data()}
        }});
    </script>
</body>
</html>
"""
```

## Output Format

1. **Migration Analysis**: Comprehensive analysis of source codebase
2. **Risk Assessment**: Identified risks with mitigation strategies
3. **Migration Plan**: Phased approach with timeline and milestones
4. **Code Examples**: Automated migration scripts and transformations
5. **Testing Strategy**: Comparison tests and validation approach
6. **Rollback Plan**: Detailed procedures for safe rollback
7. **Progress Tracking**: Real-time migration monitoring
8. **Documentation**: Migration guide and runbooks

Focus on minimizing disruption, maintaining functionality, and providing clear paths for successful code migration with comprehensive testing and rollback strategies.
````

## File: tools/compliance-check.md

````markdown
# Regulatory Compliance Check

You are a compliance expert specializing in regulatory requirements for software systems including GDPR, HIPAA, SOC2, PCI-DSS, and other industry standards. Perform comprehensive compliance audits and provide implementation guidance for achieving and maintaining compliance.

## Context
The user needs to ensure their application meets regulatory requirements and industry standards. Focus on practical implementation of compliance controls, automated monitoring, and audit trail generation.

## Requirements
$ARGUMENTS

## Instructions

### 1. Compliance Framework Analysis

Identify applicable regulations and standards:

**Regulatory Mapping**
```python
class ComplianceAnalyzer:
    def __init__(self):
        self.regulations = {
            'GDPR': {
                'scope': 'EU data protection',
                'applies_if': [
                    'Processing EU residents data',
                    'Offering goods/services to EU',
                    'Monitoring EU residents behavior'
                ],
                'key_requirements': [
                    'Privacy by design',
                    'Data minimization',
                    'Right to erasure',
                    'Data portability',
                    'Consent management',
                    'DPO appointment',
                    'Privacy notices',
                    'Data breach notification (72hrs)'
                ]
            },
            'HIPAA': {
                'scope': 'Healthcare data protection (US)',
                'applies_if': [
                    'Healthcare providers',
                    'Health plan providers', 
                    'Healthcare clearinghouses',
                    'Business associates'
                ],
                'key_requirements': [
                    'PHI encryption',
                    'Access controls',
                    'Audit logs',
                    'Business Associate Agreements',
                    'Risk assessments',
                    'Employee training',
                    'Incident response',
                    'Physical safeguards'
                ]
            },
            'SOC2': {
                'scope': 'Service organization controls',
                'applies_if': [
                    'SaaS providers',
                    'Data processors',
                    'Cloud services'
                ],
                'trust_principles': [
                    'Security',
                    'Availability', 
                    'Processing integrity',
                    'Confidentiality',
                    'Privacy'
                ]
            },
            'PCI-DSS': {
                'scope': 'Payment card data security',
                'applies_if': [
                    'Accept credit/debit cards',
                    'Process card payments',
                    'Store card data',
                    'Transmit card data'
                ],
                'compliance_levels': {
                    'Level 1': '>6M transactions/year',
                    'Level 2': '1M-6M transactions/year',
                    'Level 3': '20K-1M transactions/year',
                    'Level 4': '<20K transactions/year'
                }
            }
        }
    
    def determine_applicable_regulations(self, business_info):
        """
        Determine which regulations apply based on business context
        """
        applicable = []
        
        # Check each regulation
        for reg_name, reg_info in self.regulations.items():
            if self._check_applicability(business_info, reg_info):
                applicable.append({
                    'regulation': reg_name,
                    'reason': self._get_applicability_reason(business_info, reg_info),
                    'priority': self._calculate_priority(business_info, reg_name)
                })
        
        return sorted(applicable, key=lambda x: x['priority'], reverse=True)
```

### 2. Data Privacy Compliance

Implement privacy controls:

**GDPR Implementation**
```python
class GDPRCompliance:
    def implement_privacy_controls(self):
        """
        Implement GDPR-required privacy controls
        """
        controls = {}
        
        # 1. Consent Management
        controls['consent_management'] = '''
class ConsentManager:
    def __init__(self):
        self.consent_types = [
            'marketing_emails',
            'analytics_tracking',
            'third_party_sharing',
            'profiling'
        ]
    
    def record_consent(self, user_id, consent_type, granted):
        """
        Record user consent with full audit trail
        """
        consent_record = {
            'user_id': user_id,
            'consent_type': consent_type,
            'granted': granted,
            'timestamp': datetime.utcnow(),
            'ip_address': request.remote_addr,
            'user_agent': request.headers.get('User-Agent'),
            'version': self.get_current_privacy_policy_version(),
            'method': 'explicit_checkbox'  # Not pre-ticked
        }
        
        # Store in append-only audit log
        self.consent_audit_log.append(consent_record)
        
        # Update current consent status
        self.update_user_consents(user_id, consent_type, granted)
        
        return consent_record
    
    def verify_consent(self, user_id, consent_type):
        """
        Verify if user has given consent for specific processing
        """
        consent = self.get_user_consent(user_id, consent_type)
        return consent and consent['granted'] and not consent.get('withdrawn')
'''

        # 2. Right to Erasure (Right to be Forgotten)
        controls['right_to_erasure'] = '''
class DataErasureService:
    def process_erasure_request(self, user_id, verification_token):
        """
        Process GDPR Article 17 erasure request
        """
        # Verify request authenticity
        if not self.verify_erasure_token(user_id, verification_token):
            raise ValueError("Invalid erasure request")
        
        erasure_log = {
            'user_id': user_id,
            'requested_at': datetime.utcnow(),
            'data_categories': []
        }
        
        # 1. Personal data
        self.erase_user_profile(user_id)
        erasure_log['data_categories'].append('profile')
        
        # 2. User-generated content (anonymize instead of delete)
        self.anonymize_user_content(user_id)
        erasure_log['data_categories'].append('content_anonymized')
        
        # 3. Analytics data
        self.remove_from_analytics(user_id)
        erasure_log['data_categories'].append('analytics')
        
        # 4. Backup data (schedule deletion)
        self.schedule_backup_deletion(user_id)
        erasure_log['data_categories'].append('backups_scheduled')
        
        # 5. Notify third parties
        self.notify_processors_of_erasure(user_id)
        
        # Keep minimal record for legal compliance
        self.store_erasure_record(erasure_log)
        
        return {
            'status': 'completed',
            'erasure_id': erasure_log['id'],
            'categories_erased': erasure_log['data_categories']
        }
'''

        # 3. Data Portability
        controls['data_portability'] = '''
class DataPortabilityService:
    def export_user_data(self, user_id, format='json'):
        """
        GDPR Article 20 - Data portability
        """
        user_data = {
            'export_date': datetime.utcnow().isoformat(),
            'user_id': user_id,
            'format_version': '2.0',
            'data': {}
        }
        
        # Collect all user data
        user_data['data']['profile'] = self.get_user_profile(user_id)
        user_data['data']['preferences'] = self.get_user_preferences(user_id)
        user_data['data']['content'] = self.get_user_content(user_id)
        user_data['data']['activity'] = self.get_user_activity(user_id)
        user_data['data']['consents'] = self.get_consent_history(user_id)
        
        # Format based on request
        if format == 'json':
            return json.dumps(user_data, indent=2)
        elif format == 'csv':
            return self.convert_to_csv(user_data)
        elif format == 'xml':
            return self.convert_to_xml(user_data)
'''
        
        return controls

**Privacy by Design**
```python
# Implement privacy by design principles
class PrivacyByDesign:
    def implement_data_minimization(self):
        """
        Collect only necessary data
        """
        # Before (collecting too much)
        bad_user_model = {
            'email': str,
            'password': str,
            'full_name': str,
            'date_of_birth': date,
            'ssn': str,  # Unnecessary
            'address': str,  # Unnecessary for basic service
            'phone': str,  # Unnecessary
            'gender': str,  # Unnecessary
            'income': int  # Unnecessary
        }
        
        # After (data minimization)
        good_user_model = {
            'email': str,  # Required for authentication
            'password_hash': str,  # Never store plain text
            'display_name': str,  # Optional, user-provided
            'created_at': datetime,
            'last_login': datetime
        }
        
        return good_user_model
    
    def implement_pseudonymization(self):
        """
        Replace identifying fields with pseudonyms
        """
        def pseudonymize_record(record):
            # Generate consistent pseudonym
            user_pseudonym = hashlib.sha256(
                f"{record['user_id']}{SECRET_SALT}".encode()
            ).hexdigest()[:16]
            
            return {
                'pseudonym': user_pseudonym,
                'data': {
                    # Remove direct identifiers
                    'age_group': self._get_age_group(record['age']),
                    'region': self._get_region(record['ip_address']),
                    'activity': record['activity_data']
                }
            }
```

### 3. Security Compliance

Implement security controls for various standards:

**SOC2 Security Controls**
```python
class SOC2SecurityControls:
    def implement_access_controls(self):
        """
        SOC2 CC6.1 - Logical and physical access controls
        """
        controls = {
            'authentication': '''
# Multi-factor authentication
class MFAEnforcement:
    def enforce_mfa(self, user, resource_sensitivity):
        if resource_sensitivity == 'high':
            return self.require_mfa(user)
        elif resource_sensitivity == 'medium' and user.is_admin:
            return self.require_mfa(user)
        return self.standard_auth(user)
    
    def require_mfa(self, user):
        factors = []
        
        # Factor 1: Password (something you know)
        factors.append(self.verify_password(user))
        
        # Factor 2: TOTP/SMS (something you have)
        if user.mfa_method == 'totp':
            factors.append(self.verify_totp(user))
        elif user.mfa_method == 'sms':
            factors.append(self.verify_sms_code(user))
            
        # Factor 3: Biometric (something you are) - optional
        if user.biometric_enabled:
            factors.append(self.verify_biometric(user))
            
        return all(factors)
''',
            'authorization': '''
# Role-based access control
class RBACAuthorization:
    def __init__(self):
        self.roles = {
            'admin': ['read', 'write', 'delete', 'admin'],
            'user': ['read', 'write:own'],
            'viewer': ['read']
        }
        
    def check_permission(self, user, resource, action):
        user_permissions = self.get_user_permissions(user)
        
        # Check explicit permissions
        if action in user_permissions:
            return True
            
        # Check ownership-based permissions
        if f"{action}:own" in user_permissions:
            return self.user_owns_resource(user, resource)
            
        # Log denied access attempt
        self.log_access_denied(user, resource, action)
        return False
''',
            'encryption': '''
# Encryption at rest and in transit
class EncryptionControls:
    def __init__(self):
        self.kms = KeyManagementService()
        
    def encrypt_at_rest(self, data, classification):
        if classification == 'sensitive':
            # Use envelope encryption
            dek = self.kms.generate_data_encryption_key()
            encrypted_data = self.encrypt_with_key(data, dek)
            encrypted_dek = self.kms.encrypt_key(dek)
            
            return {
                'data': encrypted_data,
                'encrypted_key': encrypted_dek,
                'algorithm': 'AES-256-GCM',
                'key_id': self.kms.get_current_key_id()
            }
    
    def configure_tls(self):
        return {
            'min_version': 'TLS1.2',
            'ciphers': [
                'ECDHE-RSA-AES256-GCM-SHA384',
                'ECDHE-RSA-AES128-GCM-SHA256'
            ],
            'hsts': 'max-age=31536000; includeSubDomains',
            'certificate_pinning': True
        }
'''
        }
        
        return controls
```

### 4. Audit Logging and Monitoring

Implement comprehensive audit trails:

**Audit Log System**
```python
class ComplianceAuditLogger:
    def __init__(self):
        self.required_events = {
            'authentication': [
                'login_success',
                'login_failure',
                'logout',
                'password_change',
                'mfa_enabled',
                'mfa_disabled'
            ],
            'authorization': [
                'access_granted',
                'access_denied',
                'permission_changed',
                'role_assigned',
                'role_revoked'
            ],
            'data_access': [
                'data_viewed',
                'data_exported',
                'data_modified',
                'data_deleted',
                'bulk_operation'
            ],
            'compliance': [
                'consent_given',
                'consent_withdrawn',
                'data_request',
                'data_erasure',
                'privacy_settings_changed'
            ]
        }
    
    def log_event(self, event_type, details):
        """
        Create tamper-proof audit log entry
        """
        log_entry = {
            'id': str(uuid.uuid4()),
            'timestamp': datetime.utcnow().isoformat(),
            'event_type': event_type,
            'user_id': details.get('user_id'),
            'ip_address': self._get_ip_address(),
            'user_agent': request.headers.get('User-Agent'),
            'session_id': session.get('id'),
            'details': details,
            'compliance_flags': self._get_compliance_flags(event_type)
        }
        
        # Add integrity check
        log_entry['checksum'] = self._calculate_checksum(log_entry)
        
        # Store in immutable log
        self._store_audit_log(log_entry)
        
        # Real-time alerting for critical events
        if self._is_critical_event(event_type):
            self._send_security_alert(log_entry)
        
        return log_entry
    
    def _calculate_checksum(self, entry):
        """
        Create tamper-evident checksum
        """
        # Include previous entry hash for blockchain-like integrity
        previous_hash = self._get_previous_entry_hash()
        
        content = json.dumps(entry, sort_keys=True)
        return hashlib.sha256(
            f"{previous_hash}{content}{SECRET_KEY}".encode()
        ).hexdigest()
```

**Compliance Reporting**
```python
def generate_compliance_report(self, regulation, period):
    """
    Generate compliance report for auditors
    """
    report = {
        'regulation': regulation,
        'period': period,
        'generated_at': datetime.utcnow(),
        'sections': {}
    }
    
    if regulation == 'GDPR':
        report['sections'] = {
            'data_processing_activities': self._get_processing_activities(period),
            'consent_metrics': self._get_consent_metrics(period),
            'data_requests': {
                'access_requests': self._count_access_requests(period),
                'erasure_requests': self._count_erasure_requests(period),
                'portability_requests': self._count_portability_requests(period),
                'response_times': self._calculate_response_times(period)
            },
            'data_breaches': self._get_breach_reports(period),
            'third_party_processors': self._list_processors(),
            'privacy_impact_assessments': self._get_dpias(period)
        }
    
    elif regulation == 'HIPAA':
        report['sections'] = {
            'access_controls': self._audit_access_controls(period),
            'phi_access_log': self._get_phi_access_log(period),
            'risk_assessments': self._get_risk_assessments(period),
            'training_records': self._get_training_compliance(period),
            'business_associates': self._list_bas_with_agreements(),
            'incident_response': self._get_incident_reports(period)
        }
    
    return report
```

### 5. Healthcare Compliance (HIPAA)

Implement HIPAA-specific controls:

**PHI Protection**
```python
class HIPAACompliance:
    def protect_phi(self):
        """
        Implement HIPAA safeguards for Protected Health Information
        """
        # Technical Safeguards
        technical_controls = {
            'access_control': '''
class PHIAccessControl:
    def __init__(self):
        self.minimum_necessary_rule = True
        
    def grant_phi_access(self, user, patient_id, purpose):
        """
        Implement minimum necessary standard
        """
        # Verify legitimate purpose
        if not self._verify_treatment_relationship(user, patient_id, purpose):
            self._log_denied_access(user, patient_id, purpose)
            raise PermissionError("No treatment relationship")
        
        # Grant limited access based on role and purpose
        access_scope = self._determine_access_scope(user.role, purpose)
        
        # Time-limited access
        access_token = {
            'user_id': user.id,
            'patient_id': patient_id,
            'scope': access_scope,
            'purpose': purpose,
            'expires_at': datetime.utcnow() + timedelta(hours=24),
            'audit_id': str(uuid.uuid4())
        }
        
        # Log all access
        self._log_phi_access(access_token)
        
        return access_token
''',
            'encryption': '''
class PHIEncryption:
    def encrypt_phi_at_rest(self, phi_data):
        """
        HIPAA-compliant encryption for PHI
        """
        # Use FIPS 140-2 validated encryption
        encryption_config = {
            'algorithm': 'AES-256-CBC',
            'key_derivation': 'PBKDF2',
            'iterations': 100000,
            'validation': 'FIPS-140-2-Level-2'
        }
        
        # Encrypt PHI fields
        encrypted_phi = {}
        for field, value in phi_data.items():
            if self._is_phi_field(field):
                encrypted_phi[field] = self._encrypt_field(value, encryption_config)
            else:
                encrypted_phi[field] = value
        
        return encrypted_phi
    
    def secure_phi_transmission(self):
        """
        Secure PHI during transmission
        """
        return {
            'protocols': ['TLS 1.2+'],
            'vpn_required': True,
            'email_encryption': 'S/MIME or PGP required',
            'fax_alternative': 'Secure messaging portal'
        }
'''
        }
        
        # Administrative Safeguards
        admin_controls = {
            'workforce_training': '''
class HIPAATraining:
    def track_training_compliance(self, employee):
        """
        Ensure workforce HIPAA training compliance
        """
        required_modules = [
            'HIPAA Privacy Rule',
            'HIPAA Security Rule', 
            'PHI Handling Procedures',
            'Breach Notification',
            'Patient Rights',
            'Minimum Necessary Standard'
        ]
        
        training_status = {
            'employee_id': employee.id,
            'completed_modules': [],
            'pending_modules': [],
            'last_training_date': None,
            'next_due_date': None
        }
        
        for module in required_modules:
            completion = self._check_module_completion(employee.id, module)
            if completion and completion['date'] > datetime.now() - timedelta(days=365):
                training_status['completed_modules'].append(module)
            else:
                training_status['pending_modules'].append(module)
        
        return training_status
'''
        }
        
        return {
            'technical': technical_controls,
            'administrative': admin_controls
        }
```

### 6. Payment Card Compliance (PCI-DSS)

Implement PCI-DSS requirements:

**PCI-DSS Controls**
```python
class PCIDSSCompliance:
    def implement_pci_controls(self):
        """
        Implement PCI-DSS v4.0 requirements
        """
        controls = {
            'cardholder_data_protection': '''
class CardDataProtection:
    def __init__(self):
        # Never store these
        self.prohibited_data = ['cvv', 'cvv2', 'cvc2', 'cid', 'pin', 'pin_block']
        
    def handle_card_data(self, card_info):
        """
        PCI-DSS compliant card data handling
        """
        # Immediately tokenize
        token = self.tokenize_card(card_info)
        
        # If must store, only store allowed fields
        stored_data = {
            'token': token,
            'last_four': card_info['number'][-4:],
            'exp_month': card_info['exp_month'],
            'exp_year': card_info['exp_year'],
            'cardholder_name': self._encrypt(card_info['name'])
        }
        
        # Never log full card number
        self._log_transaction(token, 'XXXX-XXXX-XXXX-' + stored_data['last_four'])
        
        return stored_data
    
    def tokenize_card(self, card_info):
        """
        Replace PAN with token
        """
        # Use payment processor tokenization
        response = payment_processor.tokenize({
            'number': card_info['number'],
            'exp_month': card_info['exp_month'],
            'exp_year': card_info['exp_year']
        })
        
        return response['token']
''',
            'network_segmentation': '''
# Network segmentation for PCI compliance
class PCINetworkSegmentation:
    def configure_network_zones(self):
        """
        Implement network segmentation
        """
        zones = {
            'cde': {  # Cardholder Data Environment
                'description': 'Systems that process, store, or transmit CHD',
                'controls': [
                    'Firewall required',
                    'IDS/IPS monitoring',
                    'No direct internet access',
                    'Quarterly vulnerability scans',
                    'Annual penetration testing'
                ]
            },
            'dmz': {
                'description': 'Public-facing systems',
                'controls': [
                    'Web application firewall',
                    'No CHD storage allowed',
                    'Regular security scanning'
                ]
            },
            'internal': {
                'description': 'Internal corporate network',
                'controls': [
                    'Segmented from CDE',
                    'Limited CDE access',
                    'Standard security controls'
                ]
            }
        }
        
        return zones
''',
            'vulnerability_management': '''
class PCIVulnerabilityManagement:
    def quarterly_scan_requirements(self):
        """
        PCI-DSS quarterly scan requirements
        """
        scan_config = {
            'internal_scans': {
                'frequency': 'quarterly',
                'scope': 'all CDE systems',
                'tool': 'PCI-approved scanning vendor',
                'passing_criteria': 'No high-risk vulnerabilities'
            },
            'external_scans': {
                'frequency': 'quarterly', 
                'performed_by': 'ASV (Approved Scanning Vendor)',
                'scope': 'All external-facing IP addresses',
                'passing_criteria': 'Clean scan with no failures'
            },
            'remediation_timeline': {
                'critical': '24 hours',
                'high': '7 days',
                'medium': '30 days',
                'low': '90 days'
            }
        }
        
        return scan_config
'''
        }
        
        return controls
```

### 7. Continuous Compliance Monitoring

Set up automated compliance monitoring:

**Compliance Dashboard**
```python
class ComplianceDashboard:
    def generate_realtime_dashboard(self):
        """
        Real-time compliance status dashboard
        """
        dashboard = {
            'timestamp': datetime.utcnow(),
            'overall_compliance_score': 0,
            'regulations': {}
        }
        
        # GDPR Compliance Metrics
        dashboard['regulations']['GDPR'] = {
            'score': self.calculate_gdpr_score(),
            'status': 'COMPLIANT',
            'metrics': {
                'consent_rate': '87%',
                'data_requests_sla': '98% within 30 days',
                'privacy_policy_version': '2.1',
                'last_dpia': '2025-06-15',
                'encryption_coverage': '100%',
                'third_party_agreements': '12/12 signed'
            },
            'issues': [
                {
                    'severity': 'medium',
                    'issue': 'Cookie consent banner update needed',
                    'due_date': '2025-08-01'
                }
            ]
        }
        
        # HIPAA Compliance Metrics
        dashboard['regulations']['HIPAA'] = {
            'score': self.calculate_hipaa_score(),
            'status': 'NEEDS_ATTENTION',
            'metrics': {
                'risk_assessment_current': True,
                'workforce_training_compliance': '94%',
                'baa_agreements': '8/8 current',
                'encryption_status': 'All PHI encrypted',
                'access_reviews': 'Completed 2025-06-30',
                'incident_response_tested': '2025-05-15'
            },
            'issues': [
                {
                    'severity': 'high',
                    'issue': '3 employees overdue for training',
                    'due_date': '2025-07-25'
                }
            ]
        }
        
        return dashboard
```

**Automated Compliance Checks**
```yaml
# .github/workflows/compliance-check.yml
name: Compliance Checks

on:
  push:
    branches: [main, develop]
  pull_request:
  schedule:
    - cron: '0 0 * * *'  # Daily compliance check

jobs:
  compliance-scan:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v3
    
    - name: GDPR Compliance Check
      run: |
        python scripts/compliance/gdpr_checker.py
        
    - name: Security Headers Check
      run: |
        python scripts/compliance/security_headers.py
        
    - name: Dependency License Check
      run: |
        license-checker --onlyAllow 'MIT;Apache-2.0;BSD-3-Clause;ISC'
        
    - name: PII Detection Scan
      run: |
        # Scan for hardcoded PII
        python scripts/compliance/pii_scanner.py
        
    - name: Encryption Verification
      run: |
        # Verify all sensitive data is encrypted
        python scripts/compliance/encryption_checker.py
        
    - name: Generate Compliance Report
      if: always()
      run: |
        python scripts/compliance/generate_report.py > compliance-report.json
        
    - name: Upload Compliance Report
      uses: actions/upload-artifact@v3
      with:
        name: compliance-report
        path: compliance-report.json
```

### 8. Compliance Documentation

Generate required documentation:

**Privacy Policy Generator**
```python
def generate_privacy_policy(company_info, data_practices):
    """
    Generate GDPR-compliant privacy policy
    """
    policy = f"""
# Privacy Policy

**Last Updated**: {datetime.now().strftime('%B %d, %Y')}

## 1. Data Controller
{company_info['name']}
{company_info['address']}
Email: {company_info['privacy_email']}
DPO: {company_info.get('dpo_contact', 'privacy@company.com')}

## 2. Data We Collect
{generate_data_collection_section(data_practices['data_types'])}

## 3. Legal Basis for Processing
{generate_legal_basis_section(data_practices['purposes'])}

## 4. Your Rights
Under GDPR, you have the following rights:
- Right to access your personal data
- Right to rectification 
- Right to erasure ('right to be forgotten')
- Right to restrict processing
- Right to data portability
- Right to object
- Rights related to automated decision making

## 5. Data Retention
{generate_retention_policy(data_practices['retention_periods'])}

## 6. International Transfers
{generate_transfer_section(data_practices['international_transfers'])}

## 7. Contact Us
To exercise your rights, contact: {company_info['privacy_email']}
"""
    
    return policy
```

## Output Format

1. **Compliance Assessment**: Current compliance status across all applicable regulations
2. **Gap Analysis**: Specific areas needing attention with severity ratings
3. **Implementation Plan**: Prioritized roadmap for achieving compliance
4. **Technical Controls**: Code implementations for required controls
5. **Policy Templates**: Privacy policies, consent forms, and notices
6. **Audit Procedures**: Scripts for continuous compliance monitoring
7. **Documentation**: Required records and evidence for auditors
8. **Training Materials**: Workforce compliance training resources

Focus on practical implementation that balances compliance requirements with business operations and user experience.
````

## File: tools/config-validate.md

````markdown
# Configuration Validation

You are a configuration management expert specializing in validating, testing, and ensuring the correctness of application configurations. Create comprehensive validation schemas, implement configuration testing strategies, and ensure configurations are secure, consistent, and error-free across all environments.

## Context
The user needs to validate configuration files, implement configuration schemas, ensure consistency across environments, and prevent configuration-related errors. Focus on creating robust validation rules, type safety, security checks, and automated validation processes.

## Requirements
$ARGUMENTS

## Instructions

### 1. Configuration Analysis

Analyze existing configuration structure and identify validation needs:

**Configuration Scanner**
```python
import os
import yaml
import json
import toml
import configparser
from pathlib import Path
from typing import Dict, List, Any, Set

class ConfigurationAnalyzer:
    def analyze_project(self, project_path: str) -> Dict[str, Any]:
        """
        Analyze project configuration files and patterns
        """
        analysis = {
            'config_files': self._find_config_files(project_path),
            'config_patterns': self._identify_patterns(project_path),
            'security_issues': self._check_security_issues(project_path),
            'consistency_issues': self._check_consistency(project_path),
            'validation_coverage': self._assess_validation(project_path),
            'recommendations': []
        }
        
        self._generate_recommendations(analysis)
        return analysis
    
    def _find_config_files(self, project_path: str) -> List[Dict]:
        """Find all configuration files in project"""
        config_patterns = [
            '**/*.json', '**/*.yaml', '**/*.yml', '**/*.toml',
            '**/*.ini', '**/*.conf', '**/*.config', '**/*.env*',
            '**/*.properties', '**/config.js', '**/config.ts'
        ]
        
        config_files = []
        for pattern in config_patterns:
            for file_path in Path(project_path).glob(pattern):
                if not self._should_ignore(file_path):
                    config_files.append({
                        'path': str(file_path),
                        'type': self._detect_config_type(file_path),
                        'size': file_path.stat().st_size,
                        'environment': self._detect_environment(file_path)
                    })
        
        return config_files
    
    def _check_security_issues(self, project_path: str) -> List[Dict]:
        """Check for security issues in configurations"""
        issues = []
        
        # Patterns that might indicate secrets
        secret_patterns = [
            r'(api[_-]?key|apikey)',
            r'(secret|password|passwd|pwd)',
            r'(token|auth)',
            r'(private[_-]?key)',
            r'(aws[_-]?access|aws[_-]?secret)',
            r'(database[_-]?url|db[_-]?connection)'
        ]
        
        for config_file in self._find_config_files(project_path):
            content = Path(config_file['path']).read_text()
            
            for pattern in secret_patterns:
                if re.search(pattern, content, re.IGNORECASE):
                    # Check if it's a placeholder or actual secret
                    if self._looks_like_real_secret(content, pattern):
                        issues.append({
                            'file': config_file['path'],
                            'type': 'potential_secret',
                            'pattern': pattern,
                            'severity': 'high'
                        })
        
        return issues
    
    def _check_consistency(self, project_path: str) -> List[Dict]:
        """Check configuration consistency across environments"""
        inconsistencies = []
        
        # Group configs by base name
        config_groups = defaultdict(list)
        for config in self._find_config_files(project_path):
            base_name = self._get_base_config_name(config['path'])
            config_groups[base_name].append(config)
        
        # Check each group for inconsistencies
        for base_name, configs in config_groups.items():
            if len(configs) > 1:
                keys_by_env = {}
                for config in configs:
                    env = config.get('environment', 'default')
                    keys = self._extract_config_keys(config['path'])
                    keys_by_env[env] = keys
                
                # Find missing keys
                all_keys = set()
                for keys in keys_by_env.values():
                    all_keys.update(keys)
                
                for env, keys in keys_by_env.items():
                    missing = all_keys - keys
                    if missing:
                        inconsistencies.append({
                            'config_group': base_name,
                            'environment': env,
                            'missing_keys': list(missing),
                            'severity': 'medium'
                        })
        
        return inconsistencies
```

### 2. Schema Definition and Validation

Implement configuration schema validation:

**JSON Schema Validator**
```typescript
// config-validator.ts
import Ajv from 'ajv';
import ajvFormats from 'ajv-formats';
import ajvKeywords from 'ajv-keywords';
import { JSONSchema7 } from 'json-schema';

interface ValidationResult {
  valid: boolean;
  errors?: Array<{
    path: string;
    message: string;
    keyword: string;
    params: any;
  }>;
}

export class ConfigValidator {
  private ajv: Ajv;
  private schemas: Map<string, JSONSchema7> = new Map();
  
  constructor() {
    this.ajv = new Ajv({
      allErrors: true,
      verbose: true,
      strict: false,
      coerceTypes: true
    });
    
    // Add formats support
    ajvFormats(this.ajv);
    ajvKeywords(this.ajv);
    
    // Add custom formats
    this.addCustomFormats();
  }
  
  private addCustomFormats() {
    // URL format with protocol validation
    this.ajv.addFormat('url-https', {
      type: 'string',
      validate: (data: string) => {
        try {
          const url = new URL(data);
          return url.protocol === 'https:';
        } catch {
          return false;
        }
      }
    });
    
    // Environment variable reference
    this.ajv.addFormat('env-var', {
      type: 'string',
      validate: /^\$\{[A-Z_][A-Z0-9_]*\}$/
    });
    
    // Semantic version
    this.ajv.addFormat('semver', {
      type: 'string',
      validate: /^\d+\.\d+\.\d+(-[0-9A-Za-z-]+(\.[0-9A-Za-z-]+)*)?$/
    });
    
    // Port number
    this.ajv.addFormat('port', {
      type: 'number',
      validate: (data: number) => data >= 1 && data <= 65535
    });
    
    // Duration format (e.g., "5m", "1h", "30s")
    this.ajv.addFormat('duration', {
      type: 'string',
      validate: /^\d+[smhd]$/
    });
  }
  
  registerSchema(name: string, schema: JSONSchema7): void {
    this.schemas.set(name, schema);
    this.ajv.addSchema(schema, name);
  }
  
  validate(configData: any, schemaName: string): ValidationResult {
    const validate = this.ajv.getSchema(schemaName);
    
    if (!validate) {
      throw new Error(`Schema '${schemaName}' not found`);
    }
    
    const valid = validate(configData);
    
    if (!valid && validate.errors) {
      return {
        valid: false,
        errors: validate.errors.map(error => ({
          path: error.instancePath || '/',
          message: error.message || 'Validation error',
          keyword: error.keyword,
          params: error.params
        }))
      };
    }
    
    return { valid: true };
  }
  
  generateSchema(sampleConfig: any): JSONSchema7 {
    // Auto-generate schema from sample configuration
    const schema: JSONSchema7 = {
      type: 'object',
      properties: {},
      required: []
    };
    
    for (const [key, value] of Object.entries(sampleConfig)) {
      schema.properties![key] = this.inferSchema(value);
      
      // Make all top-level properties required by default
      if (schema.required && !key.startsWith('optional_')) {
        schema.required.push(key);
      }
    }
    
    return schema;
  }
  
  private inferSchema(value: any): JSONSchema7 {
    if (value === null) {
      return { type: 'null' };
    }
    
    if (Array.isArray(value)) {
      return {
        type: 'array',
        items: value.length > 0 ? this.inferSchema(value[0]) : {}
      };
    }
    
    if (typeof value === 'object') {
      const properties: Record<string, JSONSchema7> = {};
      const required: string[] = [];
      
      for (const [k, v] of Object.entries(value)) {
        properties[k] = this.inferSchema(v);
        if (v !== null && v !== undefined) {
          required.push(k);
        }
      }
      
      return {
        type: 'object',
        properties,
        required
      };
    }
    
    // Infer format from value patterns
    if (typeof value === 'string') {
      if (value.match(/^https?:\/\//)) {
        return { type: 'string', format: 'uri' };
      }
      if (value.match(/^\d{4}-\d{2}-\d{2}$/)) {
        return { type: 'string', format: 'date' };
      }
      if (value.match(/^[a-f0-9]{8}-[a-f0-9]{4}-[a-f0-9]{4}-[a-f0-9]{4}-[a-f0-9]{12}$/i)) {
        return { type: 'string', format: 'uuid' };
      }
    }
    
    return { type: typeof value as JSONSchema7['type'] };
  }
}

// Example schemas
export const schemas = {
  database: {
    type: 'object',
    properties: {
      host: { type: 'string', format: 'hostname' },
      port: { type: 'integer', format: 'port' },
      database: { type: 'string', minLength: 1 },
      user: { type: 'string', minLength: 1 },
      password: { type: 'string', minLength: 8 },
      ssl: {
        type: 'object',
        properties: {
          enabled: { type: 'boolean' },
          ca: { type: 'string' },
          cert: { type: 'string' },
          key: { type: 'string' }
        },
        required: ['enabled']
      },
      pool: {
        type: 'object',
        properties: {
          min: { type: 'integer', minimum: 0 },
          max: { type: 'integer', minimum: 1 },
          idleTimeout: { type: 'string', format: 'duration' }
        }
      }
    },
    required: ['host', 'port', 'database', 'user', 'password'],
    additionalProperties: false
  },
  
  api: {
    type: 'object',
    properties: {
      server: {
        type: 'object',
        properties: {
          host: { type: 'string', default: '0.0.0.0' },
          port: { type: 'integer', format: 'port', default: 3000 },
          cors: {
            type: 'object',
            properties: {
              enabled: { type: 'boolean' },
              origins: {
                type: 'array',
                items: { type: 'string', format: 'uri' }
              },
              credentials: { type: 'boolean' }
            }
          }
        },
        required: ['port']
      },
      auth: {
        type: 'object',
        properties: {
          jwt: {
            type: 'object',
            properties: {
              secret: { type: 'string', minLength: 32 },
              expiresIn: { type: 'string', format: 'duration' },
              algorithm: {
                type: 'string',
                enum: ['HS256', 'HS384', 'HS512', 'RS256', 'RS384', 'RS512']
              }
            },
            required: ['secret', 'expiresIn']
          }
        }
      },
      rateLimit: {
        type: 'object',
        properties: {
          windowMs: { type: 'integer', minimum: 1000 },
          max: { type: 'integer', minimum: 1 },
          message: { type: 'string' }
        }
      }
    },
    required: ['server', 'auth']
  }
};
```

### 3. Environment-Specific Validation

Validate configurations across environments:

**Environment Validator**
```python
# environment_validator.py
from typing import Dict, List, Set, Any
import os
import re

class EnvironmentValidator:
    def __init__(self):
        self.environments = ['development', 'staging', 'production']
        self.environment_rules = self._define_environment_rules()
    
    def _define_environment_rules(self) -> Dict[str, Dict]:
        """Define environment-specific validation rules"""
        return {
            'development': {
                'allow_debug': True,
                'require_https': False,
                'allow_wildcards': True,
                'min_password_length': 8,
                'allowed_log_levels': ['debug', 'info', 'warn', 'error']
            },
            'staging': {
                'allow_debug': True,
                'require_https': True,
                'allow_wildcards': False,
                'min_password_length': 12,
                'allowed_log_levels': ['info', 'warn', 'error']
            },
            'production': {
                'allow_debug': False,
                'require_https': True,
                'allow_wildcards': False,
                'min_password_length': 16,
                'allowed_log_levels': ['warn', 'error'],
                'require_encryption': True,
                'require_backup': True
            }
        }
    
    def validate_config(self, config: Dict, environment: str) -> List[Dict]:
        """Validate configuration for specific environment"""
        if environment not in self.environment_rules:
            raise ValueError(f"Unknown environment: {environment}")
        
        rules = self.environment_rules[environment]
        violations = []
        
        # Check debug settings
        if not rules['allow_debug'] and config.get('debug', False):
            violations.append({
                'rule': 'no_debug_in_production',
                'message': 'Debug mode is not allowed in production',
                'severity': 'critical',
                'path': 'debug'
            })
        
        # Check HTTPS requirements
        if rules['require_https']:
            urls = self._extract_urls(config)
            for url_path, url in urls:
                if url.startswith('http://') and 'localhost' not in url:
                    violations.append({
                        'rule': 'require_https',
                        'message': f'HTTPS required for {url_path}',
                        'severity': 'high',
                        'path': url_path,
                        'value': url
                    })
        
        # Check log levels
        log_level = config.get('logging', {}).get('level')
        if log_level and log_level not in rules['allowed_log_levels']:
            violations.append({
                'rule': 'invalid_log_level',
                'message': f"Log level '{log_level}' not allowed in {environment}",
                'severity': 'medium',
                'path': 'logging.level',
                'allowed': rules['allowed_log_levels']
            })
        
        # Check production-specific requirements
        if environment == 'production':
            violations.extend(self._validate_production_requirements(config))
        
        return violations
    
    def _validate_production_requirements(self, config: Dict) -> List[Dict]:
        """Additional validation for production environment"""
        violations = []
        
        # Check encryption settings
        if not config.get('security', {}).get('encryption', {}).get('enabled'):
            violations.append({
                'rule': 'encryption_required',
                'message': 'Encryption must be enabled in production',
                'severity': 'critical',
                'path': 'security.encryption.enabled'
            })
        
        # Check backup configuration
        if not config.get('backup', {}).get('enabled'):
            violations.append({
                'rule': 'backup_required',
                'message': 'Backup must be configured for production',
                'severity': 'high',
                'path': 'backup.enabled'
            })
        
        # Check monitoring
        if not config.get('monitoring', {}).get('enabled'):
            violations.append({
                'rule': 'monitoring_required',
                'message': 'Monitoring must be enabled in production',
                'severity': 'high',
                'path': 'monitoring.enabled'
            })
        
        return violations
    
    def _extract_urls(self, obj: Any, path: str = '') -> List[tuple]:
        """Recursively extract URLs from configuration"""
        urls = []
        
        if isinstance(obj, dict):
            for key, value in obj.items():
                new_path = f"{path}.{key}" if path else key
                urls.extend(self._extract_urls(value, new_path))
        elif isinstance(obj, list):
            for i, item in enumerate(obj):
                new_path = f"{path}[{i}]"
                urls.extend(self._extract_urls(item, new_path))
        elif isinstance(obj, str) and re.match(r'^https?://', obj):
            urls.append((path, obj))
        
        return urls

# Cross-environment consistency checker
class ConsistencyChecker:
    def check_consistency(self, configs: Dict[str, Dict]) -> List[Dict]:
        """Check configuration consistency across environments"""
        issues = []
        
        # Get all unique keys across environments
        all_keys = set()
        env_keys = {}
        
        for env, config in configs.items():
            keys = self._flatten_keys(config)
            env_keys[env] = keys
            all_keys.update(keys)
        
        # Check for missing keys
        for env, keys in env_keys.items():
            missing_keys = all_keys - keys
            if missing_keys:
                issues.append({
                    'type': 'missing_keys',
                    'environment': env,
                    'keys': list(missing_keys),
                    'severity': 'medium'
                })
        
        # Check for type inconsistencies
        for key in all_keys:
            types_by_env = {}
            for env, config in configs.items():
                value = self._get_nested_value(config, key)
                if value is not None:
                    types_by_env[env] = type(value).__name__
            
            unique_types = set(types_by_env.values())
            if len(unique_types) > 1:
                issues.append({
                    'type': 'type_mismatch',
                    'key': key,
                    'types': types_by_env,
                    'severity': 'high'
                })
        
        return issues
```

### 4. Configuration Testing Framework

Implement configuration testing:

**Config Test Suite**
```typescript
// config-test.ts
import { describe, it, expect, beforeEach } from '@jest/globals';
import { ConfigValidator } from './config-validator';
import { loadConfig } from './config-loader';

interface ConfigTestCase {
  name: string;
  config: any;
  environment: string;
  expectedValid: boolean;
  expectedErrors?: string[];
}

export class ConfigTestSuite {
  private validator: ConfigValidator;
  
  constructor() {
    this.validator = new ConfigValidator();
  }
  
  async runTests(testCases: ConfigTestCase[]): Promise<TestResults> {
    const results: TestResults = {
      passed: 0,
      failed: 0,
      errors: []
    };
    
    for (const testCase of testCases) {
      try {
        const result = await this.runTestCase(testCase);
        if (result.passed) {
          results.passed++;
        } else {
          results.failed++;
          results.errors.push({
            testName: testCase.name,
            errors: result.errors
          });
        }
      } catch (error) {
        results.failed++;
        results.errors.push({
          testName: testCase.name,
          errors: [error.message]
        });
      }
    }
    
    return results;
  }
  
  private async runTestCase(testCase: ConfigTestCase): Promise<TestResult> {
    // Load and validate config
    const validationResult = this.validator.validate(
      testCase.config,
      testCase.environment
    );
    
    const result: TestResult = {
      passed: validationResult.valid === testCase.expectedValid,
      errors: []
    };
    
    // Check expected errors
    if (testCase.expectedErrors && validationResult.errors) {
      for (const expectedError of testCase.expectedErrors) {
        const found = validationResult.errors.some(
          error => error.message.includes(expectedError)
        );
        
        if (!found) {
          result.passed = false;
          result.errors.push(`Expected error not found: ${expectedError}`);
        }
      }
    }
    
    return result;
  }
}

// Jest test examples
describe('Configuration Validation', () => {
  let validator: ConfigValidator;
  
  beforeEach(() => {
    validator = new ConfigValidator();
  });
  
  describe('Database Configuration', () => {
    it('should validate valid database config', () => {
      const config = {
        host: 'localhost',
        port: 5432,
        database: 'myapp',
        user: 'dbuser',
        password: 'securepassword123'
      };
      
      const result = validator.validate(config, 'database');
      expect(result.valid).toBe(true);
    });
    
    it('should reject invalid port number', () => {
      const config = {
        host: 'localhost',
        port: 70000, // Invalid port
        database: 'myapp',
        user: 'dbuser',
        password: 'securepassword123'
      };
      
      const result = validator.validate(config, 'database');
      expect(result.valid).toBe(false);
      expect(result.errors?.[0].path).toBe('/port');
    });
    
    it('should require SSL in production', () => {
      const config = {
        host: 'prod-db.example.com',
        port: 5432,
        database: 'myapp',
        user: 'dbuser',
        password: 'securepassword123',
        ssl: { enabled: false }
      };
      
      const envValidator = new EnvironmentValidator();
      const violations = envValidator.validate_config(config, 'production');
      
      expect(violations).toContainEqual(
        expect.objectContaining({
          rule: 'ssl_required_in_production'
        })
      );
    });
  });
  
  describe('API Configuration', () => {
    it('should validate CORS settings', () => {
      const config = {
        server: {
          port: 3000,
          cors: {
            enabled: true,
            origins: ['https://example.com', 'https://app.example.com'],
            credentials: true
          }
        },
        auth: {
          jwt: {
            secret: 'a'.repeat(32),
            expiresIn: '1h',
            algorithm: 'HS256'
          }
        }
      };
      
      const result = validator.validate(config, 'api');
      expect(result.valid).toBe(true);
    });
    
    it('should reject short JWT secrets', () => {
      const config = {
        server: { port: 3000 },
        auth: {
          jwt: {
            secret: 'tooshort',
            expiresIn: '1h'
          }
        }
      };
      
      const result = validator.validate(config, 'api');
      expect(result.valid).toBe(false);
      expect(result.errors?.[0].path).toBe('/auth/jwt/secret');
    });
  });
});
```

### 5. Runtime Configuration Validation

Implement runtime validation and hot-reloading:

**Runtime Config Validator**
```typescript
// runtime-validator.ts
import { EventEmitter } from 'events';
import * as chokidar from 'chokidar';
import { ConfigValidator } from './config-validator';

export class RuntimeConfigValidator extends EventEmitter {
  private validator: ConfigValidator;
  private currentConfig: any;
  private watchers: Map<string, chokidar.FSWatcher> = new Map();
  private validationCache: Map<string, ValidationResult> = new Map();
  
  constructor() {
    super();
    this.validator = new ConfigValidator();
  }
  
  async initialize(configPath: string): Promise<void> {
    // Load initial config
    this.currentConfig = await this.loadAndValidate(configPath);
    
    // Setup file watcher for hot-reloading
    this.watchConfig(configPath);
  }
  
  private async loadAndValidate(configPath: string): Promise<any> {
    try {
      // Load config
      const config = await this.loadConfig(configPath);
      
      // Validate config
      const validationResult = this.validator.validate(
        config,
        this.detectEnvironment()
      );
      
      if (!validationResult.valid) {
        this.emit('validation:error', {
          path: configPath,
          errors: validationResult.errors
        });
        
        // In development, log errors but continue
        if (this.isDevelopment()) {
          console.error('Configuration validation errors:', validationResult.errors);
          return config;
        }
        
        // In production, throw error
        throw new ConfigValidationError(
          'Configuration validation failed',
          validationResult.errors
        );
      }
      
      this.emit('validation:success', { path: configPath });
      return config;
    } catch (error) {
      this.emit('validation:error', { path: configPath, error });
      throw error;
    }
  }
  
  private watchConfig(configPath: string): void {
    const watcher = chokidar.watch(configPath, {
      persistent: true,
      ignoreInitial: true
    });
    
    watcher.on('change', async () => {
      console.log(`Configuration file changed: ${configPath}`);
      
      try {
        const newConfig = await this.loadAndValidate(configPath);
        
        // Check if config actually changed
        if (JSON.stringify(newConfig) !== JSON.stringify(this.currentConfig)) {
          const oldConfig = this.currentConfig;
          this.currentConfig = newConfig;
          
          this.emit('config:changed', {
            oldConfig,
            newConfig,
            changedKeys: this.findChangedKeys(oldConfig, newConfig)
          });
        }
      } catch (error) {
        this.emit('config:error', { error });
      }
    });
    
    this.watchers.set(configPath, watcher);
  }
  
  private findChangedKeys(oldConfig: any, newConfig: any): string[] {
    const changed: string[] = [];
    
    const findDiff = (old: any, new_: any, path: string = '') => {
      // Check all keys in old config
      for (const key in old) {
        const currentPath = path ? `${path}.${key}` : key;
        
        if (!(key in new_)) {
          changed.push(`${currentPath} (removed)`);
        } else if (typeof old[key] === 'object' && typeof new_[key] === 'object') {
          findDiff(old[key], new_[key], currentPath);
        } else if (old[key] !== new_[key]) {
          changed.push(currentPath);
        }
      }
      
      // Check for new keys
      for (const key in new_) {
        if (!(key in old)) {
          const currentPath = path ? `${path}.${key}` : key;
          changed.push(`${currentPath} (added)`);
        }
      }
    };
    
    findDiff(oldConfig, newConfig);
    return changed;
  }
  
  validateValue(path: string, value: any): ValidationResult {
    // Use cached schema for performance
    const cacheKey = `${path}:${JSON.stringify(value)}`;
    if (this.validationCache.has(cacheKey)) {
      return this.validationCache.get(cacheKey)!;
    }
    
    // Extract schema for specific path
    const schema = this.getSchemaForPath(path);
    if (!schema) {
      return { valid: true }; // No schema defined for this path
    }
    
    const result = this.validator.validateValue(value, schema);
    this.validationCache.set(cacheKey, result);
    
    return result;
  }
  
  async shutdown(): Promise<void> {
    // Close all watchers
    for (const watcher of this.watchers.values()) {
      await watcher.close();
    }
    
    this.watchers.clear();
    this.validationCache.clear();
  }
}

// Type-safe configuration access
export class TypedConfig<T> {
  constructor(
    private config: T,
    private validator: RuntimeConfigValidator
  ) {}
  
  get<K extends keyof T>(key: K): T[K] {
    const value = this.config[key];
    
    // Validate on access in development
    if (process.env.NODE_ENV === 'development') {
      const result = this.validator.validateValue(String(key), value);
      if (!result.valid) {
        console.warn(`Invalid config value for ${String(key)}:`, result.errors);
      }
    }
    
    return value;
  }
  
  getOrDefault<K extends keyof T>(key: K, defaultValue: T[K]): T[K] {
    return this.config[key] ?? defaultValue;
  }
  
  require<K extends keyof T>(key: K): NonNullable<T[K]> {
    const value = this.config[key];
    
    if (value === null || value === undefined) {
      throw new Error(`Required configuration '${String(key)}' is missing`);
    }
    
    return value as NonNullable<T[K]>;
  }
}
```

### 6. Configuration Migration

Implement configuration migration and versioning:

**Config Migration System**
```python
# config_migration.py
from typing import Dict, List, Callable, Any
from abc import ABC, abstractmethod
import semver

class ConfigMigration(ABC):
    """Base class for configuration migrations"""
    
    @property
    @abstractmethod
    def version(self) -> str:
        """Target version for this migration"""
        pass
    
    @property
    @abstractmethod
    def description(self) -> str:
        """Description of what this migration does"""
        pass
    
    @abstractmethod
    def up(self, config: Dict) -> Dict:
        """Apply migration to config"""
        pass
    
    @abstractmethod
    def down(self, config: Dict) -> Dict:
        """Revert migration from config"""
        pass
    
    def validate(self, config: Dict) -> bool:
        """Validate config after migration"""
        return True

class ConfigMigrator:
    def __init__(self):
        self.migrations: List[ConfigMigration] = []
    
    def register_migration(self, migration: ConfigMigration):
        """Register a migration"""
        self.migrations.append(migration)
        # Sort by version
        self.migrations.sort(key=lambda m: semver.VersionInfo.parse(m.version))
    
    def migrate(self, config: Dict, target_version: str) -> Dict:
        """Migrate config to target version"""
        current_version = config.get('_version', '0.0.0')
        
        if semver.compare(current_version, target_version) == 0:
            return config  # Already at target version
        
        if semver.compare(current_version, target_version) > 0:
            # Downgrade
            return self._downgrade(config, current_version, target_version)
        else:
            # Upgrade
            return self._upgrade(config, current_version, target_version)
    
    def _upgrade(self, config: Dict, from_version: str, to_version: str) -> Dict:
        """Upgrade config from one version to another"""
        result = config.copy()
        
        for migration in self.migrations:
            if (semver.compare(migration.version, from_version) > 0 and
                semver.compare(migration.version, to_version) <= 0):
                
                print(f"Applying migration to v{migration.version}: {migration.description}")
                result = migration.up(result)
                
                if not migration.validate(result):
                    raise ValueError(f"Migration to v{migration.version} failed validation")
                
                result['_version'] = migration.version
        
        return result
    
    def _downgrade(self, config: Dict, from_version: str, to_version: str) -> Dict:
        """Downgrade config from one version to another"""
        result = config.copy()
        
        # Apply migrations in reverse order
        for migration in reversed(self.migrations):
            if (semver.compare(migration.version, to_version) > 0 and
                semver.compare(migration.version, from_version) <= 0):
                
                print(f"Reverting migration from v{migration.version}: {migration.description}")
                result = migration.down(result)
                
                # Update version to previous migration's version
                prev_version = self._get_previous_version(migration.version)
                result['_version'] = prev_version
        
        return result
    
    def _get_previous_version(self, version: str) -> str:
        """Get the version before the given version"""
        for i, migration in enumerate(self.migrations):
            if migration.version == version:
                return self.migrations[i-1].version if i > 0 else '0.0.0'
        return '0.0.0'

# Example migrations
class MigrationV1_0_0(ConfigMigration):
    @property
    def version(self) -> str:
        return '1.0.0'
    
    @property
    def description(self) -> str:
        return 'Initial configuration structure'
    
    def up(self, config: Dict) -> Dict:
        # Add default structure
        return {
            '_version': '1.0.0',
            'app': config.get('app', {}),
            'database': config.get('database', {}),
            'logging': config.get('logging', {'level': 'info'})
        }
    
    def down(self, config: Dict) -> Dict:
        # Remove version info
        result = config.copy()
        result.pop('_version', None)
        return result

class MigrationV1_1_0(ConfigMigration):
    @property
    def version(self) -> str:
        return '1.1.0'
    
    @property
    def description(self) -> str:
        return 'Split database config into read/write connections'
    
    def up(self, config: Dict) -> Dict:
        result = config.copy()
        
        # Transform single database config to read/write split
        if 'database' in result and not isinstance(result['database'], dict):
            old_db = result['database']
            result['database'] = {
                'write': old_db,
                'read': old_db.copy()  # Same as write initially
            }
        
        return result
    
    def down(self, config: Dict) -> Dict:
        result = config.copy()
        
        # Revert to single database config
        if 'database' in result and 'write' in result['database']:
            result['database'] = result['database']['write']
        
        return result

class MigrationV2_0_0(ConfigMigration):
    @property
    def version(self) -> str:
        return '2.0.0'
    
    @property
    def description(self) -> str:
        return 'Add security configuration section'
    
    def up(self, config: Dict) -> Dict:
        result = config.copy()
        
        # Add security section with defaults
        if 'security' not in result:
            result['security'] = {
                'encryption': {
                    'enabled': True,
                    'algorithm': 'AES-256-GCM'
                },
                'tls': {
                    'minVersion': '1.2',
                    'ciphers': ['TLS_AES_256_GCM_SHA384', 'TLS_AES_128_GCM_SHA256']
                }
            }
        
        return result
    
    def down(self, config: Dict) -> Dict:
        result = config.copy()
        result.pop('security', None)
        return result
```

### 7. Configuration Security

Implement secure configuration handling:

**Secure Config Manager**
```typescript
// secure-config.ts
import * as crypto from 'crypto';
import { SecretManagerServiceClient } from '@google-cloud/secret-manager';
import { KeyVaultSecret, SecretClient } from '@azure/keyvault-secrets';

interface EncryptedValue {
  encrypted: true;
  value: string;
  algorithm: string;
  iv: string;
  authTag?: string;
}

export class SecureConfigManager {
  private secretsCache: Map<string, any> = new Map();
  private encryptionKey: Buffer;
  
  constructor(private options: SecureConfigOptions) {
    this.encryptionKey = this.deriveKey(options.masterKey);
  }
  
  private deriveKey(masterKey: string): Buffer {
    return crypto.pbkdf2Sync(masterKey, 'config-salt', 100000, 32, 'sha256');
  }
  
  encrypt(value: any): EncryptedValue {
    const algorithm = 'aes-256-gcm';
    const iv = crypto.randomBytes(16);
    const cipher = crypto.createCipheriv(algorithm, this.encryptionKey, iv);
    
    const stringValue = JSON.stringify(value);
    let encrypted = cipher.update(stringValue, 'utf8', 'hex');
    encrypted += cipher.final('hex');
    
    const authTag = cipher.getAuthTag();
    
    return {
      encrypted: true,
      value: encrypted,
      algorithm,
      iv: iv.toString('hex'),
      authTag: authTag.toString('hex')
    };
  }
  
  decrypt(encryptedValue: EncryptedValue): any {
    const decipher = crypto.createDecipheriv(
      encryptedValue.algorithm,
      this.encryptionKey,
      Buffer.from(encryptedValue.iv, 'hex')
    );
    
    if (encryptedValue.authTag) {
      decipher.setAuthTag(Buffer.from(encryptedValue.authTag, 'hex'));
    }
    
    let decrypted = decipher.update(encryptedValue.value, 'hex', 'utf8');
    decrypted += decipher.final('utf8');
    
    return JSON.parse(decrypted);
  }
  
  async processConfig(config: any): Promise<any> {
    const processed = {};
    
    for (const [key, value] of Object.entries(config)) {
      if (this.isEncryptedValue(value)) {
        // Decrypt encrypted values
        processed[key] = this.decrypt(value as EncryptedValue);
      } else if (this.isSecretReference(value)) {
        // Fetch from secret manager
        processed[key] = await this.fetchSecret(value as string);
      } else if (typeof value === 'object' && value !== null) {
        // Recursively process nested objects
        processed[key] = await this.processConfig(value);
      } else {
        processed[key] = value;
      }
    }
    
    return processed;
  }
  
  private isEncryptedValue(value: any): boolean {
    return typeof value === 'object' && 
           value !== null && 
           value.encrypted === true;
  }
  
  private isSecretReference(value: any): boolean {
    return typeof value === 'string' && 
           (value.startsWith('secret://') || 
            value.startsWith('vault://') ||
            value.startsWith('aws-secret://'));
  }
  
  private async fetchSecret(reference: string): Promise<any> {
    // Check cache first
    if (this.secretsCache.has(reference)) {
      return this.secretsCache.get(reference);
    }
    
    let secretValue: any;
    
    if (reference.startsWith('secret://')) {
      // Google Secret Manager
      secretValue = await this.fetchGoogleSecret(reference);
    } else if (reference.startsWith('vault://')) {
      // Azure Key Vault
      secretValue = await this.fetchAzureSecret(reference);
    } else if (reference.startsWith('aws-secret://')) {
      // AWS Secrets Manager
      secretValue = await this.fetchAWSSecret(reference);
    }
    
    // Cache the secret
    this.secretsCache.set(reference, secretValue);
    
    return secretValue;
  }
  
  private async fetchGoogleSecret(reference: string): Promise<any> {
    const secretName = reference.replace('secret://', '');
    const client = new SecretManagerServiceClient();
    
    const [version] = await client.accessSecretVersion({
      name: `projects/${this.options.gcpProject}/secrets/${secretName}/versions/latest`
    });
    
    const payload = version.payload?.data;
    if (!payload) {
      throw new Error(`Secret ${secretName} has no payload`);
    }
    
    return JSON.parse(payload.toString());
  }
  
  validateSecureConfig(config: any): ValidationResult {
    const errors: string[] = [];
    
    const checkSecrets = (obj: any, path: string = '') => {
      for (const [key, value] of Object.entries(obj)) {
        const currentPath = path ? `${path}.${key}` : key;
        
        // Check for plaintext secrets
        if (this.looksLikeSecret(key) && typeof value === 'string') {
          if (!this.isEncryptedValue(value) && !this.isSecretReference(value)) {
            errors.push(`Potential plaintext secret at ${currentPath}`);
          }
        }
        
        // Recursively check nested objects
        if (typeof value === 'object' && value !== null && !this.isEncryptedValue(value)) {
          checkSecrets(value, currentPath);
        }
      }
    };
    
    checkSecrets(config);
    
    return {
      valid: errors.length === 0,
      errors: errors.map(message => ({
        path: '',
        message,
        keyword: 'security',
        params: {}
      }))
    };
  }
  
  private looksLikeSecret(key: string): boolean {
    const secretPatterns = [
      'password', 'secret', 'key', 'token', 'credential',
      'api_key', 'apikey', 'private_key', 'auth'
    ];
    
    const lowerKey = key.toLowerCase();
    return secretPatterns.some(pattern => lowerKey.includes(pattern));
  }
}
```

### 8. Configuration Documentation

Generate configuration documentation:

**Config Documentation Generator**
```python
# config_docs_generator.py
from typing import Dict, List, Any
import json
import yaml

class ConfigDocGenerator:
    def generate_docs(self, schema: Dict, examples: Dict) -> str:
        """Generate comprehensive configuration documentation"""
        docs = ["# Configuration Reference\n"]
        
        # Add overview
        docs.append("## Overview\n")
        docs.append("This document describes all available configuration options.\n")
        
        # Add table of contents
        docs.append("## Table of Contents\n")
        toc = self._generate_toc(schema.get('properties', {}))
        docs.extend(toc)
        
        # Add configuration sections
        docs.append("\n## Configuration Options\n")
        sections = self._generate_sections(schema.get('properties', {}), examples)
        docs.extend(sections)
        
        # Add examples
        docs.append("\n## Complete Examples\n")
        docs.extend(self._generate_examples(examples))
        
        # Add validation rules
        docs.append("\n## Validation Rules\n")
        docs.extend(self._generate_validation_rules(schema))
        
        return '\n'.join(docs)
    
    def _generate_sections(self, properties: Dict, examples: Dict, level: int = 3) -> List[str]:
        """Generate documentation for each configuration section"""
        sections = []
        
        for prop_name, prop_schema in properties.items():
            # Section header
            sections.append(f"{'#' * level} {prop_name}\n")
            
            # Description
            if 'description' in prop_schema:
                sections.append(f"{prop_schema['description']}\n")
            
            # Type information
            sections.append(f"**Type:** `{prop_schema.get('type', 'any')}`\n")
            
            # Required
            if prop_name in prop_schema.get('required', []):
                sections.append("**Required:** Yes\n")
            
            # Default value
            if 'default' in prop_schema:
                sections.append(f"**Default:** `{json.dumps(prop_schema['default'])}`\n")
            
            # Validation constraints
            constraints = self._extract_constraints(prop_schema)
            if constraints:
                sections.append("**Constraints:**")
                for constraint in constraints:
                    sections.append(f"- {constraint}")
                sections.append("")
            
            # Example
            if prop_name in examples:
                sections.append("**Example:**")
                sections.append("```yaml")
                sections.append(yaml.dump({prop_name: examples[prop_name]}, default_flow_style=False))
                sections.append("```\n")
            
            # Nested properties
            if prop_schema.get('type') == 'object' and 'properties' in prop_schema:
                nested = self._generate_sections(
                    prop_schema['properties'],
                    examples.get(prop_name, {}),
                    level + 1
                )
                sections.extend(nested)
        
        return sections
    
    def _extract_constraints(self, schema: Dict) -> List[str]:
        """Extract validation constraints from schema"""
        constraints = []
        
        if 'enum' in schema:
            constraints.append(f"Must be one of: {', '.join(map(str, schema['enum']))}")
        
        if 'minimum' in schema:
            constraints.append(f"Minimum value: {schema['minimum']}")
        
        if 'maximum' in schema:
            constraints.append(f"Maximum value: {schema['maximum']}")
        
        if 'minLength' in schema:
            constraints.append(f"Minimum length: {schema['minLength']}")
        
        if 'maxLength' in schema:
            constraints.append(f"Maximum length: {schema['maxLength']}")
        
        if 'pattern' in schema:
            constraints.append(f"Must match pattern: `{schema['pattern']}`")
        
        if 'format' in schema:
            constraints.append(f"Format: {schema['format']}")
        
        return constraints

# Generate interactive config builder
class InteractiveConfigBuilder:
    def generate_html_builder(self, schema: Dict) -> str:
        """Generate interactive HTML configuration builder"""
        html = """
<!DOCTYPE html>
<html>
<head>
    <title>Configuration Builder</title>
    <style>
        body { font-family: Arial, sans-serif; margin: 20px; }
        .config-section { margin: 20px 0; padding: 20px; border: 1px solid #ddd; }
        .config-field { margin: 10px 0; }
        label { display: inline-block; width: 200px; font-weight: bold; }
        input, select { width: 300px; padding: 5px; }
        .error { color: red; font-size: 12px; }
        .preview { background: #f5f5f5; padding: 20px; margin-top: 20px; }
        pre { background: white; padding: 10px; overflow-x: auto; }
    </style>
</head>
<body>
    <h1>Configuration Builder</h1>
    <div id="config-form"></div>
    <button onclick="validateConfig()">Validate</button>
    <button onclick="exportConfig()">Export</button>
    
    <div class="preview">
        <h2>Preview</h2>
        <pre id="config-preview"></pre>
    </div>
    
    <script>
        const schema = """ + json.dumps(schema) + """;
        
        function buildForm() {
            const container = document.getElementById('config-form');
            container.innerHTML = renderSchema(schema.properties);
        }
        
        function renderSchema(properties, prefix = '') {
            let html = '';
            
            for (const [key, prop] of Object.entries(properties)) {
                const fieldId = prefix ? `${prefix}.${key}` : key;
                
                html += '<div class="config-field">';
                html += `<label for="${fieldId}">${key}:</label>`;
                
                if (prop.enum) {
                    html += `<select id="${fieldId}" onchange="updatePreview()">`;
                    for (const option of prop.enum) {
                        html += `<option value="${option}">${option}</option>`;
                    }
                    html += '</select>';
                } else if (prop.type === 'boolean') {
                    html += `<input type="checkbox" id="${fieldId}" onchange="updatePreview()">`;
                } else if (prop.type === 'number' || prop.type === 'integer') {
                    html += `<input type="number" id="${fieldId}" onchange="updatePreview()">`;
                } else {
                    html += `<input type="text" id="${fieldId}" onchange="updatePreview()">`;
                }
                
                html += `<div class="error" id="${fieldId}-error"></div>`;
                html += '</div>';
                
                if (prop.type === 'object' && prop.properties) {
                    html += '<div class="config-section">';
                    html += renderSchema(prop.properties, fieldId);
                    html += '</div>';
                }
            }
            
            return html;
        }
        
        function updatePreview() {
            const config = buildConfig();
            document.getElementById('config-preview').textContent = 
                JSON.stringify(config, null, 2);
        }
        
        function buildConfig() {
            // Build configuration from form values
            const config = {};
            // Implementation here
            return config;
        }
        
        function validateConfig() {
            // Validate against schema
            const config = buildConfig();
            // Implementation here
        }
        
        function exportConfig() {
            const config = buildConfig();
            const blob = new Blob([JSON.stringify(config, null, 2)], 
                                 {type: 'application/json'});
            const url = URL.createObjectURL(blob);
            const a = document.createElement('a');
            a.href = url;
            a.download = 'config.json';
            a.click();
        }
        
        // Initialize
        buildForm();
        updatePreview();
    </script>
</body>
</html>
        """
        return html
```

## Output Format

1. **Configuration Analysis**: Current configuration assessment
2. **Validation Schemas**: JSON Schema definitions for all configs
3. **Environment Rules**: Environment-specific validation rules
4. **Test Suite**: Comprehensive configuration tests
5. **Migration Scripts**: Version migration implementations
6. **Security Report**: Security issues and recommendations
7. **Documentation**: Auto-generated configuration reference
8. **Validation Pipeline**: CI/CD integration for config validation
9. **Interactive Tools**: Configuration builders and validators

Focus on preventing configuration errors, ensuring consistency across environments, and maintaining security best practices.
````

## File: tools/context-restore.md

````markdown
Restore saved project context for agent coordination:

[Extended thinking: This tool uses the context-manager agent to restore previously saved project context, enabling continuity across sessions and providing agents with comprehensive project knowledge.]

## Context Restoration Process

Use Task tool with subagent_type="context-manager" to restore and apply saved context.

Prompt: "Restore project context for: $ARGUMENTS. Perform the following:

1. **Locate Saved Context**
   - Find the most recent or specified context version
   - Validate context integrity
   - Check compatibility with current codebase

2. **Load Context Components**
   - Project overview and goals
   - Architectural decisions and rationale
   - Technology stack and patterns
   - Previous agent work and findings
   - Known issues and roadmap

3. **Apply Context**
   - Set up working environment based on context
   - Restore project-specific configurations
   - Load coding conventions and patterns
   - Prepare agent coordination history

4. **Validate Restoration**
   - Verify context applies to current code state
   - Identify any conflicts or outdated information
   - Flag areas that may need updates

5. **Prepare Summary**
   - Key points from restored context
   - Important decisions and patterns
   - Recent work and current focus
   - Suggested next steps

Return a comprehensive summary of the restored context and any issues encountered."

## Context Integration

The restored context will:
- Inform all subsequent agent invocations
- Maintain consistency with past decisions
- Provide historical knowledge to agents
- Enable seamless work continuation

## Usage Scenarios

Use context restoration when:
- Starting work after a break
- Switching between projects
- Onboarding to an existing project
- Needing historical project knowledge
- Coordinating complex multi-agent workflows

## Additional Options

- Restore specific context version: Include version timestamp
- Partial restoration: Restore only specific components
- Merge contexts: Combine multiple context versions
- Diff contexts: Compare current state with saved context

Context to restore: $ARGUMENTS
````

## File: tools/context-save.md

````markdown
Save current project context for future agent coordination:

[Extended thinking: This tool uses the context-manager agent to capture and preserve project state, decisions, and patterns. This enables better continuity across sessions and improved agent coordination.]

## Context Capture Process

Use Task tool with subagent_type="context-manager" to save comprehensive project context.

Prompt: "Save comprehensive project context for: $ARGUMENTS. Capture:

1. **Project Overview**
   - Project goals and objectives
   - Key architectural decisions
   - Technology stack and dependencies
   - Team conventions and patterns

2. **Current State**
   - Recently implemented features
   - Work in progress
   - Known issues and technical debt
   - Performance baselines

3. **Design Decisions**
   - Architectural choices and rationale
   - API design patterns
   - Database schema decisions
   - Security implementations

4. **Code Patterns**
   - Coding conventions used
   - Common patterns and abstractions
   - Testing strategies
   - Error handling approaches

5. **Agent Coordination History**
   - Which agents worked on what
   - Successful agent combinations
   - Agent-specific context and findings
   - Cross-agent dependencies

6. **Future Roadmap**
   - Planned features
   - Identified improvements
   - Technical debt to address
   - Performance optimization opportunities

Save this context in a structured format that can be easily restored and used by future agent invocations."

## Context Storage

The context will be saved to `.claude/context/` with:
- Timestamp-based versioning
- Structured JSON/Markdown format
- Easy restoration capabilities
- Context diffing between versions

## Usage Scenarios

This saved context enables:
- Resuming work after breaks
- Onboarding new team members
- Maintaining consistency across agent invocations
- Preserving architectural decisions
- Tracking project evolution

Context to save: $ARGUMENTS
````

## File: tools/cost-optimize.md

````markdown
# Cloud Cost Optimization

You are a cloud cost optimization expert specializing in reducing infrastructure expenses while maintaining performance and reliability. Analyze cloud spending, identify savings opportunities, and implement cost-effective architectures across AWS, Azure, and GCP.

## Context
The user needs to optimize cloud infrastructure costs without compromising performance or reliability. Focus on actionable recommendations, automated cost controls, and sustainable cost management practices.

## Requirements
$ARGUMENTS

## Instructions

### 1. Cost Analysis and Visibility

Implement comprehensive cost analysis:

**Cost Analysis Framework**
```python
import boto3
import pandas as pd
from datetime import datetime, timedelta
from typing import Dict, List, Any
import json

class CloudCostAnalyzer:
    def __init__(self, cloud_provider: str):
        self.provider = cloud_provider
        self.client = self._initialize_client()
        self.cost_data = None
        
    def analyze_costs(self, time_period: int = 30):
        """Comprehensive cost analysis"""
        analysis = {
            'total_cost': self._get_total_cost(time_period),
            'cost_by_service': self._analyze_by_service(time_period),
            'cost_by_resource': self._analyze_by_resource(time_period),
            'cost_trends': self._analyze_trends(time_period),
            'anomalies': self._detect_anomalies(time_period),
            'waste_analysis': self._identify_waste(),
            'optimization_opportunities': self._find_opportunities()
        }
        
        return self._generate_report(analysis)
    
    def _analyze_by_service(self, days: int):
        """Analyze costs by service"""
        if self.provider == 'aws':
            ce = boto3.client('ce')
            
            response = ce.get_cost_and_usage(
                TimePeriod={
                    'Start': (datetime.now() - timedelta(days=days)).strftime('%Y-%m-%d'),
                    'End': datetime.now().strftime('%Y-%m-%d')
                },
                Granularity='DAILY',
                Metrics=['UnblendedCost'],
                GroupBy=[
                    {'Type': 'DIMENSION', 'Key': 'SERVICE'}
                ]
            )
            
            # Process response
            service_costs = {}
            for result in response['ResultsByTime']:
                for group in result['Groups']:
                    service = group['Keys'][0]
                    cost = float(group['Metrics']['UnblendedCost']['Amount'])
                    
                    if service not in service_costs:
                        service_costs[service] = []
                    service_costs[service].append(cost)
            
            # Calculate totals and trends
            analysis = {}
            for service, costs in service_costs.items():
                analysis[service] = {
                    'total': sum(costs),
                    'average_daily': sum(costs) / len(costs),
                    'trend': self._calculate_trend(costs),
                    'percentage': (sum(costs) / self._get_total_cost(days)) * 100
                }
            
            return analysis
    
    def _identify_waste(self):
        """Identify wasted resources"""
        waste_analysis = {
            'unused_resources': self._find_unused_resources(),
            'oversized_resources': self._find_oversized_resources(),
            'unattached_storage': self._find_unattached_storage(),
            'idle_load_balancers': self._find_idle_load_balancers(),
            'old_snapshots': self._find_old_snapshots(),
            'untagged_resources': self._find_untagged_resources()
        }
        
        total_waste = sum(item['estimated_savings'] 
                         for category in waste_analysis.values() 
                         for item in category)
        
        waste_analysis['total_potential_savings'] = total_waste
        
        return waste_analysis
    
    def _find_unused_resources(self):
        """Find resources with no usage"""
        unused = []
        
        if self.provider == 'aws':
            # Check EC2 instances
            ec2 = boto3.client('ec2')
            cloudwatch = boto3.client('cloudwatch')
            
            instances = ec2.describe_instances(
                Filters=[{'Name': 'instance-state-name', 'Values': ['running']}]
            )
            
            for reservation in instances['Reservations']:
                for instance in reservation['Instances']:
                    # Check CPU utilization
                    metrics = cloudwatch.get_metric_statistics(
                        Namespace='AWS/EC2',
                        MetricName='CPUUtilization',
                        Dimensions=[
                            {'Name': 'InstanceId', 'Value': instance['InstanceId']}
                        ],
                        StartTime=datetime.now() - timedelta(days=7),
                        EndTime=datetime.now(),
                        Period=3600,
                        Statistics=['Average']
                    )
                    
                    if metrics['Datapoints']:
                        avg_cpu = sum(d['Average'] for d in metrics['Datapoints']) / len(metrics['Datapoints'])
                        
                        if avg_cpu < 5:  # Less than 5% CPU usage
                            unused.append({
                                'resource_type': 'EC2 Instance',
                                'resource_id': instance['InstanceId'],
                                'reason': f'Average CPU: {avg_cpu:.2f}%',
                                'estimated_savings': self._calculate_instance_cost(instance)
                            })
        
        return unused
```

### 2. Resource Rightsizing

Implement intelligent rightsizing:

**Rightsizing Engine**
```python
class ResourceRightsizer:
    def __init__(self):
        self.utilization_thresholds = {
            'cpu_low': 20,
            'cpu_high': 80,
            'memory_low': 30,
            'memory_high': 85,
            'network_low': 10,
            'network_high': 70
        }
    
    def analyze_rightsizing_opportunities(self):
        """Find rightsizing opportunities"""
        opportunities = {
            'ec2_instances': self._rightsize_ec2(),
            'rds_instances': self._rightsize_rds(),
            'containers': self._rightsize_containers(),
            'lambda_functions': self._rightsize_lambda(),
            'storage_volumes': self._rightsize_storage()
        }
        
        return self._prioritize_opportunities(opportunities)
    
    def _rightsize_ec2(self):
        """Rightsize EC2 instances"""
        recommendations = []
        
        instances = self._get_running_instances()
        
        for instance in instances:
            # Get utilization metrics
            utilization = self._get_instance_utilization(instance['InstanceId'])
            
            # Determine if oversized or undersized
            current_type = instance['InstanceType']
            recommended_type = self._recommend_instance_type(
                current_type, 
                utilization
            )
            
            if recommended_type != current_type:
                current_cost = self._get_instance_cost(current_type)
                new_cost = self._get_instance_cost(recommended_type)
                
                recommendations.append({
                    'resource_id': instance['InstanceId'],
                    'current_type': current_type,
                    'recommended_type': recommended_type,
                    'reason': self._generate_reason(utilization),
                    'current_cost': current_cost,
                    'new_cost': new_cost,
                    'monthly_savings': (current_cost - new_cost) * 730,
                    'effort': 'medium',
                    'risk': 'low' if 'downsize' in self._generate_reason(utilization) else 'medium'
                })
        
        return recommendations
    
    def _recommend_instance_type(self, current_type: str, utilization: Dict):
        """Recommend optimal instance type"""
        # Parse current instance family and size
        family, size = self._parse_instance_type(current_type)
        
        # Calculate required resources
        required_cpu = self._calculate_required_cpu(utilization['cpu'])
        required_memory = self._calculate_required_memory(utilization['memory'])
        
        # Find best matching instance
        instance_catalog = self._get_instance_catalog()
        
        candidates = []
        for instance_type, specs in instance_catalog.items():
            if (specs['vcpu'] >= required_cpu and 
                specs['memory'] >= required_memory):
                candidates.append({
                    'type': instance_type,
                    'cost': specs['cost'],
                    'vcpu': specs['vcpu'],
                    'memory': specs['memory'],
                    'efficiency_score': self._calculate_efficiency_score(
                        specs, required_cpu, required_memory
                    )
                })
        
        # Select best candidate
        if candidates:
            best = sorted(candidates, 
                         key=lambda x: (x['efficiency_score'], x['cost']))[0]
            return best['type']
        
        return current_type
    
    def create_rightsizing_automation(self):
        """Automated rightsizing implementation"""
        return '''
import boto3
from datetime import datetime
import logging

class AutomatedRightsizer:
    def __init__(self):
        self.ec2 = boto3.client('ec2')
        self.cloudwatch = boto3.client('cloudwatch')
        self.logger = logging.getLogger(__name__)
        
    def execute_rightsizing(self, recommendations: List[Dict], dry_run: bool = True):
        """Execute rightsizing recommendations"""
        results = []
        
        for recommendation in recommendations:
            try:
                if recommendation['risk'] == 'low' or self._get_approval(recommendation):
                    result = self._resize_instance(
                        recommendation['resource_id'],
                        recommendation['recommended_type'],
                        dry_run=dry_run
                    )
                    results.append(result)
            except Exception as e:
                self.logger.error(f"Failed to resize {recommendation['resource_id']}: {e}")
                
        return results
    
    def _resize_instance(self, instance_id: str, new_type: str, dry_run: bool):
        """Resize an EC2 instance"""
        # Create snapshot for rollback
        snapshot_id = self._create_snapshot(instance_id)
        
        try:
            # Stop instance
            if not dry_run:
                self.ec2.stop_instances(InstanceIds=[instance_id])
                self._wait_for_state(instance_id, 'stopped')
            
            # Change instance type
            self.ec2.modify_instance_attribute(
                InstanceId=instance_id,
                InstanceType={'Value': new_type},
                DryRun=dry_run
            )
            
            # Start instance
            if not dry_run:
                self.ec2.start_instances(InstanceIds=[instance_id])
                self._wait_for_state(instance_id, 'running')
            
            return {
                'instance_id': instance_id,
                'status': 'success',
                'new_type': new_type,
                'snapshot_id': snapshot_id
            }
            
        except Exception as e:
            # Rollback on failure
            if not dry_run:
                self._rollback_instance(instance_id, snapshot_id)
            raise
'''
```

### 3. Reserved Instances and Savings Plans

Optimize commitment-based discounts:

**Reservation Optimizer**
```python
class ReservationOptimizer:
    def __init__(self):
        self.usage_history = None
        self.existing_reservations = None
        
    def analyze_reservation_opportunities(self):
        """Analyze opportunities for reservations"""
        analysis = {
            'current_coverage': self._analyze_current_coverage(),
            'usage_patterns': self._analyze_usage_patterns(),
            'recommendations': self._generate_recommendations(),
            'roi_analysis': self._calculate_roi(),
            'risk_assessment': self._assess_commitment_risk()
        }
        
        return analysis
    
    def _analyze_usage_patterns(self):
        """Analyze historical usage patterns"""
        # Get 12 months of usage data
        usage_data = self._get_historical_usage(months=12)
        
        patterns = {
            'stable_workloads': [],
            'variable_workloads': [],
            'seasonal_patterns': [],
            'growth_trends': []
        }
        
        # Analyze each instance family
        for family in self._get_instance_families(usage_data):
            family_usage = self._filter_by_family(usage_data, family)
            
            # Calculate stability metrics
            stability = self._calculate_stability(family_usage)
            
            if stability['coefficient_of_variation'] < 0.1:
                patterns['stable_workloads'].append({
                    'family': family,
                    'average_usage': stability['mean'],
                    'min_usage': stability['min'],
                    'recommendation': 'reserved_instance',
                    'term': '3_year',
                    'payment': 'all_upfront'
                })
            elif stability['coefficient_of_variation'] < 0.3:
                patterns['variable_workloads'].append({
                    'family': family,
                    'average_usage': stability['mean'],
                    'baseline': stability['percentile_25'],
                    'recommendation': 'savings_plan',
                    'commitment': stability['percentile_25']
                })
            
            # Check for seasonal patterns
            if self._has_seasonal_pattern(family_usage):
                patterns['seasonal_patterns'].append({
                    'family': family,
                    'pattern': self._identify_seasonal_pattern(family_usage),
                    'recommendation': 'spot_with_savings_plan_baseline'
                })
        
        return patterns
    
    def _generate_recommendations(self):
        """Generate reservation recommendations"""
        recommendations = []
        
        patterns = self._analyze_usage_patterns()
        current_costs = self._calculate_current_costs()
        
        # Reserved Instance recommendations
        for workload in patterns['stable_workloads']:
            ri_options = self._calculate_ri_options(workload)
            
            for option in ri_options:
                savings = current_costs[workload['family']] - option['total_cost']
                
                if savings > 0:
                    recommendations.append({
                        'type': 'reserved_instance',
                        'family': workload['family'],
                        'quantity': option['quantity'],
                        'term': option['term'],
                        'payment': option['payment_option'],
                        'upfront_cost': option['upfront_cost'],
                        'monthly_cost': option['monthly_cost'],
                        'total_savings': savings,
                        'break_even_months': option['upfront_cost'] / (savings / 36),
                        'confidence': 'high'
                    })
        
        # Savings Plan recommendations
        for workload in patterns['variable_workloads']:
            sp_options = self._calculate_savings_plan_options(workload)
            
            for option in sp_options:
                recommendations.append({
                    'type': 'savings_plan',
                    'commitment_type': option['type'],
                    'hourly_commitment': option['commitment'],
                    'term': option['term'],
                    'estimated_savings': option['savings'],
                    'flexibility': option['flexibility_score'],
                    'confidence': 'medium'
                })
        
        return sorted(recommendations, key=lambda x: x.get('total_savings', 0), reverse=True)
    
    def create_reservation_dashboard(self):
        """Create reservation tracking dashboard"""
        return '''
<!DOCTYPE html>
<html>
<head>
    <title>Reservation & Savings Dashboard</title>
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
</head>
<body>
    <div class="dashboard">
        <div class="summary-cards">
            <div class="card">
                <h3>Current Coverage</h3>
                <div class="metric">{coverage_percentage}%</div>
                <div class="sub-metric">On-Demand: ${on_demand_cost}</div>
                <div class="sub-metric">Reserved: ${reserved_cost}</div>
            </div>
            
            <div class="card">
                <h3>Potential Savings</h3>
                <div class="metric">${potential_savings}/month</div>
                <div class="sub-metric">{recommendations_count} opportunities</div>
            </div>
            
            <div class="card">
                <h3>Expiring Soon</h3>
                <div class="metric">{expiring_count} RIs</div>
                <div class="sub-metric">Next 30 days</div>
            </div>
        </div>
        
        <div class="charts">
            <canvas id="coverageChart"></canvas>
            <canvas id="savingsChart"></canvas>
        </div>
        
        <div class="recommendations-table">
            <h3>Top Recommendations</h3>
            <table>
                <tr>
                    <th>Type</th>
                    <th>Resource</th>
                    <th>Term</th>
                    <th>Upfront</th>
                    <th>Monthly Savings</th>
                    <th>ROI</th>
                    <th>Action</th>
                </tr>
                {recommendation_rows}
            </table>
        </div>
    </div>
</body>
</html>
'''
```

### 4. Spot Instance Optimization

Leverage spot instances effectively:

**Spot Instance Manager**
```python
class SpotInstanceOptimizer:
    def __init__(self):
        self.spot_advisor = self._init_spot_advisor()
        self.interruption_handler = None
        
    def identify_spot_opportunities(self):
        """Identify workloads suitable for spot"""
        workloads = self._analyze_workloads()
        
        spot_candidates = {
            'batch_processing': [],
            'dev_test': [],
            'stateless_apps': [],
            'ci_cd': [],
            'data_processing': []
        }
        
        for workload in workloads:
            suitability = self._assess_spot_suitability(workload)
            
            if suitability['score'] > 0.7:
                spot_candidates[workload['type']].append({
                    'workload': workload['name'],
                    'current_cost': workload['cost'],
                    'spot_savings': workload['cost'] * 0.7,  # ~70% savings
                    'interruption_tolerance': suitability['interruption_tolerance'],
                    'recommended_strategy': self._recommend_spot_strategy(workload)
                })
        
        return spot_candidates
    
    def _recommend_spot_strategy(self, workload):
        """Recommend spot instance strategy"""
        if workload['interruption_tolerance'] == 'high':
            return {
                'strategy': 'spot_fleet_diverse',
                'instance_pools': 10,
                'allocation_strategy': 'capacity-optimized',
                'on_demand_base': 0,
                'spot_percentage': 100
            }
        elif workload['interruption_tolerance'] == 'medium':
            return {
                'strategy': 'mixed_instances',
                'on_demand_base': 25,
                'spot_percentage': 75,
                'spot_allocation': 'lowest-price'
            }
        else:
            return {
                'strategy': 'spot_with_fallback',
                'primary': 'spot',
                'fallback': 'on-demand',
                'checkpointing': True
            }
    
    def create_spot_configuration(self):
        """Create spot instance configuration"""
        return '''
# Terraform configuration for Spot instances
resource "aws_spot_fleet_request" "processing_fleet" {
  iam_fleet_role = aws_iam_role.spot_fleet.arn
  
  allocation_strategy = "diversified"
  target_capacity     = 100
  valid_until        = timeadd(timestamp(), "168h")
  
  # Define multiple launch specifications for diversity
  dynamic "launch_specification" {
    for_each = var.spot_instance_types
    
    content {
      instance_type     = launch_specification.value
      ami              = var.ami_id
      key_name         = var.key_name
      subnet_id        = var.subnet_ids[launch_specification.key % length(var.subnet_ids)]
      
      weighted_capacity = var.instance_weights[launch_specification.value]
      spot_price       = var.max_spot_prices[launch_specification.value]
      
      user_data = base64encode(templatefile("${path.module}/spot-init.sh", {
        interruption_handler = true
        checkpoint_s3_bucket = var.checkpoint_bucket
      }))
      
      tags = {
        Name = "spot-processing-${launch_specification.key}"
        Type = "spot"
      }
    }
  }
  
  # Interruption handling
  lifecycle {
    create_before_destroy = true
  }
}

# Spot interruption handler
resource "aws_lambda_function" "spot_interruption_handler" {
  filename         = "spot-handler.zip"
  function_name    = "spot-interruption-handler"
  role            = aws_iam_role.lambda_role.arn
  handler         = "handler.main"
  runtime         = "python3.9"
  
  environment {
    variables = {
      CHECKPOINT_BUCKET = var.checkpoint_bucket
      SNS_TOPIC_ARN    = aws_sns_topic.spot_interruptions.arn
    }
  }
}
'''
```

### 5. Storage Optimization

Optimize storage costs:

**Storage Optimizer**
```python
class StorageOptimizer:
    def analyze_storage_costs(self):
        """Comprehensive storage analysis"""
        analysis = {
            'ebs_volumes': self._analyze_ebs_volumes(),
            's3_buckets': self._analyze_s3_buckets(),
            'snapshots': self._analyze_snapshots(),
            'lifecycle_opportunities': self._find_lifecycle_opportunities(),
            'compression_opportunities': self._find_compression_opportunities()
        }
        
        return analysis
    
    def _analyze_s3_buckets(self):
        """Analyze S3 bucket costs and optimization"""
        s3 = boto3.client('s3')
        cloudwatch = boto3.client('cloudwatch')
        
        buckets = s3.list_buckets()['Buckets']
        bucket_analysis = []
        
        for bucket in buckets:
            bucket_name = bucket['Name']
            
            # Get storage metrics
            metrics = self._get_s3_metrics(bucket_name)
            
            # Analyze storage classes
            storage_class_distribution = self._get_storage_class_distribution(bucket_name)
            
            # Calculate optimization potential
            optimization = self._calculate_s3_optimization(
                bucket_name,
                metrics,
                storage_class_distribution
            )
            
            bucket_analysis.append({
                'bucket_name': bucket_name,
                'total_size_gb': metrics['size_gb'],
                'total_objects': metrics['object_count'],
                'current_cost': metrics['monthly_cost'],
                'storage_classes': storage_class_distribution,
                'optimization_recommendations': optimization['recommendations'],
                'potential_savings': optimization['savings']
            })
        
        return bucket_analysis
    
    def create_lifecycle_policies(self):
        """Create S3 lifecycle policies"""
        return '''
import boto3
from datetime import datetime

class S3LifecycleManager:
    def __init__(self):
        self.s3 = boto3.client('s3')
        
    def create_intelligent_lifecycle(self, bucket_name: str, access_patterns: Dict):
        """Create lifecycle policy based on access patterns"""
        
        rules = []
        
        # Intelligent tiering for unknown access patterns
        if access_patterns.get('unpredictable'):
            rules.append({
                'ID': 'intelligent-tiering',
                'Status': 'Enabled',
                'Transitions': [{
                    'Days': 1,
                    'StorageClass': 'INTELLIGENT_TIERING'
                }]
            })
        
        # Standard lifecycle for predictable patterns
        if access_patterns.get('predictable'):
            rules.append({
                'ID': 'standard-lifecycle',
                'Status': 'Enabled',
                'Transitions': [
                    {
                        'Days': 30,
                        'StorageClass': 'STANDARD_IA'
                    },
                    {
                        'Days': 90,
                        'StorageClass': 'GLACIER'
                    },
                    {
                        'Days': 180,
                        'StorageClass': 'DEEP_ARCHIVE'
                    }
                ]
            })
        
        # Delete old versions
        rules.append({
            'ID': 'delete-old-versions',
            'Status': 'Enabled',
            'NoncurrentVersionTransitions': [
                {
                    'NoncurrentDays': 30,
                    'StorageClass': 'GLACIER'
                }
            ],
            'NoncurrentVersionExpiration': {
                'NoncurrentDays': 90
            }
        })
        
        # Apply lifecycle configuration
        self.s3.put_bucket_lifecycle_configuration(
            Bucket=bucket_name,
            LifecycleConfiguration={'Rules': rules}
        )
        
        return rules
    
    def optimize_ebs_volumes(self):
        """Optimize EBS volume types and sizes"""
        ec2 = boto3.client('ec2')
        
        volumes = ec2.describe_volumes()['Volumes']
        optimizations = []
        
        for volume in volumes:
            # Analyze volume metrics
            iops_usage = self._get_volume_iops_usage(volume['VolumeId'])
            throughput_usage = self._get_volume_throughput_usage(volume['VolumeId'])
            
            current_type = volume['VolumeType']
            recommended_type = self._recommend_volume_type(
                iops_usage,
                throughput_usage,
                volume['Size']
            )
            
            if recommended_type != current_type:
                optimizations.append({
                    'volume_id': volume['VolumeId'],
                    'current_type': current_type,
                    'recommended_type': recommended_type,
                    'reason': self._get_optimization_reason(
                        current_type,
                        recommended_type,
                        iops_usage,
                        throughput_usage
                    ),
                    'monthly_savings': self._calculate_volume_savings(
                        volume,
                        recommended_type
                    )
                })
        
        return optimizations
'''
```

### 6. Network Cost Optimization

Reduce network transfer costs:

**Network Cost Optimizer**
```python
class NetworkCostOptimizer:
    def analyze_network_costs(self):
        """Analyze network transfer costs"""
        analysis = {
            'data_transfer_costs': self._analyze_data_transfer(),
            'nat_gateway_costs': self._analyze_nat_gateways(),
            'load_balancer_costs': self._analyze_load_balancers(),
            'vpc_endpoint_opportunities': self._find_vpc_endpoint_opportunities(),
            'cdn_optimization': self._analyze_cdn_usage()
        }
        
        return analysis
    
    def _analyze_data_transfer(self):
        """Analyze data transfer patterns and costs"""
        transfers = {
            'inter_region': self._get_inter_region_transfers(),
            'internet_egress': self._get_internet_egress(),
            'inter_az': self._get_inter_az_transfers(),
            'vpc_peering': self._get_vpc_peering_transfers()
        }
        
        recommendations = []
        
        # Analyze inter-region transfers
        if transfers['inter_region']['monthly_gb'] > 1000:
            recommendations.append({
                'type': 'region_consolidation',
                'description': 'Consider consolidating resources in fewer regions',
                'current_cost': transfers['inter_region']['monthly_cost'],
                'potential_savings': transfers['inter_region']['monthly_cost'] * 0.8
            })
        
        # Analyze internet egress
        if transfers['internet_egress']['monthly_gb'] > 10000:
            recommendations.append({
                'type': 'cdn_implementation',
                'description': 'Implement CDN to reduce origin egress',
                'current_cost': transfers['internet_egress']['monthly_cost'],
                'potential_savings': transfers['internet_egress']['monthly_cost'] * 0.6
            })
        
        return {
            'current_costs': transfers,
            'recommendations': recommendations
        }
    
    def create_network_optimization_script(self):
        """Script to implement network optimizations"""
        return '''
#!/usr/bin/env python3
import boto3
from collections import defaultdict

class NetworkOptimizer:
    def __init__(self):
        self.ec2 = boto3.client('ec2')
        self.cloudwatch = boto3.client('cloudwatch')
        
    def optimize_nat_gateways(self):
        """Consolidate and optimize NAT gateways"""
        # Get all NAT gateways
        nat_gateways = self.ec2.describe_nat_gateways()['NatGateways']
        
        # Group by VPC
        vpc_nat_gateways = defaultdict(list)
        for nat in nat_gateways:
            if nat['State'] == 'available':
                vpc_nat_gateways[nat['VpcId']].append(nat)
        
        optimizations = []
        
        for vpc_id, nats in vpc_nat_gateways.items():
            if len(nats) > 1:
                # Check if consolidation is possible
                traffic_analysis = self._analyze_nat_traffic(nats)
                
                if traffic_analysis['can_consolidate']:
                    optimizations.append({
                        'vpc_id': vpc_id,
                        'action': 'consolidate_nat',
                        'current_count': len(nats),
                        'recommended_count': traffic_analysis['recommended_count'],
                        'monthly_savings': (len(nats) - traffic_analysis['recommended_count']) * 45
                    })
        
        return optimizations
    
    def implement_vpc_endpoints(self):
        """Implement VPC endpoints for AWS services"""
        services_to_check = ['s3', 'dynamodb', 'ec2', 'sns', 'sqs']
        vpc_list = self.ec2.describe_vpcs()['Vpcs']
        
        implementations = []
        
        for vpc in vpc_list:
            vpc_id = vpc['VpcId']
            
            # Check existing endpoints
            existing = self._get_existing_endpoints(vpc_id)
            
            for service in services_to_check:
                if service not in existing:
                    # Check if service is being used
                    if self._is_service_used(vpc_id, service):
                        # Create VPC endpoint
                        endpoint = self._create_vpc_endpoint(vpc_id, service)
                        
                        implementations.append({
                            'vpc_id': vpc_id,
                            'service': service,
                            'endpoint_id': endpoint['VpcEndpointId'],
                            'estimated_savings': self._estimate_endpoint_savings(vpc_id, service)
                        })
        
        return implementations
    
    def optimize_cloudfront_distribution(self):
        """Optimize CloudFront for cost reduction"""
        cloudfront = boto3.client('cloudfront')
        
        distributions = cloudfront.list_distributions()
        optimizations = []
        
        for dist in distributions.get('DistributionList', {}).get('Items', []):
            # Analyze distribution patterns
            analysis = self._analyze_distribution(dist['Id'])
            
            if analysis['optimization_potential']:
                optimizations.append({
                    'distribution_id': dist['Id'],
                    'recommendations': [
                        {
                            'action': 'adjust_price_class',
                            'current': dist['PriceClass'],
                            'recommended': analysis['recommended_price_class'],
                            'savings': analysis['price_class_savings']
                        },
                        {
                            'action': 'optimize_cache_behaviors',
                            'cache_improvements': analysis['cache_improvements'],
                            'savings': analysis['cache_savings']
                        }
                    ]
                })
        
        return optimizations
'''
```

### 7. Container Cost Optimization

Optimize container workloads:

**Container Cost Optimizer**
```python
class ContainerCostOptimizer:
    def optimize_ecs_costs(self):
        """Optimize ECS/Fargate costs"""
        return {
            'cluster_optimization': self._optimize_clusters(),
            'task_rightsizing': self._rightsize_tasks(),
            'scheduling_optimization': self._optimize_scheduling(),
            'fargate_spot': self._implement_fargate_spot()
        }
    
    def _rightsize_tasks(self):
        """Rightsize ECS tasks"""
        ecs = boto3.client('ecs')
        cloudwatch = boto3.client('cloudwatch')
        
        clusters = ecs.list_clusters()['clusterArns']
        recommendations = []
        
        for cluster in clusters:
            # Get services
            services = ecs.list_services(cluster=cluster)['serviceArns']
            
            for service in services:
                # Get task definition
                service_detail = ecs.describe_services(
                    cluster=cluster,
                    services=[service]
                )['services'][0]
                
                task_def = service_detail['taskDefinition']
                
                # Analyze resource utilization
                utilization = self._analyze_task_utilization(cluster, service)
                
                # Generate recommendations
                if utilization['cpu']['average'] < 30 or utilization['memory']['average'] < 40:
                    recommendations.append({
                        'cluster': cluster,
                        'service': service,
                        'current_cpu': service_detail['cpu'],
                        'current_memory': service_detail['memory'],
                        'recommended_cpu': int(service_detail['cpu'] * 0.7),
                        'recommended_memory': int(service_detail['memory'] * 0.8),
                        'monthly_savings': self._calculate_task_savings(
                            service_detail,
                            utilization
                        )
                    })
        
        return recommendations
    
    def create_k8s_cost_optimization(self):
        """Kubernetes cost optimization"""
        return '''
apiVersion: v1
kind: ConfigMap
metadata:
  name: cost-optimization-config
data:
  vertical-pod-autoscaler.yaml: |
    apiVersion: autoscaling.k8s.io/v1
    kind: VerticalPodAutoscaler
    metadata:
      name: app-vpa
    spec:
      targetRef:
        apiVersion: apps/v1
        kind: Deployment
        name: app-deployment
      updatePolicy:
        updateMode: "Auto"
      resourcePolicy:
        containerPolicies:
        - containerName: app
          minAllowed:
            cpu: 100m
            memory: 128Mi
          maxAllowed:
            cpu: 2
            memory: 2Gi
  
  cluster-autoscaler-config.yaml: |
    apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: cluster-autoscaler
    spec:
      template:
        spec:
          containers:
          - image: k8s.gcr.io/autoscaling/cluster-autoscaler:v1.21.0
            name: cluster-autoscaler
            command:
            - ./cluster-autoscaler
            - --v=4
            - --stderrthreshold=info
            - --cloud-provider=aws
            - --skip-nodes-with-local-storage=false
            - --expander=priority
            - --node-group-auto-discovery=asg:tag=k8s.io/cluster-autoscaler/enabled,k8s.io/cluster-autoscaler/cluster-name
            - --scale-down-enabled=true
            - --scale-down-unneeded-time=10m
            - --scale-down-utilization-threshold=0.5
  
  spot-instance-handler.yaml: |
    apiVersion: apps/v1
    kind: DaemonSet
    metadata:
      name: aws-node-termination-handler
    spec:
      selector:
        matchLabels:
          app: aws-node-termination-handler
      template:
        spec:
          containers:
          - name: aws-node-termination-handler
            image: amazon/aws-node-termination-handler:v1.13.0
            env:
            - name: NODE_NAME
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
            - name: ENABLE_SPOT_INTERRUPTION_DRAINING
              value: "true"
            - name: ENABLE_SCHEDULED_EVENT_DRAINING
              value: "true"
'''
```

### 8. Serverless Cost Optimization

Optimize serverless workloads:

**Serverless Optimizer**
```python
class ServerlessOptimizer:
    def optimize_lambda_costs(self):
        """Optimize Lambda function costs"""
        lambda_client = boto3.client('lambda')
        cloudwatch = boto3.client('cloudwatch')
        
        functions = lambda_client.list_functions()['Functions']
        optimizations = []
        
        for function in functions:
            # Analyze function performance
            analysis = self._analyze_lambda_function(function)
            
            # Memory optimization
            if analysis['memory_optimization_possible']:
                optimizations.append({
                    'function_name': function['FunctionName'],
                    'type': 'memory_optimization',
                    'current_memory': function['MemorySize'],
                    'recommended_memory': analysis['optimal_memory'],
                    'estimated_savings': analysis['memory_savings']
                })
            
            # Timeout optimization
            if analysis['timeout_optimization_possible']:
                optimizations.append({
                    'function_name': function['FunctionName'],
                    'type': 'timeout_optimization',
                    'current_timeout': function['Timeout'],
                    'recommended_timeout': analysis['optimal_timeout'],
                    'risk_reduction': 'prevents unnecessary charges from hanging functions'
                })
        
        return optimizations
    
    def implement_lambda_cost_controls(self):
        """Implement Lambda cost controls"""
        return '''
import json
import boto3
from datetime import datetime

def lambda_cost_controller(event, context):
    """Lambda function to monitor and control Lambda costs"""
    
    cloudwatch = boto3.client('cloudwatch')
    lambda_client = boto3.client('lambda')
    
    # Get current month costs
    costs = get_current_month_lambda_costs()
    
    # Check against budget
    budget_limit = float(os.environ.get('MONTHLY_BUDGET', '1000'))
    
    if costs > budget_limit * 0.8:  # 80% of budget
        # Implement cost controls
        high_cost_functions = identify_high_cost_functions()
        
        for func in high_cost_functions:
            # Reduce concurrency
            lambda_client.put_function_concurrency(
                FunctionName=func['FunctionName'],
                ReservedConcurrentExecutions=max(
                    1, 
                    int(func['CurrentConcurrency'] * 0.5)
                )
            )
            
            # Alert
            send_cost_alert(func, costs, budget_limit)
    
    # Implement provisioned concurrency optimization
    optimize_provisioned_concurrency()
    
    return {
        'statusCode': 200,
        'body': json.dumps({
            'current_costs': costs,
            'budget_limit': budget_limit,
            'actions_taken': len(high_cost_functions)
        })
    }

def optimize_provisioned_concurrency():
    """Optimize provisioned concurrency based on usage patterns"""
    functions = get_functions_with_provisioned_concurrency()
    
    for func in functions:
        # Analyze invocation patterns
        patterns = analyze_invocation_patterns(func['FunctionName'])
        
        if patterns['predictable']:
            # Schedule provisioned concurrency
            create_scheduled_scaling(
                func['FunctionName'],
                patterns['peak_hours'],
                patterns['peak_concurrency']
            )
        else:
            # Consider removing provisioned concurrency
            if patterns['avg_cold_starts'] < 10:  # per minute
                remove_provisioned_concurrency(func['FunctionName'])
'''
```

### 9. Cost Allocation and Tagging

Implement cost allocation strategies:

**Cost Allocation Manager**
```python
class CostAllocationManager:
    def implement_tagging_strategy(self):
        """Implement comprehensive tagging strategy"""
        return {
            'required_tags': [
                {'key': 'Environment', 'values': ['prod', 'staging', 'dev', 'test']},
                {'key': 'CostCenter', 'values': 'dynamic'},
                {'key': 'Project', 'values': 'dynamic'},
                {'key': 'Owner', 'values': 'dynamic'},
                {'key': 'Department', 'values': 'dynamic'}
            ],
            'automation': self._create_tagging_automation(),
            'enforcement': self._create_tag_enforcement(),
            'reporting': self._create_cost_allocation_reports()
        }
    
    def _create_tagging_automation(self):
        """Automate resource tagging"""
        return '''
import boto3
from datetime import datetime

class AutoTagger:
    def __init__(self):
        self.tag_policies = self.load_tag_policies()
        
    def auto_tag_resources(self, event, context):
        """Auto-tag resources on creation"""
        
        # Parse CloudTrail event
        detail = event['detail']
        event_name = detail['eventName']
        
        # Map events to resource types
        if event_name.startswith('Create'):
            resource_arn = self.extract_resource_arn(detail)
            
            if resource_arn:
                # Determine tags
                tags = self.determine_tags(detail)
                
                # Apply tags
                self.apply_tags(resource_arn, tags)
                
                # Log tagging action
                self.log_tagging(resource_arn, tags)
    
    def determine_tags(self, event_detail):
        """Determine tags based on context"""
        tags = []
        
        # User-based tags
        user_identity = event_detail.get('userIdentity', {})
        if 'userName' in user_identity:
            tags.append({
                'Key': 'Creator',
                'Value': user_identity['userName']
            })
        
        # Time-based tags
        tags.append({
            'Key': 'CreatedDate',
            'Value': datetime.now().strftime('%Y-%m-%d')
        })
        
        # Environment inference
        if 'prod' in event_detail.get('sourceIPAddress', ''):
            env = 'prod'
        elif 'dev' in event_detail.get('sourceIPAddress', ''):
            env = 'dev'
        else:
            env = 'unknown'
            
        tags.append({
            'Key': 'Environment',
            'Value': env
        })
        
        return tags
    
    def create_cost_allocation_dashboard(self):
        """Create cost allocation dashboard"""
        return """
        SELECT 
            tags.environment,
            tags.department,
            tags.project,
            SUM(costs.amount) as total_cost,
            SUM(costs.amount) / SUM(SUM(costs.amount)) OVER () * 100 as percentage
        FROM 
            aws_costs costs
        JOIN 
            resource_tags tags ON costs.resource_id = tags.resource_id
        WHERE 
            costs.date >= DATE_TRUNC('month', CURRENT_DATE)
        GROUP BY 
            tags.environment,
            tags.department,
            tags.project
        ORDER BY 
            total_cost DESC
        """
'''
```

### 10. Cost Monitoring and Alerts

Implement proactive cost monitoring:

**Cost Monitoring System**
```python
class CostMonitoringSystem:
    def setup_cost_alerts(self):
        """Setup comprehensive cost alerting"""
        alerts = []
        
        # Budget alerts
        alerts.extend(self._create_budget_alerts())
        
        # Anomaly detection
        alerts.extend(self._create_anomaly_alerts())
        
        # Threshold alerts
        alerts.extend(self._create_threshold_alerts())
        
        # Forecast alerts
        alerts.extend(self._create_forecast_alerts())
        
        return alerts
    
    def _create_anomaly_alerts(self):
        """Create anomaly detection alerts"""
        ce = boto3.client('ce')
        
        # Create anomaly monitor
        monitor = ce.create_anomaly_monitor(
            AnomalyMonitor={
                'MonitorName': 'ServiceCostMonitor',
                'MonitorType': 'DIMENSIONAL',
                'MonitorDimension': 'SERVICE'
            }
        )
        
        # Create anomaly subscription
        subscription = ce.create_anomaly_subscription(
            AnomalySubscription={
                'SubscriptionName': 'CostAnomalyAlerts',
                'Threshold': 100.0,  # Alert on anomalies > $100
                'Frequency': 'DAILY',
                'MonitorArnList': [monitor['MonitorArn']],
                'Subscribers': [
                    {
                        'Type': 'EMAIL',
                        'Address': 'team@company.com'
                    },
                    {
                        'Type': 'SNS',
                        'Address': 'arn:aws:sns:us-east-1:123456789012:cost-alerts'
                    }
                ]
            }
        )
        
        return [monitor, subscription]
    
    def create_cost_dashboard(self):
        """Create executive cost dashboard"""
        return '''
<!DOCTYPE html>
<html>
<head>
    <title>Cloud Cost Dashboard</title>
    <script src="https://d3js.org/d3.v7.min.js"></script>
    <style>
        .metric-card {
            background: #f5f5f5;
            padding: 20px;
            margin: 10px;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        .alert { color: #d32f2f; }
        .warning { color: #f57c00; }
        .success { color: #388e3c; }
    </style>
</head>
<body>
    <div id="dashboard">
        <h1>Cloud Cost Optimization Dashboard</h1>
        
        <div class="summary-row">
            <div class="metric-card">
                <h3>Current Month Spend</h3>
                <div class="metric">${current_spend}</div>
                <div class="trend ${spend_trend_class}">${spend_trend}% vs last month</div>
            </div>
            
            <div class="metric-card">
                <h3>Projected Month End</h3>
                <div class="metric">${projected_spend}</div>
                <div class="budget-status">Budget: ${budget}</div>
            </div>
            
            <div class="metric-card">
                <h3>Optimization Opportunities</h3>
                <div class="metric">${total_savings_identified}</div>
                <div class="count">{opportunity_count} recommendations</div>
            </div>
            
            <div class="metric-card">
                <h3>Realized Savings</h3>
                <div class="metric">${realized_savings_mtd}</div>
                <div class="count">YTD: ${realized_savings_ytd}</div>
            </div>
        </div>
        
        <div class="charts-row">
            <div id="spend-trend-chart"></div>
            <div id="service-breakdown-chart"></div>
            <div id="optimization-progress-chart"></div>
        </div>
        
        <div class="recommendations-section">
            <h2>Top Optimization Recommendations</h2>
            <table id="recommendations-table">
                <thead>
                    <tr>
                        <th>Priority</th>
                        <th>Service</th>
                        <th>Recommendation</th>
                        <th>Monthly Savings</th>
                        <th>Effort</th>
                        <th>Action</th>
                    </tr>
                </thead>
                <tbody>
                    ${recommendation_rows}
                </tbody>
            </table>
        </div>
    </div>
    
    <script>
        // Real-time updates
        setInterval(updateDashboard, 60000);
        
        // Initialize charts
        initializeCharts();
    </script>
</body>
</html>
'''
```

## Output Format

1. **Cost Analysis Report**: Comprehensive breakdown of current cloud costs
2. **Optimization Recommendations**: Prioritized list of cost-saving opportunities
3. **Implementation Scripts**: Automated scripts for implementing optimizations
4. **Monitoring Dashboards**: Real-time cost tracking and alerting
5. **ROI Calculations**: Detailed savings projections and payback periods
6. **Risk Assessment**: Analysis of risks associated with each optimization
7. **Implementation Roadmap**: Phased approach to cost optimization
8. **Best Practices Guide**: Long-term cost management strategies

Focus on delivering immediate cost savings while establishing sustainable cost optimization practices that maintain performance and reliability standards.
````

## File: tools/data-pipeline.md

````markdown
# Data Pipeline Architecture

Design and implement a scalable data pipeline for: $ARGUMENTS

Create a production-ready data pipeline including:

1. **Data Ingestion**:
   - Multiple source connectors (APIs, databases, files, streams)
   - Schema evolution handling
   - Incremental/batch loading
   - Data quality checks at ingestion
   - Dead letter queue for failures

2. **Transformation Layer**:
   - ETL/ELT architecture decision
   - Apache Beam/Spark transformations
   - Data cleansing and normalization
   - Feature engineering pipeline
   - Business logic implementation

3. **Orchestration**:
   - Airflow/Prefect DAGs
   - Dependency management
   - Retry and failure handling
   - SLA monitoring
   - Dynamic pipeline generation

4. **Storage Strategy**:
   - Data lake architecture
   - Partitioning strategy
   - Compression choices
   - Retention policies
   - Hot/cold storage tiers

5. **Streaming Pipeline**:
   - Kafka/Kinesis integration
   - Real-time processing
   - Windowing strategies
   - Late data handling
   - Exactly-once semantics

6. **Data Quality**:
   - Automated testing
   - Data profiling
   - Anomaly detection
   - Lineage tracking
   - Quality metrics and dashboards

7. **Performance & Scale**:
   - Horizontal scaling
   - Resource optimization
   - Caching strategies
   - Query optimization
   - Cost management

Include monitoring, alerting, and data governance considerations. Make it cloud-agnostic with specific implementation examples for AWS/GCP/Azure.
````

## File: tools/data-validation.md

````markdown
# Data Validation Pipeline

Create a comprehensive data validation system for: $ARGUMENTS

Implement validation including:

1. **Schema Validation**:
   - Pydantic models for structure
   - JSON Schema generation
   - Type checking and coercion
   - Nested object validation
   - Custom validators

2. **Data Quality Checks**:
   - Null/missing value handling
   - Outlier detection
   - Statistical validation
   - Business rule enforcement
   - Referential integrity

3. **Data Profiling**:
   - Automatic type inference
   - Distribution analysis
   - Cardinality checks
   - Pattern detection
   - Anomaly identification

4. **Validation Rules**:
   - Field-level constraints
   - Cross-field validation
   - Temporal consistency
   - Format validation (email, phone, etc.)
   - Custom business logic

5. **Error Handling**:
   - Detailed error messages
   - Error categorization
   - Partial validation support
   - Error recovery strategies
   - Validation reports

6. **Performance**:
   - Streaming validation
   - Batch processing
   - Parallel validation
   - Caching strategies
   - Incremental validation

7. **Integration**:
   - API endpoint validation
   - Database constraints
   - Message queue validation
   - File upload validation
   - Real-time validation

Include data quality metrics, monitoring dashboards, and alerting. Make it extensible for custom validation rules.
````

## File: tools/db-migrate.md

````markdown
# Database Migration Strategy and Implementation

You are a database migration expert specializing in zero-downtime deployments, data integrity, and multi-database environments. Create comprehensive migration scripts with rollback strategies, validation checks, and performance optimization.

## Context
The user needs help with database migrations that ensure data integrity, minimize downtime, and provide safe rollback options. Focus on production-ready migration strategies that handle edge cases and large datasets.

## Requirements
$ARGUMENTS

## Instructions

### 1. Migration Analysis

Analyze the required database changes:

**Schema Changes**
- **Table Operations**
  - Create new tables
  - Drop unused tables
  - Rename tables
  - Alter table engines/options
  
- **Column Operations**
  - Add columns (nullable vs non-nullable)
  - Drop columns (with data preservation)
  - Rename columns
  - Change data types
  - Modify constraints
  
- **Index Operations**
  - Create indexes (online vs offline)
  - Drop indexes
  - Modify index types
  - Add composite indexes
  
- **Constraint Operations**
  - Foreign keys
  - Unique constraints
  - Check constraints
  - Default values

**Data Migrations**
- **Transformations**
  - Data type conversions
  - Normalization/denormalization
  - Calculated fields
  - Data cleaning
  
- **Relationships**
  - Moving data between tables
  - Splitting/merging tables
  - Creating junction tables
  - Handling orphaned records

### 2. Zero-Downtime Strategy

Implement migrations without service interruption:

**Expand-Contract Pattern**
```sql
-- Phase 1: Expand (backward compatible)
ALTER TABLE users ADD COLUMN email_verified BOOLEAN DEFAULT FALSE;
CREATE INDEX CONCURRENTLY idx_users_email_verified ON users(email_verified);

-- Phase 2: Migrate Data (in batches)
UPDATE users 
SET email_verified = (email_confirmation_token IS NOT NULL)
WHERE id IN (
  SELECT id FROM users 
  WHERE email_verified IS NULL 
  LIMIT 10000
);

-- Phase 3: Contract (after code deployment)
ALTER TABLE users DROP COLUMN email_confirmation_token;
```

**Blue-Green Schema Migration**
```python
# Step 1: Create new schema version
def create_v2_schema():
    """
    Create new tables with v2_ prefix
    """
    execute("""
        CREATE TABLE v2_orders (
            id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
            customer_id UUID NOT NULL,
            total_amount DECIMAL(10,2) NOT NULL,
            status VARCHAR(50) NOT NULL,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            metadata JSONB DEFAULT '{}'
        );
        
        CREATE INDEX idx_v2_orders_customer ON v2_orders(customer_id);
        CREATE INDEX idx_v2_orders_status ON v2_orders(status);
    """)

# Step 2: Sync data with dual writes
def enable_dual_writes():
    """
    Application writes to both old and new tables
    """
    # Trigger-based approach
    execute("""
        CREATE OR REPLACE FUNCTION sync_orders_to_v2() 
        RETURNS TRIGGER AS $$
        BEGIN
            INSERT INTO v2_orders (
                id, customer_id, total_amount, status, created_at
            ) VALUES (
                NEW.id, NEW.customer_id, NEW.amount, NEW.state, NEW.created
            ) ON CONFLICT (id) DO UPDATE SET
                total_amount = EXCLUDED.total_amount,
                status = EXCLUDED.status;
            RETURN NEW;
        END;
        $$ LANGUAGE plpgsql;
        
        CREATE TRIGGER sync_orders_trigger
        AFTER INSERT OR UPDATE ON orders
        FOR EACH ROW EXECUTE FUNCTION sync_orders_to_v2();
    """)

# Step 3: Backfill historical data
def backfill_data():
    """
    Copy historical data in batches
    """
    batch_size = 10000
    last_id = None
    
    while True:
        query = """
            INSERT INTO v2_orders (
                id, customer_id, total_amount, status, created_at
            )
            SELECT 
                id, customer_id, amount, state, created
            FROM orders
            WHERE ($1::uuid IS NULL OR id > $1)
            ORDER BY id
            LIMIT $2
            ON CONFLICT (id) DO NOTHING
            RETURNING id
        """
        
        results = execute(query, [last_id, batch_size])
        if not results:
            break
            
        last_id = results[-1]['id']
        time.sleep(0.1)  # Prevent overload

# Step 4: Switch reads
# Step 5: Switch writes  
# Step 6: Drop old schema
```

### 3. Migration Scripts

Generate version-controlled migration files:

**SQL Migrations**
```sql
-- migrations/001_add_user_preferences.up.sql
BEGIN;

-- Add new table
CREATE TABLE user_preferences (
    user_id UUID PRIMARY KEY REFERENCES users(id) ON DELETE CASCADE,
    theme VARCHAR(20) DEFAULT 'light',
    language VARCHAR(10) DEFAULT 'en',
    notifications JSONB DEFAULT '{"email": true, "push": false}',
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Add update trigger
CREATE TRIGGER update_user_preferences_updated_at
    BEFORE UPDATE ON user_preferences
    FOR EACH ROW
    EXECUTE FUNCTION update_updated_at_column();

-- Add indexes
CREATE INDEX idx_user_preferences_language ON user_preferences(language);

-- Seed default data
INSERT INTO user_preferences (user_id)
SELECT id FROM users
ON CONFLICT DO NOTHING;

COMMIT;

-- migrations/001_add_user_preferences.down.sql
BEGIN;

DROP TABLE IF EXISTS user_preferences CASCADE;

COMMIT;
```

**Framework Migrations (Rails/Django/Laravel)**
```python
# Django migration
from django.db import migrations, models
import django.contrib.postgres.fields

class Migration(migrations.Migration):
    dependencies = [
        ('app', '0010_previous_migration'),
    ]

    operations = [
        migrations.CreateModel(
            name='UserPreferences',
            fields=[
                ('user', models.OneToOneField(
                    'User', 
                    on_delete=models.CASCADE, 
                    primary_key=True
                )),
                ('theme', models.CharField(
                    max_length=20, 
                    default='light'
                )),
                ('language', models.CharField(
                    max_length=10, 
                    default='en',
                    db_index=True
                )),
                ('notifications', models.JSONField(
                    default=dict
                )),
                ('created_at', models.DateTimeField(
                    auto_now_add=True
                )),
                ('updated_at', models.DateTimeField(
                    auto_now=True
                )),
            ],
        ),
        
        # Custom SQL for complex operations
        migrations.RunSQL(
            sql=[
                """
                -- Forward migration
                UPDATE products 
                SET price_cents = CAST(price * 100 AS INTEGER)
                WHERE price_cents IS NULL;
                """,
            ],
            reverse_sql=[
                """
                -- Reverse migration
                UPDATE products 
                SET price = CAST(price_cents AS DECIMAL) / 100
                WHERE price IS NULL;
                """,
            ],
        ),
    ]
```

### 4. Data Integrity Checks

Implement comprehensive validation:

**Pre-Migration Validation**
```python
def validate_pre_migration():
    """
    Check data integrity before migration
    """
    checks = []
    
    # Check for NULL values in required fields
    null_check = execute("""
        SELECT COUNT(*) as count
        FROM users
        WHERE email IS NULL OR username IS NULL
    """)[0]['count']
    
    if null_check > 0:
        checks.append({
            'check': 'null_values',
            'status': 'FAILED',
            'message': f'{null_check} users with NULL email/username',
            'action': 'Fix NULL values before migration'
        })
    
    # Check for duplicate values
    duplicate_check = execute("""
        SELECT email, COUNT(*) as count
        FROM users
        GROUP BY email
        HAVING COUNT(*) > 1
    """)
    
    if duplicate_check:
        checks.append({
            'check': 'duplicates',
            'status': 'FAILED', 
            'message': f'{len(duplicate_check)} duplicate emails found',
            'action': 'Resolve duplicates before adding unique constraint'
        })
    
    # Check foreign key integrity
    orphan_check = execute("""
        SELECT COUNT(*) as count
        FROM orders o
        LEFT JOIN users u ON o.user_id = u.id
        WHERE u.id IS NULL
    """)[0]['count']
    
    if orphan_check > 0:
        checks.append({
            'check': 'orphaned_records',
            'status': 'WARNING',
            'message': f'{orphan_check} orders with non-existent users',
            'action': 'Clean up orphaned records'
        })
    
    return checks
```

**Post-Migration Validation**
```python
def validate_post_migration():
    """
    Verify migration success
    """
    validations = []
    
    # Row count validation
    old_count = execute("SELECT COUNT(*) FROM orders")[0]['count']
    new_count = execute("SELECT COUNT(*) FROM v2_orders")[0]['count']
    
    validations.append({
        'check': 'row_count',
        'expected': old_count,
        'actual': new_count,
        'status': 'PASS' if old_count == new_count else 'FAIL'
    })
    
    # Checksum validation
    old_checksum = execute("""
        SELECT 
            SUM(CAST(amount AS DECIMAL)) as total,
            COUNT(DISTINCT customer_id) as customers
        FROM orders
    """)[0]
    
    new_checksum = execute("""
        SELECT 
            SUM(total_amount) as total,
            COUNT(DISTINCT customer_id) as customers  
        FROM v2_orders
    """)[0]
    
    validations.append({
        'check': 'data_integrity',
        'status': 'PASS' if old_checksum == new_checksum else 'FAIL',
        'details': {
            'old': old_checksum,
            'new': new_checksum
        }
    })
    
    return validations
```

### 5. Rollback Procedures

Implement safe rollback strategies:

**Automatic Rollback**
```python
class MigrationRunner:
    def __init__(self, migration):
        self.migration = migration
        self.checkpoint = None
        
    def run_with_rollback(self):
        """
        Execute migration with automatic rollback on failure
        """
        try:
            # Create restore point
            self.checkpoint = self.create_checkpoint()
            
            # Run pre-checks
            pre_checks = self.migration.validate_pre()
            if any(c['status'] == 'FAILED' for c in pre_checks):
                raise MigrationError("Pre-validation failed", pre_checks)
            
            # Execute migration
            with transaction.atomic():
                self.migration.forward()
                
                # Run post-checks
                post_checks = self.migration.validate_post()
                if any(c['status'] == 'FAILED' for c in post_checks):
                    raise MigrationError("Post-validation failed", post_checks)
                    
            # Clean up checkpoint after success
            self.cleanup_checkpoint()
            
        except Exception as e:
            logger.error(f"Migration failed: {e}")
            self.rollback()
            raise
            
    def rollback(self):
        """
        Restore to checkpoint
        """
        if self.checkpoint:
            execute(f"RESTORE DATABASE FROM CHECKPOINT '{self.checkpoint}'")
```

**Manual Rollback Scripts**
```bash
#!/bin/bash
# rollback_migration.sh

MIGRATION_VERSION=$1
DATABASE=$2

echo "Rolling back migration $MIGRATION_VERSION on $DATABASE"

# Check current version
CURRENT_VERSION=$(psql -d $DATABASE -t -c "SELECT version FROM schema_migrations ORDER BY version DESC LIMIT 1")

if [ "$CURRENT_VERSION" != "$MIGRATION_VERSION" ]; then
    echo "Error: Current version ($CURRENT_VERSION) doesn't match rollback version ($MIGRATION_VERSION)"
    exit 1
fi

# Execute rollback
psql -d $DATABASE -f "migrations/${MIGRATION_VERSION}.down.sql"

# Update version table
psql -d $DATABASE -c "DELETE FROM schema_migrations WHERE version = '$MIGRATION_VERSION'"

echo "Rollback completed successfully"
```

### 6. Performance Optimization

Minimize migration impact:

**Batch Processing**
```python
def migrate_large_table(batch_size=10000):
    """
    Migrate large tables in batches
    """
    total_rows = execute("SELECT COUNT(*) FROM source_table")[0]['count']
    processed = 0
    
    while processed < total_rows:
        # Process batch
        execute("""
            INSERT INTO target_table (columns...)
            SELECT columns...
            FROM source_table
            ORDER BY id
            OFFSET %s
            LIMIT %s
            ON CONFLICT DO NOTHING
        """, [processed, batch_size])
        
        processed += batch_size
        
        # Progress tracking
        progress = (processed / total_rows) * 100
        logger.info(f"Migration progress: {progress:.1f}%")
        
        # Prevent overload
        time.sleep(0.5)
```

**Index Management**
```sql
-- Drop indexes before bulk insert
ALTER TABLE large_table DROP INDEX idx_column1;
ALTER TABLE large_table DROP INDEX idx_column2;

-- Bulk insert
INSERT INTO large_table SELECT * FROM temp_data;

-- Recreate indexes concurrently
CREATE INDEX CONCURRENTLY idx_column1 ON large_table(column1);
CREATE INDEX CONCURRENTLY idx_column2 ON large_table(column2);
```

### 7. NoSQL and Cross-Platform Migration Support

Handle modern database migrations across SQL, NoSQL, and hybrid environments:

**Advanced Multi-Database Migration Framework**
```python
from abc import ABC, abstractmethod
from typing import Dict, List, Any, Optional
import asyncio
from dataclasses import dataclass

@dataclass
class MigrationOperation:
    operation_type: str
    collection_or_table: str
    data: Dict[str, Any]
    conditions: Optional[Dict[str, Any]] = None
    batch_size: int = 1000

class DatabaseAdapter(ABC):
    @abstractmethod
    async def connect(self, connection_string: str):
        pass
    
    @abstractmethod
    async def execute_migration(self, operation: MigrationOperation):
        pass
    
    @abstractmethod
    async def validate_migration(self, operation: MigrationOperation) -> bool:
        pass
    
    @abstractmethod
    async def rollback_migration(self, operation: MigrationOperation):
        pass

class MongoDBAdapter(DatabaseAdapter):
    def __init__(self):
        self.client = None
        self.db = None
    
    async def connect(self, connection_string: str):
        from motor.motor_asyncio import AsyncIOMotorClient
        self.client = AsyncIOMotorClient(connection_string)
        self.db = self.client.get_default_database()
    
    async def execute_migration(self, operation: MigrationOperation):
        collection = self.db[operation.collection_or_table]
        
        if operation.operation_type == 'add_field':
            await self._add_field(collection, operation)
        elif operation.operation_type == 'rename_field':
            await self._rename_field(collection, operation)
        elif operation.operation_type == 'migrate_data':
            await self._migrate_data(collection, operation)
        elif operation.operation_type == 'create_index':
            await self._create_index(collection, operation)
        elif operation.operation_type == 'schema_validation':
            await self._add_schema_validation(collection, operation)
    
    async def _add_field(self, collection, operation):
        """Add new field to all documents"""
        field_name = operation.data['field_name']
        default_value = operation.data.get('default_value')
        
        # Add field to documents that don't have it
        result = await collection.update_many(
            {field_name: {"$exists": False}},
            {"$set": {field_name: default_value}}
        )
        
        return {
            'matched_count': result.matched_count,
            'modified_count': result.modified_count
        }
    
    async def _rename_field(self, collection, operation):
        """Rename field across all documents"""
        old_name = operation.data['old_name']
        new_name = operation.data['new_name']
        
        result = await collection.update_many(
            {old_name: {"$exists": True}},
            {"$rename": {old_name: new_name}}
        )
        
        return {
            'matched_count': result.matched_count,
            'modified_count': result.modified_count
        }
    
    async def _migrate_data(self, collection, operation):
        """Transform data during migration"""
        pipeline = operation.data['pipeline']
        
        # Use aggregation pipeline for complex transformations
        cursor = collection.aggregate([
            {"$match": operation.conditions or {}},
            *pipeline,
            {"$merge": {
                "into": operation.collection_or_table,
                "on": "_id",
                "whenMatched": "replace"
            }}
        ])
        
        return [doc async for doc in cursor]
    
    async def _add_schema_validation(self, collection, operation):
        """Add JSON schema validation to collection"""
        schema = operation.data['schema']
        
        await self.db.command({
            "collMod": operation.collection_or_table,
            "validator": {"$jsonSchema": schema},
            "validationLevel": "strict",
            "validationAction": "error"
        })

class DynamoDBAdapter(DatabaseAdapter):
    def __init__(self):
        self.dynamodb = None
    
    async def connect(self, connection_string: str):
        import boto3
        self.dynamodb = boto3.resource('dynamodb')
    
    async def execute_migration(self, operation: MigrationOperation):
        table = self.dynamodb.Table(operation.collection_or_table)
        
        if operation.operation_type == 'add_gsi':
            await self._add_global_secondary_index(table, operation)
        elif operation.operation_type == 'migrate_data':
            await self._migrate_table_data(table, operation)
        elif operation.operation_type == 'update_capacity':
            await self._update_capacity(table, operation)
    
    async def _add_global_secondary_index(self, table, operation):
        """Add Global Secondary Index"""
        gsi_spec = operation.data['gsi_specification']
        
        table.update(
            GlobalSecondaryIndexUpdates=[
                {
                    'Create': gsi_spec
                }
            ]
        )
    
    async def _migrate_table_data(self, table, operation):
        """Migrate data between DynamoDB tables"""
        scan_kwargs = {
            'ProjectionExpression': operation.data.get('projection'),
            'FilterExpression': operation.conditions
        }
        
        target_table = self.dynamodb.Table(operation.data['target_table'])
        
        # Scan source table and write to target
        while True:
            response = table.scan(**scan_kwargs)
            
            # Transform and write items
            with target_table.batch_writer() as batch:
                for item in response['Items']:
                    transformed_item = self._transform_item(item, operation.data['transformation'])
                    batch.put_item(Item=transformed_item)
            
            if 'LastEvaluatedKey' not in response:
                break
            scan_kwargs['ExclusiveStartKey'] = response['LastEvaluatedKey']

class CassandraAdapter(DatabaseAdapter):
    def __init__(self):
        self.session = None
    
    async def connect(self, connection_string: str):
        from cassandra.cluster import Cluster
        from cassandra.auth import PlainTextAuthProvider
        
        # Parse connection string for auth
        cluster = Cluster(['127.0.0.1'])
        self.session = cluster.connect()
    
    async def execute_migration(self, operation: MigrationOperation):
        if operation.operation_type == 'add_column':
            await self._add_column(operation)
        elif operation.operation_type == 'create_materialized_view':
            await self._create_materialized_view(operation)
        elif operation.operation_type == 'migrate_data':
            await self._migrate_data(operation)
    
    async def _add_column(self, operation):
        """Add column to Cassandra table"""
        table = operation.collection_or_table
        column_name = operation.data['column_name']
        column_type = operation.data['column_type']
        
        cql = f"ALTER TABLE {table} ADD {column_name} {column_type}"
        self.session.execute(cql)
    
    async def _create_materialized_view(self, operation):
        """Create materialized view for denormalization"""
        view_spec = operation.data['view_specification']
        self.session.execute(view_spec)

class CrossPlatformMigrator:
    def __init__(self):
        self.adapters = {
            'postgresql': PostgreSQLAdapter(),
            'mysql': MySQLAdapter(),
            'mongodb': MongoDBAdapter(),
            'dynamodb': DynamoDBAdapter(),
            'cassandra': CassandraAdapter(),
            'redis': RedisAdapter(),
            'elasticsearch': ElasticsearchAdapter()
        }
    
    async def migrate_between_platforms(self, source_config, target_config, migration_spec):
        """Migrate data between different database platforms"""
        source_adapter = self.adapters[source_config['type']]
        target_adapter = self.adapters[target_config['type']]
        
        await source_adapter.connect(source_config['connection_string'])
        await target_adapter.connect(target_config['connection_string'])
        
        # Execute migration plan
        for step in migration_spec['steps']:
            if step['type'] == 'extract':
                data = await self._extract_data(source_adapter, step)
            elif step['type'] == 'transform':
                data = await self._transform_data(data, step)
            elif step['type'] == 'load':
                await self._load_data(target_adapter, data, step)
    
    async def _extract_data(self, adapter, step):
        """Extract data from source database"""
        extraction_op = MigrationOperation(
            operation_type='extract',
            collection_or_table=step['source_table'],
            data=step.get('extraction_params', {}),
            conditions=step.get('conditions'),
            batch_size=step.get('batch_size', 1000)
        )
        
        return await adapter.execute_migration(extraction_op)
    
    async def _transform_data(self, data, step):
        """Transform data between formats"""
        transformation_rules = step['transformation_rules']
        
        transformed_data = []
        for record in data:
            transformed_record = {}
            
            for target_field, source_mapping in transformation_rules.items():
                if isinstance(source_mapping, str):
                    # Simple field mapping
                    transformed_record[target_field] = record.get(source_mapping)
                elif isinstance(source_mapping, dict):
                    # Complex transformation
                    if source_mapping['type'] == 'function':
                        func = source_mapping['function']
                        args = [record.get(arg) for arg in source_mapping['args']]
                        transformed_record[target_field] = func(*args)
                    elif source_mapping['type'] == 'concatenate':
                        fields = source_mapping['fields']
                        separator = source_mapping.get('separator', ' ')
                        values = [str(record.get(field, '')) for field in fields]
                        transformed_record[target_field] = separator.join(values)
            
            transformed_data.append(transformed_record)
        
        return transformed_data
    
    async def _load_data(self, adapter, data, step):
        """Load data into target database"""
        load_op = MigrationOperation(
            operation_type='load',
            collection_or_table=step['target_table'],
            data={'records': data},
            batch_size=step.get('batch_size', 1000)
        )
        
        return await adapter.execute_migration(load_op)

# Example usage
async def migrate_sql_to_nosql():
    """Example: Migrate from PostgreSQL to MongoDB"""
    migrator = CrossPlatformMigrator()
    
    source_config = {
        'type': 'postgresql',
        'connection_string': 'postgresql://user:pass@localhost/db'
    }
    
    target_config = {
        'type': 'mongodb',
        'connection_string': 'mongodb://localhost:27017/db'
    }
    
    migration_spec = {
        'steps': [
            {
                'type': 'extract',
                'source_table': 'users',
                'conditions': {'active': True},
                'batch_size': 5000
            },
            {
                'type': 'transform',
                'transformation_rules': {
                    '_id': 'id',
                    'full_name': {
                        'type': 'concatenate',
                        'fields': ['first_name', 'last_name'],
                        'separator': ' '
                    },
                    'metadata': {
                        'type': 'function',
                        'function': lambda created, updated: {
                            'created_at': created,
                            'updated_at': updated
                        },
                        'args': ['created_at', 'updated_at']
                    }
                }
            },
            {
                'type': 'load',
                'target_table': 'users',
                'batch_size': 1000
            }
        ]
    }
    
    await migrator.migrate_between_platforms(source_config, target_config, migration_spec)
```

### 8. Modern Migration Tools and Change Data Capture

Integrate with enterprise migration tools and real-time sync:

**Atlas Schema Migrations (MongoDB)**
```javascript
// atlas-migration.js
const { MongoClient } = require('mongodb');

class AtlasMigration {
    constructor(connectionString) {
        this.client = new MongoClient(connectionString);
        this.migrations = new Map();
    }
    
    register(version, migration) {
        this.migrations.set(version, migration);
    }
    
    async migrate() {
        await this.client.connect();
        const db = this.client.db();
        
        // Get current version
        const versionsCollection = db.collection('schema_versions');
        const currentVersion = await versionsCollection
            .findOne({}, { sort: { version: -1 } });
        
        const startVersion = currentVersion?.version || 0;
        
        // Run pending migrations
        for (const [version, migration] of this.migrations) {
            if (version > startVersion) {
                console.log(`Running migration ${version}`);
                
                const session = this.client.startSession();
                
                try {
                    await session.withTransaction(async () => {
                        await migration.up(db, session);
                        await versionsCollection.insertOne({
                            version,
                            applied_at: new Date(),
                            checksum: migration.checksum
                        });
                    });
                } catch (error) {
                    console.error(`Migration ${version} failed:`, error);
                    if (migration.down) {
                        await migration.down(db, session);
                    }
                    throw error;
                } finally {
                    await session.endSession();
                }
            }
        }
    }
}

// Example MongoDB schema migration
const migration_001 = {
    checksum: 'sha256:abc123...',
    
    async up(db, session) {
        // Add new field to existing documents
        await db.collection('users').updateMany(
            { email_verified: { $exists: false } },
            { 
                $set: { 
                    email_verified: false,
                    verification_token: null,
                    verification_expires: null
                }
            },
            { session }
        );
        
        // Create new index
        await db.collection('users').createIndex(
            { email_verified: 1, verification_expires: 1 },
            { session }
        );
        
        // Add schema validation
        await db.command({
            collMod: 'users',
            validator: {
                $jsonSchema: {
                    bsonType: 'object',
                    required: ['email', 'email_verified'],
                    properties: {
                        email: { bsonType: 'string' },
                        email_verified: { bsonType: 'bool' },
                        verification_token: { 
                            bsonType: ['string', 'null'] 
                        }
                    }
                }
            }
        }, { session });
    },
    
    async down(db, session) {
        // Remove schema validation
        await db.command({
            collMod: 'users',
            validator: {}
        }, { session });
        
        // Drop index
        await db.collection('users').dropIndex(
            { email_verified: 1, verification_expires: 1 },
            { session }
        );
        
        // Remove fields
        await db.collection('users').updateMany(
            {},
            { 
                $unset: {
                    email_verified: '',
                    verification_token: '',
                    verification_expires: ''
                }
            },
            { session }
        );
    }
};
```

**Change Data Capture (CDC) for Real-time Sync**
```python
# cdc-migration.py
import asyncio
from kafka import KafkaConsumer, KafkaProducer
from confluent_kafka.schema_registry import SchemaRegistryClient
from confluent_kafka.schema_registry.avro import AvroSerializer
import json

class CDCMigrationManager:
    def __init__(self, config):
        self.config = config
        self.consumer = None
        self.producer = None
        self.schema_registry = None
        self.active_migrations = {}
    
    async def setup_cdc_pipeline(self):
        """Setup Change Data Capture pipeline"""
        # Kafka consumer for CDC events
        self.consumer = KafkaConsumer(
            'database.changes',
            bootstrap_servers=self.config['kafka_brokers'],
            auto_offset_reset='earliest',
            enable_auto_commit=True,
            group_id='migration-consumer',
            value_deserializer=lambda m: json.loads(m.decode('utf-8'))
        )
        
        # Kafka producer for processed events
        self.producer = KafkaProducer(
            bootstrap_servers=self.config['kafka_brokers'],
            value_serializer=lambda v: json.dumps(v).encode('utf-8')
        )
        
        # Schema registry for data validation
        self.schema_registry = SchemaRegistryClient({
            'url': self.config['schema_registry_url']
        })
    
    async def process_cdc_events(self):
        """Process CDC events and apply to target databases"""
        for message in self.consumer:
            event = message.value
            
            # Parse CDC event
            operation = event['operation']  # INSERT, UPDATE, DELETE
            table = event['table']
            data = event['data']
            
            # Check if this table has active migration
            if table in self.active_migrations:
                migration_config = self.active_migrations[table]
                await self.apply_migration_transformation(event, migration_config)
            else:
                # Standard replication
                await self.replicate_change(event)
    
    async def apply_migration_transformation(self, event, migration_config):
        """Apply data transformation during migration"""
        transformation_rules = migration_config['transformation_rules']
        target_tables = migration_config['target_tables']
        
        # Transform data according to migration rules
        transformed_data = {}
        for target_field, rule in transformation_rules.items():
            if isinstance(rule, str):
                # Simple field mapping
                transformed_data[target_field] = event['data'].get(rule)
            elif isinstance(rule, dict):
                # Complex transformation
                if rule['type'] == 'function':
                    func_name = rule['function']
                    func = getattr(self, f'transform_{func_name}')
                    args = [event['data'].get(arg) for arg in rule['args']]
                    transformed_data[target_field] = func(*args)
        
        # Apply to target tables
        for target_table in target_tables:
            await self.apply_to_target(target_table, event['operation'], transformed_data)
    
    async def setup_debezium_connector(self, source_db_config):
        """Configure Debezium for CDC"""
        connector_config = {
            "name": f"migration-connector-{source_db_config['name']}",
            "config": {
                "connector.class": "io.debezium.connector.postgresql.PostgresConnector",
                "database.hostname": source_db_config['host'],
                "database.port": source_db_config['port'],
                "database.user": source_db_config['user'],
                "database.password": source_db_config['password'],
                "database.dbname": source_db_config['database'],
                "database.server.name": source_db_config['name'],
                "table.include.list": ",".join(source_db_config['tables']),
                "plugin.name": "pgoutput",
                "slot.name": f"migration_slot_{source_db_config['name']}",
                "publication.name": f"migration_pub_{source_db_config['name']}",
                "transforms": "route",
                "transforms.route.type": "org.apache.kafka.connect.transforms.RegexRouter",
                "transforms.route.regex": "([^.]+)\.([^.]+)\.([^.]+)",
                "transforms.route.replacement": "database.changes"
            }
        }
        
        # Submit connector to Kafka Connect
        import requests
        response = requests.post(
            f"{self.config['kafka_connect_url']}/connectors",
            json=connector_config,
            headers={'Content-Type': 'application/json'}
        )
        
        if response.status_code != 201:
            raise Exception(f"Failed to create connector: {response.text}")
```

**Advanced Monitoring and Observability**
```python
class EnterpriseeMigrationMonitor:
    def __init__(self, config):
        self.config = config
        self.metrics_client = self.setup_metrics_client()
        self.alerting_client = self.setup_alerting_client()
        self.migration_state = {
            'current_migrations': {},
            'completed_migrations': {},
            'failed_migrations': {}
        }
    
    def setup_metrics_client(self):
        """Setup Prometheus/Datadog metrics client"""
        from prometheus_client import Counter, Gauge, Histogram, CollectorRegistry
        
        registry = CollectorRegistry()
        
        self.metrics = {
            'migration_duration': Histogram(
                'migration_duration_seconds',
                'Time spent on migration',
                ['migration_id', 'source_db', 'target_db'],
                registry=registry
            ),
            'rows_migrated': Counter(
                'migration_rows_total',
                'Total rows migrated',
                ['migration_id', 'table_name'],
                registry=registry
            ),
            'migration_errors': Counter(
                'migration_errors_total',
                'Total migration errors',
                ['migration_id', 'error_type'],
                registry=registry
            ),
            'active_migrations': Gauge(
                'active_migrations_count',
                'Number of active migrations',
                registry=registry
            ),
            'data_lag': Gauge(
                'migration_data_lag_seconds',
                'Data lag between source and target',
                ['migration_id'],
                registry=registry
            )
        }
        
        return registry
    
    async def track_migration_progress(self, migration_id):
        """Real-time migration progress tracking"""
        migration = self.migration_state['current_migrations'][migration_id]
        
        while migration['status'] == 'running':
            # Calculate progress metrics
            progress_stats = await self.calculate_progress_stats(migration)
            
            # Update Prometheus metrics
            self.metrics['rows_migrated'].labels(
                migration_id=migration_id,
                table_name=migration['table']
            ).inc(progress_stats['rows_processed_delta'])
            
            self.metrics['data_lag'].labels(
                migration_id=migration_id
            ).set(progress_stats['lag_seconds'])
            
            # Check for anomalies
            await self.detect_migration_anomalies(migration_id, progress_stats)
            
            # Generate alerts if needed
            await self.check_alert_conditions(migration_id, progress_stats)
            
            await asyncio.sleep(30)  # Check every 30 seconds
    
    async def detect_migration_anomalies(self, migration_id, stats):
        """AI-powered anomaly detection for migrations"""
        # Simple statistical anomaly detection
        if stats['rows_per_second'] < stats['expected_rows_per_second'] * 0.5:
            await self.trigger_alert(
                'migration_slow',
                f"Migration {migration_id} is running slower than expected",
                {'stats': stats}
            )
        
        if stats['error_rate'] > 0.01:  # 1% error rate threshold
            await self.trigger_alert(
                'migration_high_error_rate',
                f"Migration {migration_id} has high error rate: {stats['error_rate']}",
                {'stats': stats}
            )
        
        if stats['memory_usage'] > 0.8:  # 80% memory usage
            await self.trigger_alert(
                'migration_high_memory',
                f"Migration {migration_id} is using high memory: {stats['memory_usage']}",
                {'stats': stats}
            )
    
    async def setup_migration_dashboard(self):
        """Setup Grafana dashboard for migration monitoring"""
        dashboard_config = {
            "dashboard": {
                "title": "Database Migration Monitoring",
                "panels": [
                    {
                        "title": "Migration Progress",
                        "type": "graph",
                        "targets": [
                            {
                                "expr": "rate(migration_rows_total[5m])",
                                "legendFormat": "{{migration_id}} - {{table_name}}"
                            }
                        ]
                    },
                    {
                        "title": "Data Lag",
                        "type": "singlestat",
                        "targets": [
                            {
                                "expr": "migration_data_lag_seconds",
                                "legendFormat": "Lag (seconds)"
                            }
                        ]
                    },
                    {
                        "title": "Error Rate",
                        "type": "graph",
                        "targets": [
                            {
                                "expr": "rate(migration_errors_total[5m])",
                                "legendFormat": "{{error_type}}"
                            }
                        ]
                    },
                    {
                        "title": "Migration Duration",
                        "type": "heatmap",
                        "targets": [
                            {
                                "expr": "migration_duration_seconds",
                                "legendFormat": "Duration"
                            }
                        ]
                    }
                ]
            }
        }
        
        # Submit dashboard to Grafana API
        import requests
        response = requests.post(
            f"{self.config['grafana_url']}/api/dashboards/db",
            json=dashboard_config,
            headers={
                'Authorization': f"Bearer {self.config['grafana_token']}",
                'Content-Type': 'application/json'
            }
        )
        
        return response.json()
```

### 9. Event Sourcing and CQRS Migrations

Handle event-driven architecture migrations:

**Event Store Migration Strategy**
```python
class EventStoreMigrator:
    def __init__(self, event_store_config):
        self.event_store = EventStore(event_store_config)
        self.event_transformers = {}
        self.aggregate_rebuilders = {}
    
    def register_event_transformer(self, event_type, transformer):
        """Register transformation for specific event type"""
        self.event_transformers[event_type] = transformer
    
    def register_aggregate_rebuilder(self, aggregate_type, rebuilder):
        """Register rebuilder for aggregate snapshots"""
        self.aggregate_rebuilders[aggregate_type] = rebuilder
    
    async def migrate_events(self, from_version, to_version):
        """Migrate events from one schema version to another"""
        # Get all events that need migration
        events_cursor = self.event_store.get_events_by_version_range(
            from_version, to_version
        )
        
        migrated_events = []
        
        async for event in events_cursor:
            if event.event_type in self.event_transformers:
                transformer = self.event_transformers[event.event_type]
                migrated_event = await transformer.transform(event)
                migrated_events.append(migrated_event)
            else:
                # No transformation needed
                migrated_events.append(event)
        
        # Write migrated events to new stream
        await self.event_store.append_events(
            f"migration-{to_version}",
            migrated_events
        )
        
        # Rebuild aggregates with new events
        await self.rebuild_aggregates(migrated_events)
    
    async def rebuild_aggregates(self, events):
        """Rebuild aggregate snapshots from migrated events"""
        aggregates_to_rebuild = set()
        
        for event in events:
            aggregates_to_rebuild.add(event.aggregate_id)
        
        for aggregate_id in aggregates_to_rebuild:
            aggregate_type = self.get_aggregate_type(aggregate_id)
            
            if aggregate_type in self.aggregate_rebuilders:
                rebuilder = self.aggregate_rebuilders[aggregate_type]
                await rebuilder.rebuild(aggregate_id)

# Example event transformation
class UserEventTransformer:
    async def transform(self, event):
        """Transform UserCreated event from v1 to v2"""
        if event.event_type == 'UserCreated' and event.version == 1:
            # v1 had separate first_name and last_name
            # v2 uses full_name
            old_data = event.data
            new_data = {
                'user_id': old_data['user_id'],
                'full_name': f"{old_data['first_name']} {old_data['last_name']}",
                'email': old_data['email'],
                'created_at': old_data['created_at']
            }
            
            return Event(
                event_id=event.event_id,
                event_type='UserCreated',
                aggregate_id=event.aggregate_id,
                version=2,
                data=new_data,
                metadata=event.metadata
            )
        
        return event
```

### 10. Cloud Database Migration Automation

Automate cloud database migrations with infrastructure as code:

**AWS Database Migration with CDK**
```typescript
// aws-db-migration.ts
import * as cdk from 'aws-cdk-lib';
import * as dms from 'aws-cdk-lib/aws-dms';
import * as rds from 'aws-cdk-lib/aws-rds';
import * as ec2 from 'aws-cdk-lib/aws-ec2';
import * as lambda from 'aws-cdk-lib/aws-lambda';
import * as stepfunctions from 'aws-cdk-lib/aws-stepfunctions';
import * as sfnTasks from 'aws-cdk-lib/aws-stepfunctions-tasks';

export class DatabaseMigrationStack extends cdk.Stack {
    constructor(scope: cdk.App, id: string, props?: cdk.StackProps) {
        super(scope, id, props);
        
        // Create VPC for migration
        const vpc = new ec2.Vpc(this, 'MigrationVPC', {
            maxAzs: 2,
            subnetConfiguration: [
                {
                    cidrMask: 24,
                    name: 'private',
                    subnetType: ec2.SubnetType.PRIVATE_WITH_EGRESS
                },
                {
                    cidrMask: 24,
                    name: 'public',
                    subnetType: ec2.SubnetType.PUBLIC
                }
            ]
        });
        
        // DMS Replication Instance
        const replicationInstance = new dms.CfnReplicationInstance(this, 'ReplicationInstance', {
            replicationInstanceClass: 'dms.t3.medium',
            replicationInstanceIdentifier: 'migration-instance',
            allocatedStorage: 100,
            autoMinorVersionUpgrade: true,
            multiAz: false,
            publiclyAccessible: false,
            replicationSubnetGroupIdentifier: this.createSubnetGroup(vpc).ref
        });
        
        // Source and Target Endpoints
        const sourceEndpoint = new dms.CfnEndpoint(this, 'SourceEndpoint', {
            endpointType: 'source',
            engineName: 'postgres',
            serverName: 'source-db.example.com',
            port: 5432,
            databaseName: 'source_db',
            username: 'migration_user',
            password: 'migration_password'
        });
        
        const targetEndpoint = new dms.CfnEndpoint(this, 'TargetEndpoint', {
            endpointType: 'target',
            engineName: 'postgres',
            serverName: 'target-db.example.com',
            port: 5432,
            databaseName: 'target_db',
            username: 'migration_user',
            password: 'migration_password'
        });
        
        // Migration Task
        const migrationTask = new dms.CfnReplicationTask(this, 'MigrationTask', {
            replicationTaskIdentifier: 'full-load-and-cdc',
            sourceEndpointArn: sourceEndpoint.ref,
            targetEndpointArn: targetEndpoint.ref,
            replicationInstanceArn: replicationInstance.ref,
            migrationType: 'full-load-and-cdc',
            tableMappings: JSON.stringify({
                "rules": [
                    {
                        "rule-type": "selection",
                        "rule-id": "1",
                        "rule-name": "1",
                        "object-locator": {
                            "schema-name": "public",
                            "table-name": "%"
                        },
                        "rule-action": "include"
                    }
                ]
            }),
            replicationTaskSettings: JSON.stringify({
                "TargetMetadata": {
                    "TargetSchema": "",
                    "SupportLobs": true,
                    "FullLobMode": false,
                    "LobChunkSize": 0,
                    "LimitedSizeLobMode": true,
                    "LobMaxSize": 32,
                    "LoadMaxFileSize": 0,
                    "ParallelLoadThreads": 0,
                    "ParallelLoadBufferSize": 0,
                    "BatchApplyEnabled": false,
                    "TaskRecoveryTableEnabled": false
                },
                "FullLoadSettings": {
                    "TargetTablePrepMode": "DROP_AND_CREATE",
                    "CreatePkAfterFullLoad": false,
                    "StopTaskCachedChangesApplied": false,
                    "StopTaskCachedChangesNotApplied": false,
                    "MaxFullLoadSubTasks": 8,
                    "TransactionConsistencyTimeout": 600,
                    "CommitRate": 10000
                },
                "Logging": {
                    "EnableLogging": true,
                    "LogComponents": [
                        {
                            "Id": "SOURCE_UNLOAD",
                            "Severity": "LOGGER_SEVERITY_DEFAULT"
                        },
                        {
                            "Id": "TARGET_LOAD",
                            "Severity": "LOGGER_SEVERITY_DEFAULT"
                        }
                    ]
                }
            })
        });
        
        // Migration orchestration with Step Functions
        this.createMigrationOrchestration(migrationTask);
    }
    
    private createSubnetGroup(vpc: ec2.Vpc): dms.CfnReplicationSubnetGroup {
        return new dms.CfnReplicationSubnetGroup(this, 'ReplicationSubnetGroup', {
            replicationSubnetGroupDescription: 'Subnet group for DMS',
            replicationSubnetGroupIdentifier: 'migration-subnet-group',
            subnetIds: vpc.privateSubnets.map(subnet => subnet.subnetId)
        });
    }
    
    private createMigrationOrchestration(migrationTask: dms.CfnReplicationTask): void {
        // Lambda functions for migration steps
        const startMigrationFunction = new lambda.Function(this, 'StartMigration', {
            runtime: lambda.Runtime.PYTHON_3_9,
            handler: 'index.handler',
            code: lambda.Code.fromInline(`
import boto3
import json

def handler(event, context):
    dms = boto3.client('dms')
    task_arn = event['task_arn']
    
    response = dms.start_replication_task(
        ReplicationTaskArn=task_arn,
        StartReplicationTaskType='start-replication'
    )
    
    return {
        'statusCode': 200,
        'task_arn': task_arn,
        'task_status': response['ReplicationTask']['Status']
    }
            `)
        });
        
        const checkMigrationStatusFunction = new lambda.Function(this, 'CheckMigrationStatus', {
            runtime: lambda.Runtime.PYTHON_3_9,
            handler: 'index.handler',
            code: lambda.Code.fromInline(`
import boto3
import json

def handler(event, context):
    dms = boto3.client('dms')
    task_arn = event['task_arn']
    
    response = dms.describe_replication_tasks(
        Filters=[
            {
                'Name': 'replication-task-arn',
                'Values': [task_arn]
            }
        ]
    )
    
    task = response['ReplicationTasks'][0]
    status = task['Status']
    
    return {
        'task_arn': task_arn,
        'task_status': status,
        'is_complete': status in ['stopped', 'failed', 'ready']
    }
            `)
        });
        
        // Step Function definition
        const startMigrationTask = new sfnTasks.LambdaInvoke(this, 'StartMigrationTask', {
            lambdaFunction: startMigrationFunction,
            inputPath: '$',
            outputPath: '$'
        });
        
        const checkStatusTask = new sfnTasks.LambdaInvoke(this, 'CheckMigrationStatusTask', {
            lambdaFunction: checkMigrationStatusFunction,
            inputPath: '$',
            outputPath: '$'
        });
        
        const waitTask = new stepfunctions.Wait(this, 'WaitForMigration', {
            time: stepfunctions.WaitTime.duration(cdk.Duration.minutes(5))
        });
        
        const migrationComplete = new stepfunctions.Succeed(this, 'MigrationComplete');
        const migrationFailed = new stepfunctions.Fail(this, 'MigrationFailed');
        
        // Define state machine
        const definition = startMigrationTask
            .next(waitTask)
            .next(checkStatusTask)
            .next(new stepfunctions.Choice(this, 'IsMigrationComplete?')
                .when(stepfunctions.Condition.booleanEquals('$.is_complete', true),
                      new stepfunctions.Choice(this, 'MigrationSuccessful?')
                          .when(stepfunctions.Condition.stringEquals('$.task_status', 'stopped'), migrationComplete)
                          .otherwise(migrationFailed))
                .otherwise(waitTask));
        
        new stepfunctions.StateMachine(this, 'MigrationStateMachine', {
            definition: definition,
            timeout: cdk.Duration.hours(24)
        });
    }
}
```

## Output Format

1. **Comprehensive Migration Strategy**: Multi-database platform support with NoSQL integration
2. **Cross-Platform Migration Tools**: SQL to NoSQL, NoSQL to SQL, and hybrid migrations
3. **Modern Tooling Integration**: Atlas, Debezium, Flyway, Prisma, and cloud-native solutions
4. **Change Data Capture Pipeline**: Real-time synchronization with Kafka and schema registry
5. **Event Sourcing Migrations**: Event store transformations and aggregate rebuilding
6. **Cloud Infrastructure Automation**: AWS DMS, GCP Database Migration Service, Azure DMS
7. **Enterprise Monitoring Suite**: Prometheus metrics, Grafana dashboards, and anomaly detection
8. **Advanced Validation Framework**: Multi-database integrity checks and performance benchmarks
9. **Automated Rollback Procedures**: Platform-specific recovery strategies
10. **Performance Optimization**: Batch processing, parallel execution, and resource management

Focus on zero-downtime migrations with comprehensive validation, automated rollbacks, and enterprise-grade monitoring across all supported database platforms.

## Cross-Command Integration

This command integrates seamlessly with other development workflow commands to create a comprehensive database-first development pipeline:

### Integration with API Development (`/api-scaffold`)
```python
# integrated-db-api-config.py
class IntegratedDatabaseApiConfig:
    def __init__(self):
        self.api_config = self.load_api_config()        # From /api-scaffold
        self.db_config = self.load_db_config()          # From /db-migrate
        self.migration_config = self.load_migration_config()
    
    def generate_api_aware_migrations(self):
        """Generate migrations that consider API endpoints and schemas"""
        return {
            # API-aware migration strategy
            'api_migration_strategy': f"""
-- Migration with API endpoint consideration
-- Migration: {datetime.now().strftime('%Y%m%d_%H%M%S')}_api_aware_schema_update.sql

-- Check API dependency before migration
DO $$
BEGIN
    -- Verify API endpoints that depend on this schema
    IF EXISTS (
        SELECT 1 FROM api_endpoints 
        WHERE schema_dependencies @> '["users", "profiles"]'
        AND is_active = true
    ) THEN
        RAISE NOTICE 'Found active API endpoints depending on this schema';
        
        -- Create migration strategy with API versioning
        CREATE TABLE IF NOT EXISTS api_migration_log (
            id SERIAL PRIMARY KEY,
            migration_name VARCHAR(255) NOT NULL,
            api_version VARCHAR(50) NOT NULL,
            schema_changes JSONB,
            rollback_script TEXT,
            created_at TIMESTAMP DEFAULT NOW()
        );
        
        -- Log this migration for API tracking
        INSERT INTO api_migration_log (
            migration_name, 
            api_version, 
            schema_changes
        ) VALUES (
            'api_aware_schema_update',
            '{self.api_config.get("version", "v1")}',
            '{{"tables": ["users", "profiles"], "type": "schema_update"}}'::jsonb
        );
    END IF;
END $$;

-- Backward-compatible schema changes
ALTER TABLE users ADD COLUMN IF NOT EXISTS new_field VARCHAR(255);

-- Create view for API backward compatibility
CREATE OR REPLACE VIEW users_api_v1 AS 
SELECT 
    id,
    username,
    email,
    -- Maintain API compatibility
    COALESCE(new_field, 'default_value') as new_field,
    created_at,
    updated_at
FROM users;

-- Grant API service access
GRANT SELECT ON users_api_v1 TO {self.api_config.get("db_user", "api_service")};

COMMIT;
            """,
            
            # Database connection pool optimization for API
            'connection_pool_config': {
                'fastapi': f"""
# FastAPI with optimized database connections
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker
from sqlalchemy.pool import QueuePool

class DatabaseConfig:
    def __init__(self):
        self.database_url = "{self.db_config.get('url', 'postgresql://localhost/app')}"
        self.api_config = {self.api_config}
        
    def create_engine(self):
        return create_engine(
            self.database_url,
            poolclass=QueuePool,
            pool_size={self.api_config.get('db_pool_size', 20)},
            max_overflow={self.api_config.get('db_max_overflow', 0)},
            pool_pre_ping=True,
            pool_recycle=3600,
            echo={str(self.api_config.get('debug', False)).lower()}
        )
    
    def get_session_maker(self):
        engine = self.create_engine()
        return sessionmaker(autocommit=False, autoflush=False, bind=engine)

# Migration-aware API dependencies
async def get_db_with_migration_check():
    # Check if migrations are running
    async with get_db() as session:
        result = await session.execute(
            text("SELECT COUNT(*) FROM schema_migrations WHERE is_running = true")
        )
        running_migrations = result.scalar()
        
        if running_migrations > 0:
            raise HTTPException(
                status_code=503,
                detail="Database migrations in progress. API temporarily unavailable."
            )
        
        yield session
                """,
                
                'express': f"""
// Express.js with database migration awareness
const {{ Pool }} = require('pg');
const express = require('express');
const app = express();

class DatabaseManager {{
    constructor() {{
        this.pool = new Pool({{
            connectionString: '{self.db_config.get('url', 'postgresql://localhost/app')}',
            max: {self.api_config.get('db_pool_size', 20)},
            idleTimeoutMillis: 30000,
            connectionTimeoutMillis: 2000,
        }});
        
        this.migrationStatus = new Map();
    }}
    
    async checkMigrationStatus() {{
        try {{
            const client = await this.pool.connect();
            const result = await client.query(
                'SELECT COUNT(*) as count FROM schema_migrations WHERE is_running = true'
            );
            client.release();
            
            return result.rows[0].count === '0';
        }} catch (error) {{
            console.error('Failed to check migration status:', error);
            return false;
        }}
    }}
    
    // Middleware to check migration status
    migrationStatusMiddleware() {{
        return async (req, res, next) => {{
            const isSafe = await this.checkMigrationStatus();
            
            if (!isSafe) {{
                return res.status(503).json({{
                    error: 'Database migrations in progress',
                    message: 'API temporarily unavailable during database updates'
                }});
            }}
            
            next();
        }};
    }}
}}

const dbManager = new DatabaseManager();
app.use('/api', dbManager.migrationStatusMiddleware());
                """
            }
        }
    
    def generate_api_schema_sync(self):
        """Generate API schema synchronization with database"""
        return f"""
# API Schema Synchronization
import asyncio
import aiohttp
from sqlalchemy import text

class ApiSchemaSync:
    def __init__(self, api_base_url="{self.api_config.get('base_url', 'http://localhost:8000')}"):
        self.api_base_url = api_base_url
        self.db_config = {self.db_config}
    
    async def notify_api_of_schema_change(self, migration_name, schema_changes):
        '''Notify API service of database schema changes'''
        async with aiohttp.ClientSession() as session:
            payload = {{
                'migration_name': migration_name,
                'schema_changes': schema_changes,
                'timestamp': datetime.now().isoformat()
            }}
            
            try:
                async with session.post(
                    f"{{self.api_base_url}}/internal/schema-update",
                    json=payload,
                    timeout=30
                ) as response:
                    if response.status == 200:
                        print(f"API notified of schema changes: {{migration_name}}")
                    else:
                        print(f"Failed to notify API: {{response.status}}")
            except Exception as e:
                print(f"Error notifying API: {{e}}")
    
    async def validate_api_compatibility(self, proposed_changes):
        '''Validate that proposed schema changes won't break API'''
        async with aiohttp.ClientSession() as session:
            try:
                async with session.post(
                    f"{{self.api_base_url}}/internal/validate-schema",
                    json={{'proposed_changes': proposed_changes}},
                    timeout=30
                ) as response:
                    result = await response.json()
                    return result.get('compatible', False), result.get('issues', [])
            except Exception as e:
                print(f"Error validating API compatibility: {{e}}")
                return False, [f"Validation service unavailable: {{e}}"]
        """
```

### Complete Workflow Integration
```python
# complete-database-workflow.py
class CompleteDatabaseWorkflow:
    def __init__(self):
        self.configs = {
            'api': self.load_api_config(),           # From /api-scaffold
            'testing': self.load_test_config(),      # From /test-harness
            'security': self.load_security_config(), # From /security-scan
            'docker': self.load_docker_config(),     # From /docker-optimize
            'k8s': self.load_k8s_config(),          # From /k8s-manifest
            'frontend': self.load_frontend_config(), # From /frontend-optimize
            'database': self.load_db_config()        # From /db-migrate
        }
    
    async def execute_complete_workflow(self):
        console.log("üöÄ Starting complete database migration workflow...")
        
        # 1. Pre-migration Security Scan
        security_scan = await self.run_security_scan()
        console.log("‚úÖ Database security scan completed")
        
        # 2. API Compatibility Check
        api_compatibility = await self.check_api_compatibility()
        console.log("‚úÖ API compatibility verified")
        
        # 3. Container-based Migration Testing
        container_tests = await self.run_container_tests()
        console.log("‚úÖ Container-based migration tests passed")
        
        # 4. Production Migration with Monitoring
        migration_result = await self.run_production_migration()
        console.log("‚úÖ Production migration completed")
        
        # 5. Frontend Cache Invalidation
        cache_invalidation = await self.invalidate_frontend_caches()
        console.log("‚úÖ Frontend caches invalidated")
        
        # 6. Kubernetes Deployment Update
        k8s_deployment = await self.update_k8s_deployment()
        console.log("‚úÖ Kubernetes deployment updated")
        
        # 7. Post-migration Testing Pipeline
        post_migration_tests = await self.run_post_migration_tests()
        console.log("‚úÖ Post-migration tests completed")
        
        return {
            'status': 'success',
            'workflow_id': self.generate_workflow_id(),
            'components': {
                security_scan,
                api_compatibility,
                container_tests,
                migration_result,
                cache_invalidation,
                k8s_deployment,
                post_migration_tests
            },
            'migration_summary': {
                'zero_downtime': True,
                'rollback_plan': 'available',
                'performance_impact': 'minimal',
                'security_validated': True
            }
        }
```

This integrated database migration workflow ensures that database changes are coordinated across all layers of the application stack, from API compatibility to frontend cache invalidation, creating a comprehensive database-first development pipeline that maintains data integrity and system reliability.

Focus on enterprise-grade migrations with zero-downtime deployments, comprehensive monitoring, and platform-agnostic strategies for modern polyglot persistence architectures.
````

## File: tools/debug-trace.md

````markdown
# Debug and Trace Configuration

You are a debugging expert specializing in setting up comprehensive debugging environments, distributed tracing, and diagnostic tools. Configure debugging workflows, implement tracing solutions, and establish troubleshooting practices for development and production environments.

## Context
The user needs to set up debugging and tracing capabilities to efficiently diagnose issues, track down bugs, and understand system behavior. Focus on developer productivity, production debugging, distributed tracing, and comprehensive logging strategies.

## Requirements
$ARGUMENTS

## Instructions

### 1. Development Environment Debugging

Set up comprehensive debugging environments:

**VS Code Debug Configuration**
```json
// .vscode/launch.json
{
    "version": "0.2.0",
    "configurations": [
        {
            "name": "Debug Node.js App",
            "type": "node",
            "request": "launch",
            "runtimeExecutable": "node",
            "runtimeArgs": ["--inspect-brk", "--enable-source-maps"],
            "program": "${workspaceFolder}/src/index.js",
            "env": {
                "NODE_ENV": "development",
                "DEBUG": "*",
                "NODE_OPTIONS": "--max-old-space-size=4096"
            },
            "sourceMaps": true,
            "resolveSourceMapLocations": [
                "${workspaceFolder}/**",
                "!**/node_modules/**"
            ],
            "skipFiles": [
                "<node_internals>/**",
                "node_modules/**"
            ],
            "console": "integratedTerminal",
            "outputCapture": "std"
        },
        {
            "name": "Debug TypeScript",
            "type": "node",
            "request": "launch",
            "program": "${workspaceFolder}/src/index.ts",
            "preLaunchTask": "tsc: build - tsconfig.json",
            "outFiles": ["${workspaceFolder}/dist/**/*.js"],
            "sourceMaps": true,
            "smartStep": true,
            "internalConsoleOptions": "openOnSessionStart"
        },
        {
            "name": "Debug Jest Tests",
            "type": "node",
            "request": "launch",
            "program": "${workspaceFolder}/node_modules/.bin/jest",
            "args": [
                "--runInBand",
                "--no-cache",
                "--watchAll=false",
                "--detectOpenHandles"
            ],
            "console": "integratedTerminal",
            "internalConsoleOptions": "neverOpen",
            "env": {
                "NODE_ENV": "test"
            }
        },
        {
            "name": "Attach to Process",
            "type": "node",
            "request": "attach",
            "processId": "${command:PickProcess}",
            "protocol": "inspector",
            "restart": true,
            "sourceMaps": true
        }
    ],
    "compounds": [
        {
            "name": "Full Stack Debug",
            "configurations": ["Debug Backend", "Debug Frontend"],
            "stopAll": true
        }
    ]
}
```

**Chrome DevTools Configuration**
```javascript
// debug-helpers.js
class DebugHelper {
    constructor() {
        this.setupDevTools();
        this.setupConsoleHelpers();
        this.setupPerformanceMarkers();
    }
    
    setupDevTools() {
        if (typeof window !== 'undefined') {
            // Add debug namespace
            window.DEBUG = window.DEBUG || {};
            
            // Store references to important objects
            window.DEBUG.store = () => window.__REDUX_STORE__;
            window.DEBUG.router = () => window.__ROUTER__;
            window.DEBUG.components = new Map();
            
            // Performance debugging
            window.DEBUG.measureRender = (componentName) => {
                performance.mark(`${componentName}-start`);
                return () => {
                    performance.mark(`${componentName}-end`);
                    performance.measure(
                        componentName,
                        `${componentName}-start`,
                        `${componentName}-end`
                    );
                };
            };
            
            // Memory debugging
            window.DEBUG.heapSnapshot = async () => {
                if ('memory' in performance) {
                    const snapshot = await performance.measureUserAgentSpecificMemory();
                    console.table(snapshot);
                    return snapshot;
                }
            };
        }
    }
    
    setupConsoleHelpers() {
        // Enhanced console logging
        const styles = {
            error: 'color: #ff0000; font-weight: bold;',
            warn: 'color: #ff9800; font-weight: bold;',
            info: 'color: #2196f3; font-weight: bold;',
            debug: 'color: #4caf50; font-weight: bold;',
            trace: 'color: #9c27b0; font-weight: bold;'
        };
        
        Object.entries(styles).forEach(([level, style]) => {
            const original = console[level];
            console[level] = function(...args) {
                if (process.env.NODE_ENV === 'development') {
                    const timestamp = new Date().toISOString();
                    original.call(console, `%c[${timestamp}] ${level.toUpperCase()}:`, style, ...args);
                }
            };
        });
    }
}

// React DevTools integration
if (process.env.NODE_ENV === 'development') {
    // Expose React internals
    window.__REACT_DEVTOOLS_GLOBAL_HOOK__ = {
        ...window.__REACT_DEVTOOLS_GLOBAL_HOOK__,
        onCommitFiberRoot: (id, root) => {
            // Custom commit logging
            console.debug('React commit:', root);
        }
    };
}
```

### 2. Remote Debugging Setup

Configure remote debugging capabilities:

**Remote Debug Server**
```javascript
// remote-debug-server.js
const inspector = require('inspector');
const WebSocket = require('ws');
const http = require('http');

class RemoteDebugServer {
    constructor(options = {}) {
        this.port = options.port || 9229;
        this.host = options.host || '0.0.0.0';
        this.wsPort = options.wsPort || 9230;
        this.sessions = new Map();
    }
    
    start() {
        // Open inspector
        inspector.open(this.port, this.host, true);
        
        // Create WebSocket server for remote connections
        this.wss = new WebSocket.Server({ port: this.wsPort });
        
        this.wss.on('connection', (ws) => {
            const sessionId = this.generateSessionId();
            this.sessions.set(sessionId, ws);
            
            ws.on('message', (message) => {
                this.handleDebugCommand(sessionId, message);
            });
            
            ws.on('close', () => {
                this.sessions.delete(sessionId);
            });
            
            // Send initial session info
            ws.send(JSON.stringify({
                type: 'session',
                sessionId,
                debugUrl: `chrome-devtools://devtools/bundled/inspector.html?ws=${this.host}:${this.port}`
            }));
        });
        
        console.log(`Remote debug server listening on ws://${this.host}:${this.wsPort}`);
    }
    
    handleDebugCommand(sessionId, message) {
        const command = JSON.parse(message);
        
        switch (command.type) {
            case 'evaluate':
                this.evaluateExpression(sessionId, command.expression);
                break;
            case 'setBreakpoint':
                this.setBreakpoint(command.file, command.line);
                break;
            case 'heapSnapshot':
                this.takeHeapSnapshot(sessionId);
                break;
            case 'profile':
                this.startProfiling(sessionId, command.duration);
                break;
        }
    }
    
    evaluateExpression(sessionId, expression) {
        const session = new inspector.Session();
        session.connect();
        
        session.post('Runtime.evaluate', {
            expression,
            generatePreview: true,
            includeCommandLineAPI: true
        }, (error, result) => {
            const ws = this.sessions.get(sessionId);
            if (ws) {
                ws.send(JSON.stringify({
                    type: 'evaluateResult',
                    result: result || error
                }));
            }
        });
        
        session.disconnect();
    }
}

// Docker remote debugging setup
FROM node:18
RUN apt-get update && apt-get install -y \
    chromium \
    gdb \
    strace \
    tcpdump \
    vim
    
EXPOSE 9229 9230
ENV NODE_OPTIONS="--inspect=0.0.0.0:9229"
CMD ["node", "--inspect-brk=0.0.0.0:9229", "index.js"]
```

### 3. Distributed Tracing

Implement comprehensive distributed tracing:

**OpenTelemetry Setup**
```javascript
// tracing.js
const { NodeSDK } = require('@opentelemetry/sdk-node');
const { getNodeAutoInstrumentations } = require('@opentelemetry/auto-instrumentations-node');
const { Resource } = require('@opentelemetry/resources');
const { SemanticResourceAttributes } = require('@opentelemetry/semantic-conventions');
const { JaegerExporter } = require('@opentelemetry/exporter-jaeger');
const { BatchSpanProcessor } = require('@opentelemetry/sdk-trace-base');

class TracingSystem {
    constructor(serviceName) {
        this.serviceName = serviceName;
        this.sdk = null;
    }
    
    initialize() {
        const jaegerExporter = new JaegerExporter({
            endpoint: process.env.JAEGER_ENDPOINT || 'http://localhost:14268/api/traces',
        });
        
        const resource = Resource.default().merge(
            new Resource({
                [SemanticResourceAttributes.SERVICE_NAME]: this.serviceName,
                [SemanticResourceAttributes.SERVICE_VERSION]: process.env.SERVICE_VERSION || '1.0.0',
                [SemanticResourceAttributes.DEPLOYMENT_ENVIRONMENT]: process.env.NODE_ENV || 'development',
            })
        );
        
        this.sdk = new NodeSDK({
            resource,
            spanProcessor: new BatchSpanProcessor(jaegerExporter),
            instrumentations: [
                getNodeAutoInstrumentations({
                    '@opentelemetry/instrumentation-fs': {
                        enabled: false, // Too noisy
                    },
                    '@opentelemetry/instrumentation-http': {
                        requestHook: (span, request) => {
                            span.setAttribute('http.request.body', JSON.stringify(request.body));
                        },
                        responseHook: (span, response) => {
                            span.setAttribute('http.response.size', response.length);
                        },
                    },
                    '@opentelemetry/instrumentation-express': {
                        requestHook: (span, req) => {
                            span.setAttribute('user.id', req.user?.id);
                            span.setAttribute('session.id', req.session?.id);
                        },
                    },
                }),
            ],
        });
        
        this.sdk.start();
        
        // Graceful shutdown
        process.on('SIGTERM', () => {
            this.sdk.shutdown()
                .then(() => console.log('Tracing terminated'))
                .catch((error) => console.error('Error terminating tracing', error))
                .finally(() => process.exit(0));
        });
    }
    
    // Custom span creation
    createSpan(name, fn, attributes = {}) {
        const tracer = trace.getTracer(this.serviceName);
        return tracer.startActiveSpan(name, async (span) => {
            try {
                // Add custom attributes
                Object.entries(attributes).forEach(([key, value]) => {
                    span.setAttribute(key, value);
                });
                
                // Execute function
                const result = await fn(span);
                
                span.setStatus({ code: SpanStatusCode.OK });
                return result;
            } catch (error) {
                span.recordException(error);
                span.setStatus({
                    code: SpanStatusCode.ERROR,
                    message: error.message,
                });
                throw error;
            } finally {
                span.end();
            }
        });
    }
}

// Distributed tracing middleware
class TracingMiddleware {
    constructor() {
        this.tracer = trace.getTracer('http-middleware');
    }
    
    express() {
        return (req, res, next) => {
            const span = this.tracer.startSpan(`${req.method} ${req.path}`, {
                kind: SpanKind.SERVER,
                attributes: {
                    'http.method': req.method,
                    'http.url': req.url,
                    'http.target': req.path,
                    'http.host': req.hostname,
                    'http.scheme': req.protocol,
                    'http.user_agent': req.get('user-agent'),
                    'http.request_content_length': req.get('content-length'),
                },
            });
            
            // Inject trace context into request
            req.span = span;
            req.traceId = span.spanContext().traceId;
            
            // Add trace ID to response headers
            res.setHeader('X-Trace-Id', req.traceId);
            
            // Override res.end to capture response data
            const originalEnd = res.end;
            res.end = function(...args) {
                span.setAttribute('http.status_code', res.statusCode);
                span.setAttribute('http.response_content_length', res.get('content-length'));
                
                if (res.statusCode >= 400) {
                    span.setStatus({
                        code: SpanStatusCode.ERROR,
                        message: `HTTP ${res.statusCode}`,
                    });
                }
                
                span.end();
                originalEnd.apply(res, args);
            };
            
            next();
        };
    }
}
```

### 4. Debug Logging Framework

Implement structured debug logging:

**Advanced Logger**
```javascript
// debug-logger.js
const winston = require('winston');
const { ElasticsearchTransport } = require('winston-elasticsearch');

class DebugLogger {
    constructor(options = {}) {
        this.service = options.service || 'app';
        this.level = process.env.LOG_LEVEL || 'debug';
        this.logger = this.createLogger();
    }
    
    createLogger() {
        const formats = [
            winston.format.timestamp(),
            winston.format.errors({ stack: true }),
            winston.format.splat(),
            winston.format.json(),
        ];
        
        if (process.env.NODE_ENV === 'development') {
            formats.push(winston.format.colorize());
            formats.push(winston.format.printf(this.devFormat));
        }
        
        const transports = [
            new winston.transports.Console({
                level: this.level,
                handleExceptions: true,
                handleRejections: true,
            }),
        ];
        
        // Add file transport for debugging
        if (process.env.DEBUG_LOG_FILE) {
            transports.push(
                new winston.transports.File({
                    filename: process.env.DEBUG_LOG_FILE,
                    level: 'debug',
                    maxsize: 10485760, // 10MB
                    maxFiles: 5,
                })
            );
        }
        
        // Add Elasticsearch for production
        if (process.env.ELASTICSEARCH_URL) {
            transports.push(
                new ElasticsearchTransport({
                    level: 'info',
                    clientOpts: {
                        node: process.env.ELASTICSEARCH_URL,
                    },
                    index: `logs-${this.service}`,
                })
            );
        }
        
        return winston.createLogger({
            level: this.level,
            format: winston.format.combine(...formats),
            defaultMeta: {
                service: this.service,
                environment: process.env.NODE_ENV,
                hostname: require('os').hostname(),
                pid: process.pid,
            },
            transports,
        });
    }
    
    devFormat(info) {
        const { timestamp, level, message, ...meta } = info;
        const metaString = Object.keys(meta).length ? 
            '\n' + JSON.stringify(meta, null, 2) : '';
        
        return `${timestamp} [${level}]: ${message}${metaString}`;
    }
    
    // Debug-specific methods
    trace(message, meta = {}) {
        const stack = new Error().stack;
        this.logger.debug(message, {
            ...meta,
            trace: stack,
            timestamp: Date.now(),
        });
    }
    
    timing(label, fn) {
        const start = process.hrtime.bigint();
        const result = fn();
        const end = process.hrtime.bigint();
        const duration = Number(end - start) / 1000000; // Convert to ms
        
        this.logger.debug(`Timing: ${label}`, {
            duration,
            unit: 'ms',
        });
        
        return result;
    }
    
    memory() {
        const usage = process.memoryUsage();
        this.logger.debug('Memory usage', {
            rss: `${Math.round(usage.rss / 1024 / 1024)}MB`,
            heapTotal: `${Math.round(usage.heapTotal / 1024 / 1024)}MB`,
            heapUsed: `${Math.round(usage.heapUsed / 1024 / 1024)}MB`,
            external: `${Math.round(usage.external / 1024 / 1024)}MB`,
        });
    }
}

// Debug context manager
class DebugContext {
    constructor() {
        this.contexts = new Map();
    }
    
    create(id, metadata = {}) {
        const context = {
            id,
            startTime: Date.now(),
            metadata,
            logs: [],
            spans: [],
        };
        
        this.contexts.set(id, context);
        return context;
    }
    
    log(contextId, level, message, data = {}) {
        const context = this.contexts.get(contextId);
        if (context) {
            context.logs.push({
                timestamp: Date.now(),
                level,
                message,
                data,
            });
        }
    }
    
    export(contextId) {
        const context = this.contexts.get(contextId);
        if (!context) return null;
        
        return {
            ...context,
            duration: Date.now() - context.startTime,
            logCount: context.logs.length,
        };
    }
}
```

### 5. Source Map Configuration

Set up source map support for production debugging:

**Source Map Setup**
```javascript
// webpack.config.js
module.exports = {
    mode: 'production',
    devtool: 'hidden-source-map', // Generate source maps but don't reference them
    
    output: {
        filename: '[name].[contenthash].js',
        sourceMapFilename: 'sourcemaps/[name].[contenthash].js.map',
    },
    
    plugins: [
        // Upload source maps to error tracking service
        new SentryWebpackPlugin({
            authToken: process.env.SENTRY_AUTH_TOKEN,
            org: 'your-org',
            project: 'your-project',
            include: './dist',
            ignore: ['node_modules'],
            urlPrefix: '~/',
            release: process.env.RELEASE_VERSION,
            deleteAfterCompile: true,
        }),
    ],
};

// Runtime source map support
require('source-map-support').install({
    environment: 'node',
    handleUncaughtExceptions: false,
    retrieveSourceMap(source) {
        // Custom source map retrieval for production
        if (process.env.NODE_ENV === 'production') {
            const sourceMapUrl = getSourceMapUrl(source);
            if (sourceMapUrl) {
                const map = fetchSourceMap(sourceMapUrl);
                return {
                    url: source,
                    map: map,
                };
            }
        }
        return null;
    },
});

// Stack trace enhancement
Error.prepareStackTrace = (error, stack) => {
    const mapped = stack.map(frame => {
        const fileName = frame.getFileName();
        const lineNumber = frame.getLineNumber();
        const columnNumber = frame.getColumnNumber();
        
        // Try to get original position
        const original = getOriginalPosition(fileName, lineNumber, columnNumber);
        
        return {
            function: frame.getFunctionName() || '<anonymous>',
            file: original?.source || fileName,
            line: original?.line || lineNumber,
            column: original?.column || columnNumber,
            native: frame.isNative(),
            async: frame.isAsync(),
        };
    });
    
    return {
        message: error.message,
        stack: mapped,
    };
};
```

### 6. Performance Profiling

Implement performance profiling tools:

**Performance Profiler**
```javascript
// performance-profiler.js
const v8Profiler = require('v8-profiler-next');
const fs = require('fs');
const path = require('path');

class PerformanceProfiler {
    constructor(options = {}) {
        this.outputDir = options.outputDir || './profiles';
        this.profiles = new Map();
        
        // Ensure output directory exists
        if (!fs.existsSync(this.outputDir)) {
            fs.mkdirSync(this.outputDir, { recursive: true });
        }
    }
    
    startCPUProfile(id, options = {}) {
        const title = options.title || `cpu-profile-${id}`;
        v8Profiler.startProfiling(title, true);
        
        this.profiles.set(id, {
            type: 'cpu',
            title,
            startTime: Date.now(),
        });
        
        return id;
    }
    
    stopCPUProfile(id) {
        const profileInfo = this.profiles.get(id);
        if (!profileInfo || profileInfo.type !== 'cpu') {
            throw new Error(`CPU profile ${id} not found`);
        }
        
        const profile = v8Profiler.stopProfiling(profileInfo.title);
        const duration = Date.now() - profileInfo.startTime;
        
        // Export profile
        const fileName = `${profileInfo.title}-${Date.now()}.cpuprofile`;
        const filePath = path.join(this.outputDir, fileName);
        
        profile.export((error, result) => {
            if (!error) {
                fs.writeFileSync(filePath, result);
                console.log(`CPU profile saved to ${filePath}`);
            }
            profile.delete();
        });
        
        this.profiles.delete(id);
        
        return {
            id,
            duration,
            filePath,
        };
    }
    
    takeHeapSnapshot(tag = '') {
        const fileName = `heap-${tag}-${Date.now()}.heapsnapshot`;
        const filePath = path.join(this.outputDir, fileName);
        
        const snapshot = v8Profiler.takeSnapshot();
        
        // Export snapshot
        snapshot.export((error, result) => {
            if (!error) {
                fs.writeFileSync(filePath, result);
                console.log(`Heap snapshot saved to ${filePath}`);
            }
            snapshot.delete();
        });
        
        return filePath;
    }
    
    measureFunction(fn, name = 'anonymous') {
        const measurements = {
            name,
            executions: 0,
            totalTime: 0,
            minTime: Infinity,
            maxTime: 0,
            avgTime: 0,
            lastExecution: null,
        };
        
        return new Proxy(fn, {
            apply(target, thisArg, args) {
                const start = process.hrtime.bigint();
                
                try {
                    const result = target.apply(thisArg, args);
                    
                    if (result instanceof Promise) {
                        return result.finally(() => {
                            this.recordExecution(start);
                        });
                    }
                    
                    this.recordExecution(start);
                    return result;
                } catch (error) {
                    this.recordExecution(start);
                    throw error;
                }
            },
            
            recordExecution(start) {
                const end = process.hrtime.bigint();
                const duration = Number(end - start) / 1000000; // Convert to ms
                
                measurements.executions++;
                measurements.totalTime += duration;
                measurements.minTime = Math.min(measurements.minTime, duration);
                measurements.maxTime = Math.max(measurements.maxTime, duration);
                measurements.avgTime = measurements.totalTime / measurements.executions;
                measurements.lastExecution = new Date();
                
                // Log slow executions
                if (duration > 100) {
                    console.warn(`Slow function execution: ${name} took ${duration}ms`);
                }
            },
            
            get(target, prop) {
                if (prop === 'measurements') {
                    return measurements;
                }
                return target[prop];
            },
        });
    }
}

// Memory leak detector
class MemoryLeakDetector {
    constructor() {
        this.snapshots = [];
        this.threshold = 50 * 1024 * 1024; // 50MB
    }
    
    start(interval = 60000) {
        this.interval = setInterval(() => {
            this.checkMemory();
        }, interval);
    }
    
    checkMemory() {
        const usage = process.memoryUsage();
        const snapshot = {
            timestamp: Date.now(),
            heapUsed: usage.heapUsed,
            external: usage.external,
            rss: usage.rss,
        };
        
        this.snapshots.push(snapshot);
        
        // Keep only last 10 snapshots
        if (this.snapshots.length > 10) {
            this.snapshots.shift();
        }
        
        // Check for memory leak pattern
        if (this.snapshots.length >= 5) {
            const trend = this.calculateTrend();
            if (trend.increasing && trend.delta > this.threshold) {
                console.error('Potential memory leak detected!', {
                    trend,
                    current: snapshot,
                });
                
                // Take heap snapshot for analysis
                const profiler = new PerformanceProfiler();
                profiler.takeHeapSnapshot('leak-detection');
            }
        }
    }
    
    calculateTrend() {
        const recent = this.snapshots.slice(-5);
        const first = recent[0];
        const last = recent[recent.length - 1];
        
        const delta = last.heapUsed - first.heapUsed;
        const increasing = recent.every((s, i) => 
            i === 0 || s.heapUsed > recent[i - 1].heapUsed
        );
        
        return {
            increasing,
            delta,
            rate: delta / (last.timestamp - first.timestamp) * 1000 * 60, // MB per minute
        };
    }
}
```

### 7. Debug Configuration Management

Centralize debug configurations:

**Debug Configuration**
```javascript
// debug-config.js
class DebugConfiguration {
    constructor() {
        this.config = {
            // Debug levels
            levels: {
                error: 0,
                warn: 1,
                info: 2,
                debug: 3,
                trace: 4,
            },
            
            // Feature flags
            features: {
                remoteDebugging: process.env.ENABLE_REMOTE_DEBUG === 'true',
                tracing: process.env.ENABLE_TRACING === 'true',
                profiling: process.env.ENABLE_PROFILING === 'true',
                memoryMonitoring: process.env.ENABLE_MEMORY_MONITORING === 'true',
            },
            
            // Debug endpoints
            endpoints: {
                jaeger: process.env.JAEGER_ENDPOINT || 'http://localhost:14268',
                elasticsearch: process.env.ELASTICSEARCH_URL || 'http://localhost:9200',
                sentry: process.env.SENTRY_DSN,
            },
            
            // Sampling rates
            sampling: {
                traces: parseFloat(process.env.TRACE_SAMPLING_RATE || '0.1'),
                profiles: parseFloat(process.env.PROFILE_SAMPLING_RATE || '0.01'),
                logs: parseFloat(process.env.LOG_SAMPLING_RATE || '1.0'),
            },
        };
    }
    
    isEnabled(feature) {
        return this.config.features[feature] || false;
    }
    
    getLevel() {
        const level = process.env.DEBUG_LEVEL || 'info';
        return this.config.levels[level] || 2;
    }
    
    shouldSample(type) {
        const rate = this.config.sampling[type] || 1.0;
        return Math.random() < rate;
    }
}

// Debug middleware factory
class DebugMiddlewareFactory {
    static create(app, config) {
        const middlewares = [];
        
        if (config.isEnabled('tracing')) {
            const tracingMiddleware = new TracingMiddleware();
            middlewares.push(tracingMiddleware.express());
        }
        
        if (config.isEnabled('profiling')) {
            middlewares.push(this.profilingMiddleware());
        }
        
        if (config.isEnabled('memoryMonitoring')) {
            const detector = new MemoryLeakDetector();
            detector.start();
        }
        
        // Debug routes
        if (process.env.NODE_ENV === 'development') {
            app.get('/debug/heap', (req, res) => {
                const profiler = new PerformanceProfiler();
                const path = profiler.takeHeapSnapshot('manual');
                res.json({ heapSnapshot: path });
            });
            
            app.get('/debug/profile', async (req, res) => {
                const profiler = new PerformanceProfiler();
                const id = profiler.startCPUProfile('manual');
                
                setTimeout(() => {
                    const result = profiler.stopCPUProfile(id);
                    res.json(result);
                }, 10000);
            });
            
            app.get('/debug/metrics', (req, res) => {
                res.json({
                    memory: process.memoryUsage(),
                    cpu: process.cpuUsage(),
                    uptime: process.uptime(),
                });
            });
        }
        
        return middlewares;
    }
    
    static profilingMiddleware() {
        const profiler = new PerformanceProfiler();
        
        return (req, res, next) => {
            if (Math.random() < 0.01) { // 1% sampling
                const id = profiler.startCPUProfile(`request-${Date.now()}`);
                
                res.on('finish', () => {
                    profiler.stopCPUProfile(id);
                });
            }
            
            next();
        };
    }
}
```

### 8. Production Debugging

Enable safe production debugging:

**Production Debug Tools**
```javascript
// production-debug.js
class ProductionDebugger {
    constructor(options = {}) {
        this.enabled = process.env.PRODUCTION_DEBUG === 'true';
        this.authToken = process.env.DEBUG_AUTH_TOKEN;
        this.allowedIPs = (process.env.DEBUG_ALLOWED_IPS || '').split(',');
    }
    
    middleware() {
        return (req, res, next) => {
            if (!this.enabled) {
                return next();
            }
            
            // Check authorization
            const token = req.headers['x-debug-token'];
            const ip = req.ip || req.connection.remoteAddress;
            
            if (token !== this.authToken || !this.allowedIPs.includes(ip)) {
                return next();
            }
            
            // Add debug headers
            res.setHeader('X-Debug-Enabled', 'true');
            
            // Enable debug mode for this request
            req.debugMode = true;
            req.debugContext = new DebugContext().create(req.id);
            
            // Override console for this request
            const originalConsole = { ...console };
            ['log', 'debug', 'info', 'warn', 'error'].forEach(method => {
                console[method] = (...args) => {
                    req.debugContext.log(req.id, method, args[0], args.slice(1));
                    originalConsole[method](...args);
                };
            });
            
            // Restore console on response
            res.on('finish', () => {
                Object.assign(console, originalConsole);
                
                // Send debug info if requested
                if (req.headers['x-debug-response'] === 'true') {
                    const debugInfo = req.debugContext.export(req.id);
                    res.setHeader('X-Debug-Info', JSON.stringify(debugInfo));
                }
            });
            
            next();
        };
    }
}

// Conditional breakpoints in production
class ConditionalBreakpoint {
    constructor(condition, callback) {
        this.condition = condition;
        this.callback = callback;
        this.hits = 0;
    }
    
    check(context) {
        if (this.condition(context)) {
            this.hits++;
            
            // Log breakpoint hit
            console.debug('Conditional breakpoint hit', {
                condition: this.condition.toString(),
                hits: this.hits,
                context,
            });
            
            // Execute callback
            if (this.callback) {
                this.callback(context);
            }
            
            // In production, don't actually break
            if (process.env.NODE_ENV === 'production') {
                // Take snapshot instead
                const profiler = new PerformanceProfiler();
                profiler.takeHeapSnapshot(`breakpoint-${Date.now()}`);
            } else {
                // In development, use debugger
                debugger;
            }
        }
    }
}

// Usage
const breakpoints = new Map();

// Set conditional breakpoint
breakpoints.set('high-memory', new ConditionalBreakpoint(
    (context) => context.memoryUsage > 500 * 1024 * 1024, // 500MB
    (context) => {
        console.error('High memory usage detected', context);
        // Send alert
        alerting.send('high-memory', context);
    }
));

// Check breakpoints in code
function checkBreakpoints(context) {
    breakpoints.forEach(breakpoint => {
        breakpoint.check(context);
    });
}
```

### 9. Debug Dashboard

Create a debug dashboard for monitoring:

**Debug Dashboard**
```html
<!-- debug-dashboard.html -->
<!DOCTYPE html>
<html>
<head>
    <title>Debug Dashboard</title>
    <style>
        body { font-family: monospace; background: #1e1e1e; color: #d4d4d4; }
        .container { max-width: 1200px; margin: 0 auto; padding: 20px; }
        .metric { background: #252526; padding: 15px; margin: 10px 0; border-radius: 5px; }
        .metric h3 { margin: 0 0 10px 0; color: #569cd6; }
        .chart { height: 200px; background: #1e1e1e; margin: 10px 0; }
        .log-entry { padding: 5px; border-bottom: 1px solid #3e3e3e; }
        .error { color: #f44747; }
        .warn { color: #ff9800; }
        .info { color: #4fc3f7; }
        .debug { color: #4caf50; }
    </style>
</head>
<body>
    <div class="container">
        <h1>Debug Dashboard</h1>
        
        <div class="metric">
            <h3>System Metrics</h3>
            <div id="metrics"></div>
        </div>
        
        <div class="metric">
            <h3>Memory Usage</h3>
            <canvas id="memoryChart" class="chart"></canvas>
        </div>
        
        <div class="metric">
            <h3>Request Traces</h3>
            <div id="traces"></div>
        </div>
        
        <div class="metric">
            <h3>Debug Logs</h3>
            <div id="logs"></div>
        </div>
    </div>
    
    <script>
        // WebSocket connection for real-time updates
        const ws = new WebSocket('ws://localhost:9231/debug');
        
        ws.onmessage = (event) => {
            const data = JSON.parse(event.data);
            
            switch (data.type) {
                case 'metrics':
                    updateMetrics(data.payload);
                    break;
                case 'trace':
                    addTrace(data.payload);
                    break;
                case 'log':
                    addLog(data.payload);
                    break;
            }
        };
        
        function updateMetrics(metrics) {
            const container = document.getElementById('metrics');
            container.innerHTML = `
                <div>CPU: ${metrics.cpu.percent}%</div>
                <div>Memory: ${metrics.memory.used}MB / ${metrics.memory.total}MB</div>
                <div>Uptime: ${metrics.uptime}s</div>
                <div>Active Requests: ${metrics.activeRequests}</div>
            `;
        }
        
        function addTrace(trace) {
            const container = document.getElementById('traces');
            const entry = document.createElement('div');
            entry.className = 'log-entry';
            entry.innerHTML = `
                <span>${trace.timestamp}</span>
                <span>${trace.method} ${trace.path}</span>
                <span>${trace.duration}ms</span>
                <span>${trace.status}</span>
            `;
            container.insertBefore(entry, container.firstChild);
        }
        
        function addLog(log) {
            const container = document.getElementById('logs');
            const entry = document.createElement('div');
            entry.className = `log-entry ${log.level}`;
            entry.innerHTML = `
                <span>${log.timestamp}</span>
                <span>[${log.level.toUpperCase()}]</span>
                <span>${log.message}</span>
            `;
            container.insertBefore(entry, container.firstChild);
            
            // Keep only last 100 logs
            while (container.children.length > 100) {
                container.removeChild(container.lastChild);
            }
        }
        
        // Memory usage chart
        const memoryChart = document.getElementById('memoryChart').getContext('2d');
        const memoryData = [];
        
        function updateMemoryChart(usage) {
            memoryData.push({
                time: new Date(),
                value: usage,
            });
            
            // Keep last 50 points
            if (memoryData.length > 50) {
                memoryData.shift();
            }
            
            // Draw chart
            // ... chart drawing logic
        }
    </script>
</body>
</html>
```

### 10. IDE Integration

Configure IDE debugging features:

**IDE Debug Extensions**
```json
// .vscode/extensions.json
{
    "recommendations": [
        "ms-vscode.vscode-js-debug",
        "msjsdiag.debugger-for-chrome",
        "ms-vscode.vscode-typescript-tslint-plugin",
        "dbaeumer.vscode-eslint",
        "ms-azuretools.vscode-docker",
        "humao.rest-client",
        "eamodio.gitlens",
        "usernamehw.errorlens",
        "wayou.vscode-todo-highlight",
        "formulahendry.code-runner"
    ]
}

// .vscode/tasks.json
{
    "version": "2.0.0",
    "tasks": [
        {
            "label": "Start Debug Server",
            "type": "npm",
            "script": "debug",
            "problemMatcher": [],
            "presentation": {
                "reveal": "always",
                "panel": "dedicated"
            }
        },
        {
            "label": "Profile Application",
            "type": "shell",
            "command": "node --inspect-brk --cpu-prof --cpu-prof-dir=./profiles ${workspaceFolder}/src/index.js",
            "problemMatcher": []
        },
        {
            "label": "Memory Snapshot",
            "type": "shell",
            "command": "node --inspect --expose-gc ${workspaceFolder}/scripts/heap-snapshot.js",
            "problemMatcher": []
        }
    ]
}
```

## Output Format

1. **Debug Configuration**: Complete setup for all debugging tools
2. **Integration Guide**: Step-by-step integration instructions
3. **Troubleshooting Playbook**: Common debugging scenarios and solutions
4. **Performance Baselines**: Metrics for comparison
5. **Debug Scripts**: Automated debugging utilities
6. **Dashboard Setup**: Real-time debugging interface
7. **Documentation**: Team debugging guidelines
8. **Emergency Procedures**: Production debugging protocols

Focus on creating a comprehensive debugging environment that enhances developer productivity and enables rapid issue resolution in all environments.
````

## File: tools/deploy-checklist.md

````markdown
# Deployment Checklist and Configuration

Generate deployment configuration and checklist for: $ARGUMENTS

Create comprehensive deployment artifacts:

1. **Pre-Deployment Checklist**:
   - [ ] All tests passing
   - [ ] Security scan completed
   - [ ] Performance benchmarks met
   - [ ] Documentation updated
   - [ ] Database migrations tested
   - [ ] Rollback plan documented
   - [ ] Monitoring alerts configured
   - [ ] Load testing completed

2. **Infrastructure Configuration**:
   - Docker/containerization setup
   - Kubernetes manifests
   - Terraform/IaC scripts
   - Environment variables
   - Secrets management
   - Network policies
   - Auto-scaling rules

3. **CI/CD Pipeline**:
   - GitHub Actions/GitLab CI
   - Build optimization
   - Test parallelization
   - Security scanning
   - Image building
   - Deployment stages
   - Rollback automation

4. **Database Deployment**:
   - Migration scripts
   - Backup procedures
   - Connection pooling
   - Read replica setup
   - Failover configuration
   - Data seeding
   - Version compatibility

5. **Monitoring Setup**:
   - Application metrics
   - Infrastructure metrics
   - Log aggregation
   - Error tracking
   - Uptime monitoring
   - Custom dashboards
   - Alert channels

6. **Security Configuration**:
   - SSL/TLS setup
   - API key rotation
   - CORS policies
   - Rate limiting
   - WAF rules
   - Security headers
   - Vulnerability scanning

7. **Post-Deployment**:
   - [ ] Smoke tests
   - [ ] Performance validation
   - [ ] Monitoring verification
   - [ ] Documentation published
   - [ ] Team notification
   - [ ] Customer communication
   - [ ] Metrics baseline

Include environment-specific configurations (dev, staging, prod) and disaster recovery procedures.
````

## File: tools/deps-audit.md

````markdown
# Dependency Audit and Security Analysis

You are a dependency security expert specializing in vulnerability scanning, license compliance, and supply chain security. Analyze project dependencies for known vulnerabilities, licensing issues, outdated packages, and provide actionable remediation strategies.

## Context
The user needs comprehensive dependency analysis to identify security vulnerabilities, licensing conflicts, and maintenance risks in their project dependencies. Focus on actionable insights with automated fixes where possible.

## Requirements
$ARGUMENTS

## Instructions

### 1. Dependency Discovery

Scan and inventory all project dependencies:

**Multi-Language Detection**
```python
import os
import json
import toml
import yaml
from pathlib import Path

class DependencyDiscovery:
    def __init__(self, project_path):
        self.project_path = Path(project_path)
        self.dependency_files = {
            'npm': ['package.json', 'package-lock.json', 'yarn.lock'],
            'python': ['requirements.txt', 'Pipfile', 'Pipfile.lock', 'pyproject.toml', 'poetry.lock'],
            'ruby': ['Gemfile', 'Gemfile.lock'],
            'java': ['pom.xml', 'build.gradle', 'build.gradle.kts'],
            'go': ['go.mod', 'go.sum'],
            'rust': ['Cargo.toml', 'Cargo.lock'],
            'php': ['composer.json', 'composer.lock'],
            'dotnet': ['*.csproj', 'packages.config', 'project.json']
        }
        
    def discover_all_dependencies(self):
        """
        Discover all dependencies across different package managers
        """
        dependencies = {}
        
        # NPM/Yarn dependencies
        if (self.project_path / 'package.json').exists():
            dependencies['npm'] = self._parse_npm_dependencies()
            
        # Python dependencies
        if (self.project_path / 'requirements.txt').exists():
            dependencies['python'] = self._parse_requirements_txt()
        elif (self.project_path / 'Pipfile').exists():
            dependencies['python'] = self._parse_pipfile()
        elif (self.project_path / 'pyproject.toml').exists():
            dependencies['python'] = self._parse_pyproject_toml()
            
        # Go dependencies
        if (self.project_path / 'go.mod').exists():
            dependencies['go'] = self._parse_go_mod()
            
        return dependencies
    
    def _parse_npm_dependencies(self):
        """
        Parse NPM package.json and lock files
        """
        with open(self.project_path / 'package.json', 'r') as f:
            package_json = json.load(f)
            
        deps = {}
        
        # Direct dependencies
        for dep_type in ['dependencies', 'devDependencies', 'peerDependencies']:
            if dep_type in package_json:
                for name, version in package_json[dep_type].items():
                    deps[name] = {
                        'version': version,
                        'type': dep_type,
                        'direct': True
                    }
        
        # Parse lock file for exact versions
        if (self.project_path / 'package-lock.json').exists():
            with open(self.project_path / 'package-lock.json', 'r') as f:
                lock_data = json.load(f)
                self._parse_npm_lock(lock_data, deps)
                
        return deps
```

**Dependency Tree Analysis**
```python
def build_dependency_tree(dependencies):
    """
    Build complete dependency tree including transitive dependencies
    """
    tree = {
        'root': {
            'name': 'project',
            'version': '1.0.0',
            'dependencies': {}
        }
    }
    
    def add_dependencies(node, deps, visited=None):
        if visited is None:
            visited = set()
            
        for dep_name, dep_info in deps.items():
            if dep_name in visited:
                # Circular dependency detected
                node['dependencies'][dep_name] = {
                    'circular': True,
                    'version': dep_info['version']
                }
                continue
                
            visited.add(dep_name)
            
            node['dependencies'][dep_name] = {
                'version': dep_info['version'],
                'type': dep_info.get('type', 'runtime'),
                'dependencies': {}
            }
            
            # Recursively add transitive dependencies
            if 'dependencies' in dep_info:
                add_dependencies(
                    node['dependencies'][dep_name],
                    dep_info['dependencies'],
                    visited.copy()
                )
    
    add_dependencies(tree['root'], dependencies)
    return tree
```

### 2. Vulnerability Scanning

Check dependencies against vulnerability databases:

**CVE Database Check**
```python
import requests
from datetime import datetime

class VulnerabilityScanner:
    def __init__(self):
        self.vulnerability_apis = {
            'npm': 'https://registry.npmjs.org/-/npm/v1/security/advisories/bulk',
            'pypi': 'https://pypi.org/pypi/{package}/json',
            'rubygems': 'https://rubygems.org/api/v1/gems/{package}.json',
            'maven': 'https://ossindex.sonatype.org/api/v3/component-report'
        }
        
    def scan_vulnerabilities(self, dependencies):
        """
        Scan dependencies for known vulnerabilities
        """
        vulnerabilities = []
        
        for package_name, package_info in dependencies.items():
            vulns = self._check_package_vulnerabilities(
                package_name,
                package_info['version'],
                package_info.get('ecosystem', 'npm')
            )
            
            if vulns:
                vulnerabilities.extend(vulns)
                
        return self._analyze_vulnerabilities(vulnerabilities)
    
    def _check_package_vulnerabilities(self, name, version, ecosystem):
        """
        Check specific package for vulnerabilities
        """
        if ecosystem == 'npm':
            return self._check_npm_vulnerabilities(name, version)
        elif ecosystem == 'pypi':
            return self._check_python_vulnerabilities(name, version)
        elif ecosystem == 'maven':
            return self._check_java_vulnerabilities(name, version)
            
    def _check_npm_vulnerabilities(self, name, version):
        """
        Check NPM package vulnerabilities
        """
        # Using npm audit API
        response = requests.post(
            'https://registry.npmjs.org/-/npm/v1/security/advisories/bulk',
            json={name: [version]}
        )
        
        vulnerabilities = []
        if response.status_code == 200:
            data = response.json()
            if name in data:
                for advisory in data[name]:
                    vulnerabilities.append({
                        'package': name,
                        'version': version,
                        'severity': advisory['severity'],
                        'title': advisory['title'],
                        'cve': advisory.get('cves', []),
                        'description': advisory['overview'],
                        'recommendation': advisory['recommendation'],
                        'patched_versions': advisory['patched_versions'],
                        'published': advisory['created']
                    })
                    
        return vulnerabilities
```

**Severity Analysis**
```python
def analyze_vulnerability_severity(vulnerabilities):
    """
    Analyze and prioritize vulnerabilities by severity
    """
    severity_scores = {
        'critical': 9.0,
        'high': 7.0,
        'moderate': 4.0,
        'low': 1.0
    }
    
    analysis = {
        'total': len(vulnerabilities),
        'by_severity': {
            'critical': [],
            'high': [],
            'moderate': [],
            'low': []
        },
        'risk_score': 0,
        'immediate_action_required': []
    }
    
    for vuln in vulnerabilities:
        severity = vuln['severity'].lower()
        analysis['by_severity'][severity].append(vuln)
        
        # Calculate risk score
        base_score = severity_scores.get(severity, 0)
        
        # Adjust score based on factors
        if vuln.get('exploit_available', False):
            base_score *= 1.5
        if vuln.get('publicly_disclosed', True):
            base_score *= 1.2
        if 'remote_code_execution' in vuln.get('description', '').lower():
            base_score *= 2.0
            
        vuln['risk_score'] = base_score
        analysis['risk_score'] += base_score
        
        # Flag immediate action items
        if severity in ['critical', 'high'] or base_score > 8.0:
            analysis['immediate_action_required'].append({
                'package': vuln['package'],
                'severity': severity,
                'action': f"Update to {vuln['patched_versions']}"
            })
    
    # Sort by risk score
    for severity in analysis['by_severity']:
        analysis['by_severity'][severity].sort(
            key=lambda x: x.get('risk_score', 0),
            reverse=True
        )
    
    return analysis
```

### 3. License Compliance

Analyze dependency licenses for compatibility:

**License Detection**
```python
class LicenseAnalyzer:
    def __init__(self):
        self.license_compatibility = {
            'MIT': ['MIT', 'BSD', 'Apache-2.0', 'ISC'],
            'Apache-2.0': ['Apache-2.0', 'MIT', 'BSD'],
            'GPL-3.0': ['GPL-3.0', 'GPL-2.0'],
            'BSD-3-Clause': ['BSD-3-Clause', 'MIT', 'Apache-2.0'],
            'proprietary': []
        }
        
        self.license_restrictions = {
            'GPL-3.0': 'Copyleft - requires source code disclosure',
            'AGPL-3.0': 'Strong copyleft - network use requires source disclosure',
            'proprietary': 'Cannot be used without explicit license',
            'unknown': 'License unclear - legal review required'
        }
        
    def analyze_licenses(self, dependencies, project_license='MIT'):
        """
        Analyze license compatibility
        """
        issues = []
        license_summary = {}
        
        for package_name, package_info in dependencies.items():
            license_type = package_info.get('license', 'unknown')
            
            # Track license usage
            if license_type not in license_summary:
                license_summary[license_type] = []
            license_summary[license_type].append(package_name)
            
            # Check compatibility
            if not self._is_compatible(project_license, license_type):
                issues.append({
                    'package': package_name,
                    'license': license_type,
                    'issue': f'Incompatible with project license {project_license}',
                    'severity': 'high',
                    'recommendation': self._get_license_recommendation(
                        license_type,
                        project_license
                    )
                })
            
            # Check for restrictive licenses
            if license_type in self.license_restrictions:
                issues.append({
                    'package': package_name,
                    'license': license_type,
                    'issue': self.license_restrictions[license_type],
                    'severity': 'medium',
                    'recommendation': 'Review usage and ensure compliance'
                })
        
        return {
            'summary': license_summary,
            'issues': issues,
            'compliance_status': 'FAIL' if issues else 'PASS'
        }
```

**License Report**
```markdown
## License Compliance Report

### Summary
- **Project License**: MIT
- **Total Dependencies**: 245
- **License Issues**: 3
- **Compliance Status**: ‚ö†Ô∏è REVIEW REQUIRED

### License Distribution
| License | Count | Packages |
|---------|-------|----------|
| MIT | 180 | express, lodash, ... |
| Apache-2.0 | 45 | aws-sdk, ... |
| BSD-3-Clause | 15 | ... |
| GPL-3.0 | 3 | [ISSUE] package1, package2, package3 |
| Unknown | 2 | [ISSUE] mystery-lib, old-package |

### Compliance Issues

#### High Severity
1. **GPL-3.0 Dependencies**
   - Packages: package1, package2, package3
   - Issue: GPL-3.0 is incompatible with MIT license
   - Risk: May require open-sourcing your entire project
   - Recommendation: 
     - Replace with MIT/Apache licensed alternatives
     - Or change project license to GPL-3.0

#### Medium Severity
2. **Unknown Licenses**
   - Packages: mystery-lib, old-package
   - Issue: Cannot determine license compatibility
   - Risk: Potential legal exposure
   - Recommendation:
     - Contact package maintainers
     - Review source code for license information
     - Consider replacing with known alternatives
```

### 4. Outdated Dependencies

Identify and prioritize dependency updates:

**Version Analysis**
```python
def analyze_outdated_dependencies(dependencies):
    """
    Check for outdated dependencies
    """
    outdated = []
    
    for package_name, package_info in dependencies.items():
        current_version = package_info['version']
        latest_version = fetch_latest_version(package_name, package_info['ecosystem'])
        
        if is_outdated(current_version, latest_version):
            # Calculate how outdated
            version_diff = calculate_version_difference(current_version, latest_version)
            
            outdated.append({
                'package': package_name,
                'current': current_version,
                'latest': latest_version,
                'type': version_diff['type'],  # major, minor, patch
                'releases_behind': version_diff['count'],
                'age_days': get_version_age(package_name, current_version),
                'breaking_changes': version_diff['type'] == 'major',
                'update_effort': estimate_update_effort(version_diff),
                'changelog': fetch_changelog(package_name, current_version, latest_version)
            })
    
    return prioritize_updates(outdated)

def prioritize_updates(outdated_deps):
    """
    Prioritize updates based on multiple factors
    """
    for dep in outdated_deps:
        score = 0
        
        # Security updates get highest priority
        if dep.get('has_security_fix', False):
            score += 100
            
        # Major version updates
        if dep['type'] == 'major':
            score += 20
        elif dep['type'] == 'minor':
            score += 10
        else:
            score += 5
            
        # Age factor
        if dep['age_days'] > 365:
            score += 30
        elif dep['age_days'] > 180:
            score += 20
        elif dep['age_days'] > 90:
            score += 10
            
        # Number of releases behind
        score += min(dep['releases_behind'] * 2, 20)
        
        dep['priority_score'] = score
        dep['priority'] = 'critical' if score > 80 else 'high' if score > 50 else 'medium'
    
    return sorted(outdated_deps, key=lambda x: x['priority_score'], reverse=True)
```

### 5. Dependency Size Analysis

Analyze bundle size impact:

**Bundle Size Impact**
```javascript
// Analyze NPM package sizes
const analyzeBundleSize = async (dependencies) => {
    const sizeAnalysis = {
        totalSize: 0,
        totalGzipped: 0,
        packages: [],
        recommendations: []
    };
    
    for (const [packageName, info] of Object.entries(dependencies)) {
        try {
            // Fetch package stats
            const response = await fetch(
                `https://bundlephobia.com/api/size?package=${packageName}@${info.version}`
            );
            const data = await response.json();
            
            const packageSize = {
                name: packageName,
                version: info.version,
                size: data.size,
                gzip: data.gzip,
                dependencyCount: data.dependencyCount,
                hasJSNext: data.hasJSNext,
                hasSideEffects: data.hasSideEffects
            };
            
            sizeAnalysis.packages.push(packageSize);
            sizeAnalysis.totalSize += data.size;
            sizeAnalysis.totalGzipped += data.gzip;
            
            // Size recommendations
            if (data.size > 1000000) { // 1MB
                sizeAnalysis.recommendations.push({
                    package: packageName,
                    issue: 'Large bundle size',
                    size: `${(data.size / 1024 / 1024).toFixed(2)} MB`,
                    suggestion: 'Consider lighter alternatives or lazy loading'
                });
            }
        } catch (error) {
            console.error(`Failed to analyze ${packageName}:`, error);
        }
    }
    
    // Sort by size
    sizeAnalysis.packages.sort((a, b) => b.size - a.size);
    
    // Add top offenders
    sizeAnalysis.topOffenders = sizeAnalysis.packages.slice(0, 10);
    
    return sizeAnalysis;
};
```

### 6. Supply Chain Security

Check for dependency hijacking and typosquatting:

**Supply Chain Checks**
```python
def check_supply_chain_security(dependencies):
    """
    Perform supply chain security checks
    """
    security_issues = []
    
    for package_name, package_info in dependencies.items():
        # Check for typosquatting
        typo_check = check_typosquatting(package_name)
        if typo_check['suspicious']:
            security_issues.append({
                'type': 'typosquatting',
                'package': package_name,
                'severity': 'high',
                'similar_to': typo_check['similar_packages'],
                'recommendation': 'Verify package name spelling'
            })
        
        # Check maintainer changes
        maintainer_check = check_maintainer_changes(package_name)
        if maintainer_check['recent_changes']:
            security_issues.append({
                'type': 'maintainer_change',
                'package': package_name,
                'severity': 'medium',
                'details': maintainer_check['changes'],
                'recommendation': 'Review recent package changes'
            })
        
        # Check for suspicious patterns
        if contains_suspicious_patterns(package_info):
            security_issues.append({
                'type': 'suspicious_behavior',
                'package': package_name,
                'severity': 'high',
                'patterns': package_info['suspicious_patterns'],
                'recommendation': 'Audit package source code'
            })
    
    return security_issues

def check_typosquatting(package_name):
    """
    Check if package name might be typosquatting
    """
    common_packages = [
        'react', 'express', 'lodash', 'axios', 'webpack',
        'babel', 'jest', 'typescript', 'eslint', 'prettier'
    ]
    
    for legit_package in common_packages:
        distance = levenshtein_distance(package_name.lower(), legit_package)
        if 0 < distance <= 2:  # Close but not exact match
            return {
                'suspicious': True,
                'similar_packages': [legit_package],
                'distance': distance
            }
    
    return {'suspicious': False}
```

### 7. Automated Remediation

Generate automated fixes:

**Update Scripts**
```bash
#!/bin/bash
# Auto-update dependencies with security fixes

echo "üîí Security Update Script"
echo "========================"

# NPM/Yarn updates
if [ -f "package.json" ]; then
    echo "üì¶ Updating NPM dependencies..."
    
    # Audit and auto-fix
    npm audit fix --force
    
    # Update specific vulnerable packages
    npm update package1@^2.0.0 package2@~3.1.0
    
    # Run tests
    npm test
    
    if [ $? -eq 0 ]; then
        echo "‚úÖ NPM updates successful"
    else
        echo "‚ùå Tests failed, reverting..."
        git checkout package-lock.json
    fi
fi

# Python updates
if [ -f "requirements.txt" ]; then
    echo "üêç Updating Python dependencies..."
    
    # Create backup
    cp requirements.txt requirements.txt.backup
    
    # Update vulnerable packages
    pip-compile --upgrade-package package1 --upgrade-package package2
    
    # Test installation
    pip install -r requirements.txt --dry-run
    
    if [ $? -eq 0 ]; then
        echo "‚úÖ Python updates successful"
    else
        echo "‚ùå Update failed, reverting..."
        mv requirements.txt.backup requirements.txt
    fi
fi
```

**Pull Request Generation**
```python
def generate_dependency_update_pr(updates):
    """
    Generate PR with dependency updates
    """
    pr_body = f"""
## üîí Dependency Security Update

This PR updates {len(updates)} dependencies to address security vulnerabilities and outdated packages.

### Security Fixes ({sum(1 for u in updates if u['has_security'])})

| Package | Current | Updated | Severity | CVE |
|---------|---------|---------|----------|-----|
"""
    
    for update in updates:
        if update['has_security']:
            pr_body += f"| {update['package']} | {update['current']} | {update['target']} | {update['severity']} | {', '.join(update['cves'])} |\n"
    
    pr_body += """

### Other Updates

| Package | Current | Updated | Type | Age |
|---------|---------|---------|------|-----|
"""
    
    for update in updates:
        if not update['has_security']:
            pr_body += f"| {update['package']} | {update['current']} | {update['target']} | {update['type']} | {update['age_days']} days |\n"
    
    pr_body += """

### Testing
- [ ] All tests pass
- [ ] No breaking changes identified
- [ ] Bundle size impact reviewed

### Review Checklist
- [ ] Security vulnerabilities addressed
- [ ] License compliance maintained
- [ ] No unexpected dependencies added
- [ ] Performance impact assessed

cc @security-team
"""
    
    return {
        'title': f'chore(deps): Security update for {len(updates)} dependencies',
        'body': pr_body,
        'branch': f'deps/security-update-{datetime.now().strftime("%Y%m%d")}',
        'labels': ['dependencies', 'security']
    }
```

### 8. Monitoring and Alerts

Set up continuous dependency monitoring:

**GitHub Actions Workflow**
```yaml
name: Dependency Audit

on:
  schedule:
    - cron: '0 0 * * *'  # Daily
  push:
    paths:
      - 'package*.json'
      - 'requirements.txt'
      - 'Gemfile*'
      - 'go.mod'
  workflow_dispatch:

jobs:
  security-audit:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Run NPM Audit
      if: hashFiles('package.json')
      run: |
        npm audit --json > npm-audit.json
        if [ $(jq '.vulnerabilities.total' npm-audit.json) -gt 0 ]; then
          echo "::error::Found $(jq '.vulnerabilities.total' npm-audit.json) vulnerabilities"
          exit 1
        fi
    
    - name: Run Python Safety Check
      if: hashFiles('requirements.txt')
      run: |
        pip install safety
        safety check --json > safety-report.json
        
    - name: Check Licenses
      run: |
        npx license-checker --json > licenses.json
        python scripts/check_license_compliance.py
    
    - name: Create Issue for Critical Vulnerabilities
      if: failure()
      uses: actions/github-script@v6
      with:
        script: |
          const audit = require('./npm-audit.json');
          const critical = audit.vulnerabilities.critical;
          
          if (critical > 0) {
            github.rest.issues.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title: `üö® ${critical} critical vulnerabilities found`,
              body: 'Dependency audit found critical vulnerabilities. See workflow run for details.',
              labels: ['security', 'dependencies', 'critical']
            });
          }
```

## Output Format

1. **Executive Summary**: High-level risk assessment and action items
2. **Vulnerability Report**: Detailed CVE analysis with severity ratings
3. **License Compliance**: Compatibility matrix and legal risks
4. **Update Recommendations**: Prioritized list with effort estimates
5. **Supply Chain Analysis**: Typosquatting and hijacking risks
6. **Remediation Scripts**: Automated update commands and PR generation
7. **Size Impact Report**: Bundle size analysis and optimization tips
8. **Monitoring Setup**: CI/CD integration for continuous scanning

Focus on actionable insights that help maintain secure, compliant, and efficient dependency management.
````

## File: tools/deps-upgrade.md

````markdown
# Dependency Upgrade Strategy

You are a dependency management expert specializing in safe, incremental upgrades of project dependencies. Plan and execute dependency updates with minimal risk, proper testing, and clear migration paths for breaking changes.

## Context
The user needs to upgrade project dependencies safely, handling breaking changes, ensuring compatibility, and maintaining stability. Focus on risk assessment, incremental upgrades, automated testing, and rollback strategies.

## Requirements
$ARGUMENTS

## Instructions

### 1. Dependency Update Analysis

Assess current dependency state and upgrade needs:

**Comprehensive Dependency Audit**
```python
import json
import subprocess
from datetime import datetime, timedelta
from packaging import version

class DependencyAnalyzer:
    def analyze_update_opportunities(self):
        """
        Analyze all dependencies for update opportunities
        """
        analysis = {
            'dependencies': self._analyze_dependencies(),
            'update_strategy': self._determine_strategy(),
            'risk_assessment': self._assess_risks(),
            'priority_order': self._prioritize_updates()
        }
        
        return analysis
    
    def _analyze_dependencies(self):
        """Analyze each dependency"""
        deps = {}
        
        # NPM analysis
        if self._has_npm():
            npm_output = subprocess.run(
                ['npm', 'outdated', '--json'],
                capture_output=True,
                text=True
            )
            if npm_output.stdout:
                npm_data = json.loads(npm_output.stdout)
                for pkg, info in npm_data.items():
                    deps[pkg] = {
                        'current': info['current'],
                        'wanted': info['wanted'],
                        'latest': info['latest'],
                        'type': info.get('type', 'dependencies'),
                        'ecosystem': 'npm',
                        'update_type': self._categorize_update(
                            info['current'], 
                            info['latest']
                        )
                    }
        
        # Python analysis
        if self._has_python():
            pip_output = subprocess.run(
                ['pip', 'list', '--outdated', '--format=json'],
                capture_output=True,
                text=True
            )
            if pip_output.stdout:
                pip_data = json.loads(pip_output.stdout)
                for pkg_info in pip_data:
                    deps[pkg_info['name']] = {
                        'current': pkg_info['version'],
                        'latest': pkg_info['latest_version'],
                        'ecosystem': 'pip',
                        'update_type': self._categorize_update(
                            pkg_info['version'],
                            pkg_info['latest_version']
                        )
                    }
        
        return deps
    
    def _categorize_update(self, current_ver, latest_ver):
        """Categorize update by semver"""
        try:
            current = version.parse(current_ver)
            latest = version.parse(latest_ver)
            
            if latest.major > current.major:
                return 'major'
            elif latest.minor > current.minor:
                return 'minor'
            elif latest.micro > current.micro:
                return 'patch'
            else:
                return 'none'
        except:
            return 'unknown'
```

### 2. Breaking Change Detection

Identify potential breaking changes:

**Breaking Change Scanner**
```python
class BreakingChangeDetector:
    def detect_breaking_changes(self, package_name, current_version, target_version):
        """
        Detect breaking changes between versions
        """
        breaking_changes = {
            'api_changes': [],
            'removed_features': [],
            'changed_behavior': [],
            'migration_required': False,
            'estimated_effort': 'low'
        }
        
        # Fetch changelog
        changelog = self._fetch_changelog(package_name, current_version, target_version)
        
        # Parse for breaking changes
        breaking_patterns = [
            r'BREAKING CHANGE:',
            r'BREAKING:',
            r'removed',
            r'deprecated',
            r'no longer',
            r'renamed',
            r'moved to',
            r'replaced by'
        ]
        
        for pattern in breaking_patterns:
            matches = re.finditer(pattern, changelog, re.IGNORECASE)
            for match in matches:
                context = self._extract_context(changelog, match.start())
                breaking_changes['api_changes'].append(context)
        
        # Check for specific patterns
        if package_name == 'react':
            breaking_changes.update(self._check_react_breaking_changes(
                current_version, target_version
            ))
        elif package_name == 'webpack':
            breaking_changes.update(self._check_webpack_breaking_changes(
                current_version, target_version
            ))
        
        # Estimate migration effort
        breaking_changes['estimated_effort'] = self._estimate_effort(breaking_changes)
        
        return breaking_changes
    
    def _check_react_breaking_changes(self, current, target):
        """React-specific breaking changes"""
        changes = {
            'api_changes': [],
            'migration_required': False
        }
        
        # React 15 to 16
        if current.startswith('15') and target.startswith('16'):
            changes['api_changes'].extend([
                'PropTypes moved to separate package',
                'React.createClass deprecated',
                'String refs deprecated'
            ])
            changes['migration_required'] = True
        
        # React 16 to 17
        elif current.startswith('16') and target.startswith('17'):
            changes['api_changes'].extend([
                'Event delegation changes',
                'No event pooling',
                'useEffect cleanup timing changes'
            ])
        
        # React 17 to 18
        elif current.startswith('17') and target.startswith('18'):
            changes['api_changes'].extend([
                'Automatic batching',
                'Stricter StrictMode',
                'Suspense changes',
                'New root API'
            ])
            changes['migration_required'] = True
        
        return changes
```

### 3. Migration Guide Generation

Create detailed migration guides:

**Migration Guide Generator**
```python
def generate_migration_guide(package_name, current_version, target_version, breaking_changes):
    """
    Generate step-by-step migration guide
    """
    guide = f"""
# Migration Guide: {package_name} {current_version} ‚Üí {target_version}

## Overview
This guide will help you upgrade {package_name} from version {current_version} to {target_version}.

**Estimated time**: {estimate_migration_time(breaking_changes)}
**Risk level**: {assess_risk_level(breaking_changes)}
**Breaking changes**: {len(breaking_changes['api_changes'])}

## Pre-Migration Checklist

- [ ] Current test suite passing
- [ ] Backup created / Git commit point marked
- [ ] Dependencies compatibility checked
- [ ] Team notified of upgrade

## Migration Steps

### Step 1: Update Dependencies

```bash
# Create a new branch
git checkout -b upgrade/{package_name}-{target_version}

# Update package
npm install {package_name}@{target_version}

# Update peer dependencies if needed
{generate_peer_deps_commands(package_name, target_version)}
```

### Step 2: Address Breaking Changes

{generate_breaking_change_fixes(breaking_changes)}

### Step 3: Update Code Patterns

{generate_code_updates(package_name, current_version, target_version)}

### Step 4: Run Codemods (if available)

{generate_codemod_commands(package_name, target_version)}

### Step 5: Test & Verify

```bash
# Run linter to catch issues
npm run lint

# Run tests
npm test

# Run type checking
npm run type-check

# Manual testing checklist
```

{generate_test_checklist(package_name, breaking_changes)}

### Step 6: Performance Validation

{generate_performance_checks(package_name)}

## Rollback Plan

If issues arise, follow these steps to rollback:

```bash
# Revert package version
git checkout package.json package-lock.json
npm install

# Or use the backup branch
git checkout main
git branch -D upgrade/{package_name}-{target_version}
```

## Common Issues & Solutions

{generate_common_issues(package_name, target_version)}

## Resources

- [Official Migration Guide]({get_official_guide_url(package_name, target_version)})
- [Changelog]({get_changelog_url(package_name, target_version)})
- [Community Discussions]({get_community_url(package_name)})
"""
    
    return guide
```

### 4. Incremental Upgrade Strategy

Plan safe incremental upgrades:

**Incremental Upgrade Planner**
```python
class IncrementalUpgrader:
    def plan_incremental_upgrade(self, package_name, current, target):
        """
        Plan incremental upgrade path
        """
        # Get all versions between current and target
        all_versions = self._get_versions_between(package_name, current, target)
        
        # Identify safe stopping points
        safe_versions = self._identify_safe_versions(all_versions)
        
        # Create upgrade path
        upgrade_path = self._create_upgrade_path(current, target, safe_versions)
        
        plan = f"""
## Incremental Upgrade Plan: {package_name}

### Current State
- Version: {current}
- Target: {target}
- Total steps: {len(upgrade_path)}

### Upgrade Path

"""
        for i, step in enumerate(upgrade_path, 1):
            plan += f"""
#### Step {i}: Upgrade to {step['version']}

**Risk Level**: {step['risk_level']}
**Breaking Changes**: {step['breaking_changes']}

```bash
# Upgrade command
npm install {package_name}@{step['version']}

# Test command
npm test -- --updateSnapshot

# Verification
npm run integration-tests
```

**Key Changes**:
{self._summarize_changes(step)}

**Testing Focus**:
{self._get_test_focus(step)}

---
"""
        
        return plan
    
    def _identify_safe_versions(self, versions):
        """Identify safe intermediate versions"""
        safe_versions = []
        
        for v in versions:
            # Safe versions are typically:
            # - Last patch of each minor version
            # - Versions with long stability period
            # - Versions before major API changes
            if (self._is_last_patch(v, versions) or 
                self._has_stability_period(v) or
                self._is_pre_breaking_change(v)):
                safe_versions.append(v)
        
        return safe_versions
```

### 5. Automated Testing Strategy

Ensure upgrades don't break functionality:

**Upgrade Test Suite**
```javascript
// upgrade-tests.js
const { runUpgradeTests } = require('./upgrade-test-framework');

async function testDependencyUpgrade(packageName, targetVersion) {
    const testSuite = {
        preUpgrade: async () => {
            // Capture baseline
            const baseline = {
                unitTests: await runTests('unit'),
                integrationTests: await runTests('integration'),
                e2eTests: await runTests('e2e'),
                performance: await capturePerformanceMetrics(),
                bundleSize: await measureBundleSize()
            };
            
            return baseline;
        },
        
        postUpgrade: async (baseline) => {
            // Run same tests after upgrade
            const results = {
                unitTests: await runTests('unit'),
                integrationTests: await runTests('integration'),
                e2eTests: await runTests('e2e'),
                performance: await capturePerformanceMetrics(),
                bundleSize: await measureBundleSize()
            };
            
            // Compare results
            const comparison = compareResults(baseline, results);
            
            return {
                passed: comparison.passed,
                failures: comparison.failures,
                regressions: comparison.regressions,
                improvements: comparison.improvements
            };
        },
        
        smokeTests: [
            async () => {
                // Critical path testing
                await testCriticalUserFlows();
            },
            async () => {
                // API compatibility
                await testAPICompatibility();
            },
            async () => {
                // Build process
                await testBuildProcess();
            }
        ]
    };
    
    return runUpgradeTests(testSuite);
}
```

### 6. Compatibility Matrix

Check compatibility across dependencies:

**Compatibility Checker**
```python
def generate_compatibility_matrix(dependencies):
    """
    Generate compatibility matrix for dependencies
    """
    matrix = {}
    
    for dep_name, dep_info in dependencies.items():
        matrix[dep_name] = {
            'current': dep_info['current'],
            'target': dep_info['latest'],
            'compatible_with': check_compatibility(dep_name, dep_info['latest']),
            'conflicts': find_conflicts(dep_name, dep_info['latest']),
            'peer_requirements': get_peer_requirements(dep_name, dep_info['latest'])
        }
    
    # Generate report
    report = """
## Dependency Compatibility Matrix

| Package | Current | Target | Compatible With | Conflicts | Action Required |
|---------|---------|--------|-----------------|-----------|-----------------|
"""
    
    for pkg, info in matrix.items():
        compatible = '‚úÖ' if not info['conflicts'] else '‚ö†Ô∏è'
        conflicts = ', '.join(info['conflicts']) if info['conflicts'] else 'None'
        action = 'Safe to upgrade' if not info['conflicts'] else 'Resolve conflicts first'
        
        report += f"| {pkg} | {info['current']} | {info['target']} | {compatible} | {conflicts} | {action} |\n"
    
    return report

def check_compatibility(package_name, version):
    """Check what this package is compatible with"""
    # Check package.json or requirements.txt
    peer_deps = get_peer_dependencies(package_name, version)
    compatible_packages = []
    
    for peer_pkg, peer_version_range in peer_deps.items():
        if is_installed(peer_pkg):
            current_peer_version = get_installed_version(peer_pkg)
            if satisfies_version_range(current_peer_version, peer_version_range):
                compatible_packages.append(f"{peer_pkg}@{current_peer_version}")
    
    return compatible_packages
```

### 7. Rollback Strategy

Implement safe rollback procedures:

**Rollback Manager**
```bash
#!/bin/bash
# rollback-dependencies.sh

# Create rollback point
create_rollback_point() {
    echo "üìå Creating rollback point..."
    
    # Save current state
    cp package.json package.json.backup
    cp package-lock.json package-lock.json.backup
    
    # Git tag
    git tag -a "pre-upgrade-$(date +%Y%m%d-%H%M%S)" -m "Pre-upgrade snapshot"
    
    # Database snapshot if needed
    if [ -f "database-backup.sh" ]; then
        ./database-backup.sh
    fi
    
    echo "‚úÖ Rollback point created"
}

# Perform rollback
rollback() {
    echo "üîÑ Performing rollback..."
    
    # Restore package files
    mv package.json.backup package.json
    mv package-lock.json.backup package-lock.json
    
    # Reinstall dependencies
    rm -rf node_modules
    npm ci
    
    # Run post-rollback tests
    npm test
    
    echo "‚úÖ Rollback complete"
}

# Verify rollback
verify_rollback() {
    echo "üîç Verifying rollback..."
    
    # Check critical functionality
    npm run test:critical
    
    # Check service health
    curl -f http://localhost:3000/health || exit 1
    
    echo "‚úÖ Rollback verified"
}
```

### 8. Batch Update Strategy

Handle multiple updates efficiently:

**Batch Update Planner**
```python
def plan_batch_updates(dependencies):
    """
    Plan efficient batch updates
    """
    # Group by update type
    groups = {
        'patch': [],
        'minor': [],
        'major': [],
        'security': []
    }
    
    for dep, info in dependencies.items():
        if info.get('has_security_vulnerability'):
            groups['security'].append(dep)
        else:
            groups[info['update_type']].append(dep)
    
    # Create update batches
    batches = []
    
    # Batch 1: Security updates (immediate)
    if groups['security']:
        batches.append({
            'priority': 'CRITICAL',
            'name': 'Security Updates',
            'packages': groups['security'],
            'strategy': 'immediate',
            'testing': 'full'
        })
    
    # Batch 2: Patch updates (safe)
    if groups['patch']:
        batches.append({
            'priority': 'HIGH',
            'name': 'Patch Updates',
            'packages': groups['patch'],
            'strategy': 'grouped',
            'testing': 'smoke'
        })
    
    # Batch 3: Minor updates (careful)
    if groups['minor']:
        batches.append({
            'priority': 'MEDIUM',
            'name': 'Minor Updates',
            'packages': groups['minor'],
            'strategy': 'incremental',
            'testing': 'regression'
        })
    
    # Batch 4: Major updates (planned)
    if groups['major']:
        batches.append({
            'priority': 'LOW',
            'name': 'Major Updates',
            'packages': groups['major'],
            'strategy': 'individual',
            'testing': 'comprehensive'
        })
    
    return generate_batch_plan(batches)
```

### 9. Framework-Specific Upgrades

Handle framework upgrades:

**Framework Upgrade Guides**
```python
framework_upgrades = {
    'angular': {
        'upgrade_command': 'ng update',
        'pre_checks': [
            'ng update @angular/core@{version} --dry-run',
            'npm audit',
            'ng lint'
        ],
        'post_upgrade': [
            'ng update @angular/cli',
            'npm run test',
            'npm run e2e'
        ],
        'common_issues': {
            'ivy_renderer': 'Enable Ivy in tsconfig.json',
            'strict_mode': 'Update TypeScript configurations',
            'deprecated_apis': 'Use Angular migration schematics'
        }
    },
    'react': {
        'upgrade_command': 'npm install react@{version} react-dom@{version}',
        'codemods': [
            'npx react-codemod rename-unsafe-lifecycles',
            'npx react-codemod error-boundaries'
        ],
        'verification': [
            'npm run build',
            'npm test -- --coverage',
            'npm run analyze-bundle'
        ]
    },
    'vue': {
        'upgrade_command': 'npm install vue@{version}',
        'migration_tool': 'npx @vue/migration-tool',
        'breaking_changes': {
            '2_to_3': [
                'Composition API',
                'Multiple root elements',
                'Teleport component',
                'Fragments'
            ]
        }
    }
}
```

### 10. Post-Upgrade Monitoring

Monitor application after upgrades:

```javascript
// post-upgrade-monitoring.js
const monitoring = {
    metrics: {
        performance: {
            'page_load_time': { threshold: 3000, unit: 'ms' },
            'api_response_time': { threshold: 500, unit: 'ms' },
            'memory_usage': { threshold: 512, unit: 'MB' }
        },
        errors: {
            'error_rate': { threshold: 0.01, unit: '%' },
            'console_errors': { threshold: 0, unit: 'count' }
        },
        bundle: {
            'size': { threshold: 5, unit: 'MB' },
            'gzip_size': { threshold: 1.5, unit: 'MB' }
        }
    },
    
    checkHealth: async function() {
        const results = {};
        
        for (const [category, metrics] of Object.entries(this.metrics)) {
            results[category] = {};
            
            for (const [metric, config] of Object.entries(metrics)) {
                const value = await this.measureMetric(metric);
                results[category][metric] = {
                    value,
                    threshold: config.threshold,
                    unit: config.unit,
                    status: value <= config.threshold ? 'PASS' : 'FAIL'
                };
            }
        }
        
        return results;
    },
    
    generateReport: function(results) {
        let report = '## Post-Upgrade Health Check\n\n';
        
        for (const [category, metrics] of Object.entries(results)) {
            report += `### ${category}\n\n`;
            report += '| Metric | Value | Threshold | Status |\n';
            report += '|--------|-------|-----------|--------|\n';
            
            for (const [metric, data] of Object.entries(metrics)) {
                const status = data.status === 'PASS' ? '‚úÖ' : '‚ùå';
                report += `| ${metric} | ${data.value}${data.unit} | ${data.threshold}${data.unit} | ${status} |\n`;
            }
            
            report += '\n';
        }
        
        return report;
    }
};
```

## Output Format

1. **Upgrade Overview**: Summary of available updates with risk assessment
2. **Priority Matrix**: Ordered list of updates by importance and safety
3. **Migration Guides**: Step-by-step guides for each major upgrade
4. **Compatibility Report**: Dependency compatibility analysis
5. **Test Strategy**: Automated tests for validating upgrades
6. **Rollback Plan**: Clear procedures for reverting if needed
7. **Monitoring Dashboard**: Post-upgrade health metrics
8. **Timeline**: Realistic schedule for implementing upgrades

Focus on safe, incremental upgrades that maintain system stability while keeping dependencies current and secure.
````

## File: tools/doc-generate.md

````markdown
# Automated Documentation Generation

You are a documentation expert specializing in creating comprehensive, maintainable documentation from code. Generate API docs, architecture diagrams, user guides, and technical references using AI-powered analysis and industry best practices.

## Context
The user needs automated documentation generation that extracts information from code, creates clear explanations, and maintains consistency across documentation types. Focus on creating living documentation that stays synchronized with code.

## Requirements
$ARGUMENTS

## Instructions

### 1. Code Analysis for Documentation

Extract documentation elements from source code:

**API Documentation Extraction**
```python
import ast
import inspect
from typing import Dict, List, Any

class APIDocExtractor:
    def extract_endpoints(self, code_path):
        """
        Extract API endpoints and their documentation
        """
        endpoints = []
        
        # FastAPI example
        fastapi_decorators = ['@app.get', '@app.post', '@app.put', '@app.delete']
        
        with open(code_path, 'r') as f:
            tree = ast.parse(f.read())
            
        for node in ast.walk(tree):
            if isinstance(node, ast.FunctionDef):
                # Check for route decorators
                for decorator in node.decorator_list:
                    if self._is_route_decorator(decorator):
                        endpoint = {
                            'method': self._extract_method(decorator),
                            'path': self._extract_path(decorator),
                            'function': node.name,
                            'docstring': ast.get_docstring(node),
                            'parameters': self._extract_parameters(node),
                            'returns': self._extract_returns(node),
                            'examples': self._extract_examples(node)
                        }
                        endpoints.append(endpoint)
                        
        return endpoints
    
    def _extract_parameters(self, func_node):
        """
        Extract function parameters with types
        """
        params = []
        for arg in func_node.args.args:
            param = {
                'name': arg.arg,
                'type': None,
                'required': True,
                'description': ''
            }
            
            # Extract type annotation
            if arg.annotation:
                param['type'] = ast.unparse(arg.annotation)
                
            params.append(param)
            
        return params
```

**Type and Schema Documentation**
```python
# Extract Pydantic models
def extract_pydantic_schemas(file_path):
    """
    Extract Pydantic model definitions for API documentation
    """
    schemas = []
    
    with open(file_path, 'r') as f:
        tree = ast.parse(f.read())
        
    for node in ast.walk(tree):
        if isinstance(node, ast.ClassDef):
            # Check if inherits from BaseModel
            if any(base.id == 'BaseModel' for base in node.bases if hasattr(base, 'id')):
                schema = {
                    'name': node.name,
                    'description': ast.get_docstring(node),
                    'fields': []
                }
                
                # Extract fields
                for item in node.body:
                    if isinstance(item, ast.AnnAssign):
                        field = {
                            'name': item.target.id,
                            'type': ast.unparse(item.annotation),
                            'required': item.value is None,
                            'default': ast.unparse(item.value) if item.value else None
                        }
                        schema['fields'].append(field)
                        
                schemas.append(schema)
                
    return schemas

# TypeScript interface extraction
function extractTypeScriptInterfaces(code) {
    const interfaces = [];
    const interfaceRegex = /interface\s+(\w+)\s*{([^}]+)}/g;
    
    let match;
    while ((match = interfaceRegex.exec(code)) !== null) {
        const name = match[1];
        const body = match[2];
        
        const fields = [];
        const fieldRegex = /(\w+)(\?)?\s*:\s*([^;]+);/g;
        
        let fieldMatch;
        while ((fieldMatch = fieldRegex.exec(body)) !== null) {
            fields.push({
                name: fieldMatch[1],
                required: !fieldMatch[2],
                type: fieldMatch[3].trim()
            });
        }
        
        interfaces.push({ name, fields });
    }
    
    return interfaces;
}
```

### 2. API Documentation Generation

Create comprehensive API documentation:

**OpenAPI/Swagger Generation**
```yaml
openapi: 3.0.0
info:
  title: ${API_TITLE}
  version: ${VERSION}
  description: |
    ${DESCRIPTION}
    
    ## Authentication
    ${AUTH_DESCRIPTION}
    
    ## Rate Limiting
    ${RATE_LIMIT_INFO}
    
  contact:
    email: ${CONTACT_EMAIL}
  license:
    name: ${LICENSE}
    url: ${LICENSE_URL}

servers:
  - url: https://api.example.com/v1
    description: Production server
  - url: https://staging-api.example.com/v1
    description: Staging server

security:
  - bearerAuth: []
  - apiKey: []

paths:
  /users:
    get:
      summary: List all users
      description: |
        Retrieve a paginated list of users with optional filtering
      operationId: listUsers
      tags:
        - Users
      parameters:
        - name: page
          in: query
          description: Page number for pagination
          required: false
          schema:
            type: integer
            default: 1
            minimum: 1
        - name: limit
          in: query
          description: Number of items per page
          required: false
          schema:
            type: integer
            default: 20
            minimum: 1
            maximum: 100
        - name: search
          in: query
          description: Search term for filtering users
          required: false
          schema:
            type: string
      responses:
        '200':
          description: Successful response
          content:
            application/json:
              schema:
                type: object
                properties:
                  data:
                    type: array
                    items:
                      $ref: '#/components/schemas/User'
                  pagination:
                    $ref: '#/components/schemas/Pagination'
              examples:
                success:
                  value:
                    data:
                      - id: "123"
                        email: "user@example.com"
                        name: "John Doe"
                    pagination:
                      page: 1
                      limit: 20
                      total: 100
        '401':
          $ref: '#/components/responses/Unauthorized'
        '429':
          $ref: '#/components/responses/RateLimitExceeded'

components:
  schemas:
    User:
      type: object
      required:
        - id
        - email
      properties:
        id:
          type: string
          format: uuid
          description: Unique user identifier
        email:
          type: string
          format: email
          description: User's email address
        name:
          type: string
          description: User's full name
        createdAt:
          type: string
          format: date-time
          description: Account creation timestamp
```

**API Client SDK Documentation**
```python
"""
# API Client Documentation

## Installation

```bash
pip install your-api-client
```

## Quick Start

```python
from your_api import Client

# Initialize client
client = Client(api_key="your-api-key")

# List users
users = client.users.list(page=1, limit=20)

# Get specific user
user = client.users.get("user-id")

# Create user
new_user = client.users.create(
    email="user@example.com",
    name="John Doe"
)
```

## Authentication

The client supports multiple authentication methods:

### API Key Authentication

```python
client = Client(api_key="your-api-key")
```

### OAuth2 Authentication

```python
client = Client(
    client_id="your-client-id",
    client_secret="your-client-secret"
)
```

## Error Handling

```python
from your_api.exceptions import APIError, RateLimitError

try:
    user = client.users.get("user-id")
except RateLimitError as e:
    print(f"Rate limit exceeded. Retry after {e.retry_after} seconds")
except APIError as e:
    print(f"API error: {e.message}")
```

## Pagination

```python
# Automatic pagination
for user in client.users.list_all():
    print(user.email)

# Manual pagination
page = 1
while True:
    response = client.users.list(page=page)
    for user in response.data:
        print(user.email)
    
    if not response.has_next:
        break
    page += 1
```
"""
```

### 3. Architecture Documentation

Generate architecture diagrams and documentation:

**System Architecture Diagram (Mermaid)**
```mermaid
graph TB
    subgraph "Frontend"
        UI[React UI]
        Mobile[Mobile App]
    end
    
    subgraph "API Gateway"
        Gateway[Kong/nginx]
        RateLimit[Rate Limiter]
        Auth[Auth Service]
    end
    
    subgraph "Microservices"
        UserService[User Service]
        OrderService[Order Service]
        PaymentService[Payment Service]
        NotificationService[Notification Service]
    end
    
    subgraph "Data Layer"
        PostgresMain[(PostgreSQL)]
        Redis[(Redis Cache)]
        Elasticsearch[(Elasticsearch)]
        S3[S3 Storage]
    end
    
    subgraph "Message Queue"
        Kafka[Apache Kafka]
    end
    
    UI --> Gateway
    Mobile --> Gateway
    Gateway --> Auth
    Gateway --> RateLimit
    Gateway --> UserService
    Gateway --> OrderService
    OrderService --> PaymentService
    PaymentService --> Kafka
    Kafka --> NotificationService
    UserService --> PostgresMain
    UserService --> Redis
    OrderService --> PostgresMain
    OrderService --> Elasticsearch
    NotificationService --> S3
```

**Component Documentation**
```markdown
## System Components

### User Service
**Purpose**: Manages user accounts, authentication, and profiles

**Responsibilities**:
- User registration and authentication
- Profile management
- Role-based access control
- Password reset and account recovery

**Technology Stack**:
- Language: Python 3.11
- Framework: FastAPI
- Database: PostgreSQL
- Cache: Redis
- Authentication: JWT

**API Endpoints**:
- `POST /users` - Create new user
- `GET /users/{id}` - Get user details
- `PUT /users/{id}` - Update user
- `DELETE /users/{id}` - Delete user
- `POST /auth/login` - User login
- `POST /auth/refresh` - Refresh token

**Dependencies**:
- PostgreSQL for user data storage
- Redis for session caching
- Email service for notifications

**Configuration**:
```yaml
user_service:
  port: 8001
  database:
    host: postgres.internal
    port: 5432
    name: users_db
  redis:
    host: redis.internal
    port: 6379
  jwt:
    secret: ${JWT_SECRET}
    expiry: 3600
```
```

### 4. Code Documentation

Generate inline documentation and README files:

**Function Documentation**
```python
def generate_function_docs(func):
    """
    Generate comprehensive documentation for a function
    """
    doc_template = '''
def {name}({params}){return_type}:
    """
    {summary}
    
    {description}
    
    Args:
        {args}
    
    Returns:
        {returns}
    
    Raises:
        {raises}
    
    Examples:
        {examples}
    
    Note:
        {notes}
    """
'''
    
    # Extract function metadata
    sig = inspect.signature(func)
    params = []
    args_doc = []
    
    for param_name, param in sig.parameters.items():
        param_str = param_name
        if param.annotation != param.empty:
            param_str += f": {param.annotation.__name__}"
        if param.default != param.empty:
            param_str += f" = {param.default}"
        params.append(param_str)
        
        # Generate argument documentation
        args_doc.append(f"{param_name} ({param.annotation.__name__}): Description of {param_name}")
    
    return_type = ""
    if sig.return_annotation != sig.empty:
        return_type = f" -> {sig.return_annotation.__name__}"
    
    return doc_template.format(
        name=func.__name__,
        params=", ".join(params),
        return_type=return_type,
        summary=f"Brief description of {func.__name__}",
        description="Detailed explanation of what the function does",
        args="\n        ".join(args_doc),
        returns=f"{sig.return_annotation.__name__}: Description of return value",
        raises="ValueError: If invalid input\n        TypeError: If wrong type",
        examples=f">>> {func.__name__}(param1, param2)\n        expected_output",
        notes="Additional important information"
    )
```

**README Generation**
```markdown
# ${PROJECT_NAME}

${BADGES}

${SHORT_DESCRIPTION}

## Table of Contents

- [Features](#features)
- [Installation](#installation)
- [Quick Start](#quick-start)
- [Documentation](#documentation)
- [API Reference](#api-reference)
- [Configuration](#configuration)
- [Development](#development)
- [Testing](#testing)
- [Deployment](#deployment)
- [Contributing](#contributing)
- [License](#license)

## Features

${FEATURES_LIST}

## Installation

### Prerequisites

- Python 3.8+
- PostgreSQL 12+
- Redis 6+

### Using pip

```bash
pip install ${PACKAGE_NAME}
```

### Using Docker

```bash
docker pull ${DOCKER_IMAGE}
docker run -p 8000:8000 ${DOCKER_IMAGE}
```

### From source

```bash
git clone https://github.com/${GITHUB_ORG}/${REPO_NAME}.git
cd ${REPO_NAME}
pip install -e .
```

## Quick Start

```python
${QUICK_START_CODE}
```

## Documentation

Full documentation is available at [https://docs.example.com](https://docs.example.com)

### API Reference

- [REST API Documentation](./docs/api/README.md)
- [Python SDK Reference](./docs/sdk/python.md)
- [JavaScript SDK Reference](./docs/sdk/javascript.md)

## Configuration

### Environment Variables

| Variable | Description | Default | Required |
|----------|-------------|---------|----------|
| DATABASE_URL | PostgreSQL connection string | - | Yes |
| REDIS_URL | Redis connection string | - | Yes |
| SECRET_KEY | Application secret key | - | Yes |
| DEBUG | Enable debug mode | false | No |

### Configuration File

```yaml
${CONFIG_EXAMPLE}
```

## Development

### Setting up the development environment

```bash
# Clone repository
git clone https://github.com/${GITHUB_ORG}/${REPO_NAME}.git
cd ${REPO_NAME}

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements-dev.txt

# Run tests
pytest

# Start development server
python manage.py runserver
```

### Code Style

We use [Black](https://github.com/psf/black) for code formatting and [Flake8](https://flake8.pycqa.org/) for linting.

```bash
# Format code
black .

# Run linter
flake8 .
```

## Testing

```bash
# Run all tests
pytest

# Run with coverage
pytest --cov=your_package

# Run specific test file
pytest tests/test_users.py

# Run integration tests
pytest tests/integration/
```

## Deployment

### Docker

```dockerfile
${DOCKERFILE_EXAMPLE}
```

### Kubernetes

```yaml
${K8S_DEPLOYMENT_EXAMPLE}
```

## Contributing

Please read [CONTRIBUTING.md](CONTRIBUTING.md) for details on our code of conduct and the process for submitting pull requests.

### Development Workflow

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## License

This project is licensed under the ${LICENSE} License - see the [LICENSE](LICENSE) file for details.

## Acknowledgments

${ACKNOWLEDGMENTS}
```

### 5. User Documentation

Generate end-user documentation:

**User Guide Template**
```markdown
# User Guide

## Getting Started

### Creating Your First ${FEATURE}

1. **Navigate to the Dashboard**
   
   Click on the ${FEATURE} tab in the main navigation menu.
   
   ![Dashboard Screenshot](./images/dashboard.png)

2. **Click "Create New"**
   
   You'll find the "Create New" button in the top right corner.
   
   ![Create Button](./images/create-button.png)

3. **Fill in the Details**
   
   - **Name**: Enter a descriptive name
   - **Description**: Add optional details
   - **Settings**: Configure as needed
   
   ![Form Screenshot](./images/form.png)

4. **Save Your Changes**
   
   Click "Save" to create your ${FEATURE}.

### Common Tasks

#### Editing ${FEATURE}

1. Find your ${FEATURE} in the list
2. Click the "Edit" button
3. Make your changes
4. Click "Save"

#### Deleting ${FEATURE}

> ‚ö†Ô∏è **Warning**: Deletion is permanent and cannot be undone.

1. Find your ${FEATURE} in the list
2. Click the "Delete" button
3. Confirm the deletion

### Troubleshooting

#### ${FEATURE} Not Appearing

**Problem**: Created ${FEATURE} doesn't show in the list

**Solution**: 
1. Check filters - ensure "All" is selected
2. Refresh the page
3. Check permissions with your administrator

#### Error Messages

| Error | Meaning | Solution |
|-------|---------|----------|
| "Name required" | The name field is empty | Enter a name |
| "Permission denied" | You don't have access | Contact admin |
| "Server error" | Technical issue | Try again later |
```

### 6. Interactive Documentation

Generate interactive documentation elements:

**API Playground**
```html
<!DOCTYPE html>
<html>
<head>
    <title>API Documentation</title>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/swagger-ui-dist@latest/swagger-ui.css">
</head>
<body>
    <div id="swagger-ui"></div>
    
    <script src="https://cdn.jsdelivr.net/npm/swagger-ui-dist@latest/swagger-ui-bundle.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/swagger-ui-dist@latest/swagger-ui-standalone-preset.js"></script>
    <script>
        window.onload = function() {
            const ui = SwaggerUIBundle({
                url: "/api/openapi.json",
                dom_id: '#swagger-ui',
                deepLinking: true,
                presets: [
                    SwaggerUIBundle.presets.apis,
                    SwaggerUIStandalonePreset
                ],
                plugins: [
                    SwaggerUIBundle.plugins.DownloadUrl
                ],
                layout: "StandaloneLayout",
                onComplete: function() {
                    // Add try it out functionality
                    ui.preauthorizeApiKey("apiKey", "your-api-key");
                }
            });
            window.ui = ui;
        }
    </script>
</body>
</html>
```

**Code Examples Generator**
```python
def generate_code_examples(endpoint, languages=['python', 'javascript', 'curl']):
    """
    Generate code examples for API endpoints
    """
    examples = {}
    
    # Python example
    examples['python'] = f'''
import requests

url = "https://api.example.com{endpoint['path']}"
headers = {{
    "Authorization": "Bearer YOUR_API_KEY",
    "Content-Type": "application/json"
}}

response = requests.{endpoint['method'].lower()}(url, headers=headers)
print(response.json())
'''
    
    # JavaScript example
    examples['javascript'] = f'''
const response = await fetch('https://api.example.com{endpoint['path']}', {{
    method: '{endpoint['method']}',
    headers: {{
        'Authorization': 'Bearer YOUR_API_KEY',
        'Content-Type': 'application/json'
    }}
}});

const data = await response.json();
console.log(data);
'''
    
    # cURL example
    examples['curl'] = f'''
curl -X {endpoint['method']} https://api.example.com{endpoint['path']} \\
    -H "Authorization: Bearer YOUR_API_KEY" \\
    -H "Content-Type: application/json"
'''
    
    return examples
```

### 7. Documentation CI/CD

Automate documentation updates:

**GitHub Actions Workflow**
```yaml
name: Generate Documentation

on:
  push:
    branches: [main]
    paths:
      - 'src/**'
      - 'api/**'
  workflow_dispatch:

jobs:
  generate-docs:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Install dependencies
      run: |
        pip install -r requirements-docs.txt
        npm install -g @redocly/cli
    
    - name: Generate API documentation
      run: |
        python scripts/generate_openapi.py > docs/api/openapi.json
        redocly build-docs docs/api/openapi.json -o docs/api/index.html
    
    - name: Generate code documentation
      run: |
        sphinx-build -b html docs/source docs/build
    
    - name: Generate architecture diagrams
      run: |
        python scripts/generate_diagrams.py
        
    - name: Deploy to GitHub Pages
      uses: peaceiris/actions-gh-pages@v3
      with:
        github_token: ${{ secrets.GITHUB_TOKEN }}
        publish_dir: ./docs/build
```

### 8. Documentation Quality Checks

Ensure documentation completeness:

**Documentation Coverage**
```python
class DocCoverage:
    def check_coverage(self, codebase_path):
        """
        Check documentation coverage for codebase
        """
        results = {
            'total_functions': 0,
            'documented_functions': 0,
            'total_classes': 0,
            'documented_classes': 0,
            'total_modules': 0,
            'documented_modules': 0,
            'missing_docs': []
        }
        
        for file_path in glob.glob(f"{codebase_path}/**/*.py", recursive=True):
            module = ast.parse(open(file_path).read())
            
            # Check module docstring
            if ast.get_docstring(module):
                results['documented_modules'] += 1
            else:
                results['missing_docs'].append({
                    'type': 'module',
                    'file': file_path
                })
            results['total_modules'] += 1
            
            # Check functions and classes
            for node in ast.walk(module):
                if isinstance(node, ast.FunctionDef):
                    results['total_functions'] += 1
                    if ast.get_docstring(node):
                        results['documented_functions'] += 1
                    else:
                        results['missing_docs'].append({
                            'type': 'function',
                            'name': node.name,
                            'file': file_path,
                            'line': node.lineno
                        })
                        
                elif isinstance(node, ast.ClassDef):
                    results['total_classes'] += 1
                    if ast.get_docstring(node):
                        results['documented_classes'] += 1
                    else:
                        results['missing_docs'].append({
                            'type': 'class',
                            'name': node.name,
                            'file': file_path,
                            'line': node.lineno
                        })
        
        # Calculate coverage
        results['function_coverage'] = (
            results['documented_functions'] / results['total_functions'] * 100
            if results['total_functions'] > 0 else 100
        )
        results['class_coverage'] = (
            results['documented_classes'] / results['total_classes'] * 100
            if results['total_classes'] > 0 else 100
        )
        
        return results
```

## Output Format

1. **API Documentation**: OpenAPI spec with interactive playground
2. **Architecture Diagrams**: System, sequence, and component diagrams
3. **Code Documentation**: Inline docs, docstrings, and type hints
4. **User Guides**: Step-by-step tutorials with screenshots
5. **Developer Guides**: Setup, contribution, and API usage guides
6. **Reference Documentation**: Complete API reference with examples
7. **Documentation Site**: Deployed static site with search functionality

Focus on creating documentation that is accurate, comprehensive, and easy to maintain alongside code changes.
````

## File: tools/docker-optimize.md

````markdown
# Docker Optimization

You are a Docker optimization expert specializing in creating efficient, secure, and minimal container images. Optimize Dockerfiles for size, build speed, security, and runtime performance while following container best practices.

## Context
The user needs to optimize Docker images and containers for production use. Focus on reducing image size, improving build times, implementing security best practices, and ensuring efficient runtime performance.

## Requirements
$ARGUMENTS

## Instructions

### 1. Container Optimization Strategy Selection

Choose the right optimization approach based on your application type and requirements:

**Optimization Strategy Matrix**
```python
from typing import Dict, List, Any, Optional
from dataclasses import dataclass
from pathlib import Path
import docker
import json
import subprocess
import tempfile

@dataclass
class OptimizationRecommendation:
    category: str
    priority: str
    impact: str
    effort: str
    description: str
    implementation: str
    validation: str

class SmartDockerOptimizer:
    def __init__(self):
        self.client = docker.from_env()
        self.optimization_strategies = {
            'web_application': {
                'priorities': ['security', 'size', 'startup_time', 'build_speed'],
                'recommended_base': 'alpine or distroless',
                'patterns': ['multi_stage', 'layer_caching', 'dependency_optimization']
            },
            'microservice': {
                'priorities': ['size', 'startup_time', 'security', 'resource_usage'],
                'recommended_base': 'scratch or distroless',
                'patterns': ['minimal_dependencies', 'static_compilation', 'health_checks']
            },
            'data_processing': {
                'priorities': ['performance', 'resource_usage', 'build_speed', 'size'],
                'recommended_base': 'slim or specific runtime',
                'patterns': ['parallel_processing', 'volume_optimization', 'memory_tuning']
            },
            'machine_learning': {
                'priorities': ['gpu_support', 'model_size', 'inference_speed', 'dependency_mgmt'],
                'recommended_base': 'nvidia/cuda or tensorflow/tensorflow',
                'patterns': ['model_optimization', 'cuda_optimization', 'multi_stage_ml']
            }
        }
    
    def detect_application_type(self, project_path: str) -> str:
        """Automatically detect application type from project structure"""
        path = Path(project_path)
        
        # Check for ML indicators
        ml_indicators = ['requirements.txt', 'environment.yml', 'model.pkl', 'model.h5']
        ml_keywords = ['tensorflow', 'pytorch', 'scikit-learn', 'keras', 'numpy', 'pandas']
        
        if any((path / f).exists() for f in ml_indicators):
            if (path / 'requirements.txt').exists():
                with open(path / 'requirements.txt') as f:
                    content = f.read().lower()
                    if any(keyword in content for keyword in ml_keywords):
                        return 'machine_learning'
        
        # Check for microservice indicators
        if any(f.name in ['go.mod', 'main.go', 'cmd'] for f in path.iterdir()):
            return 'microservice'
        
        # Check for data processing
        data_indicators = ['airflow', 'kafka', 'spark', 'hadoop']
        if any((path / f).exists() for f in ['docker-compose.yml', 'k8s']):
            return 'data_processing'
        
        # Default to web application
        return 'web_application'
    
    def analyze_dockerfile_comprehensively(self, dockerfile_path: str, project_path: str) -> Dict[str, Any]:
        """
        Comprehensive Dockerfile analysis with modern optimization recommendations
        """
        app_type = self.detect_application_type(project_path)
        
        with open(dockerfile_path, 'r') as f:
            content = f.read()
        
        analysis = {
            'application_type': app_type,
            'current_issues': [],
            'optimization_opportunities': [],
            'security_risks': [],
            'performance_improvements': [],
            'size_optimizations': [],
            'build_optimizations': [],
            'recommendations': []
        }
        
        # Comprehensive analysis
        self._analyze_base_image_strategy(content, analysis)
        self._analyze_layer_efficiency(content, analysis)
        self._analyze_security_posture(content, analysis)
        self._analyze_build_performance(content, analysis)
        self._analyze_runtime_optimization(content, analysis)
        self._generate_strategic_recommendations(analysis, app_type)
        
        return analysis
    
    def _analyze_base_image_strategy(self, content: str, analysis: Dict):
        """Analyze base image selection and optimization opportunities"""
        base_image_patterns = {
            'outdated_versions': {
                'pattern': r'FROM\s+([^:]+):(?!latest)([0-9]+\.[0-9]+)(?:\s|$)',
                'severity': 'medium',
                'recommendation': 'Consider updating to latest stable version'
            },
            'latest_tag': {
                'pattern': r'FROM\s+([^:]+):latest',
                'severity': 'high',
                'recommendation': 'Pin to specific version for reproducible builds'
            },
            'large_base_images': {
                'patterns': [
                    r'FROM\s+ubuntu(?!.*slim)',
                    r'FROM\s+centos',
                    r'FROM\s+debian(?!.*slim)',
                    r'FROM\s+node(?!.*alpine)'
                ],
                'severity': 'medium',
                'recommendation': 'Consider using smaller alternatives (alpine, slim, distroless)'
            },
            'missing_multi_stage': {
                'pattern': r'FROM\s+(?!.*AS\s+)',
                'count_threshold': 1,
                'severity': 'low',
                'recommendation': 'Consider multi-stage builds for smaller final images'
            }
        }
        
        # Check for base image optimization opportunities
        for issue_type, config in base_image_patterns.items():
            if 'patterns' in config:
                for pattern in config['patterns']:
                    if re.search(pattern, content, re.IGNORECASE):
                        analysis['size_optimizations'].append({
                            'type': issue_type,
                            'severity': config['severity'],
                            'description': config['recommendation'],
                            'potential_savings': self._estimate_size_savings(issue_type)
                        })
            elif 'pattern' in config:
                matches = re.findall(config['pattern'], content, re.IGNORECASE)
                if matches:
                    analysis['current_issues'].append({
                        'type': issue_type,
                        'severity': config['severity'],
                        'instances': len(matches),
                        'description': config['recommendation']
                    })
    
    def _analyze_layer_efficiency(self, content: str, analysis: Dict):
        """Analyze Docker layer efficiency and caching opportunities"""
        lines = content.split('\n')
        run_commands = [line for line in lines if line.strip().startswith('RUN')]
        
        # Multiple RUN commands analysis
        if len(run_commands) > 3:
            analysis['build_optimizations'].append({
                'type': 'excessive_layers',
                'severity': 'medium',
                'current_count': len(run_commands),
                'recommended_count': '1-3',
                'description': f'Found {len(run_commands)} RUN commands. Consider combining related operations.',
                'implementation': 'Combine RUN commands with && to reduce layers'
            })
        
        # Package manager cleanup analysis
        package_managers = {
            'apt': {'install': r'apt-get\s+install', 'cleanup': r'rm\s+-rf\s+/var/lib/apt/lists'},
            'yum': {'install': r'yum\s+install', 'cleanup': r'yum\s+clean\s+all'},
            'apk': {'install': r'apk\s+add', 'cleanup': r'rm\s+-rf\s+/var/cache/apk'}
        }
        
        for pm_name, patterns in package_managers.items():
            if re.search(patterns['install'], content) and not re.search(patterns['cleanup'], content):
                analysis['size_optimizations'].append({
                    'type': f'{pm_name}_cleanup_missing',
                    'severity': 'medium',
                    'description': f'Missing {pm_name} cache cleanup',
                    'potential_savings': '50-200MB',
                    'implementation': f'Add cleanup command in same RUN layer'
                })
        
        # Copy optimization analysis
        copy_commands = [line for line in lines if line.strip().startswith(('COPY', 'ADD'))]
        if any('.' in cmd for cmd in copy_commands):
            analysis['build_optimizations'].append({
                'type': 'inefficient_copy',
                'severity': 'low',
                'description': 'Consider using .dockerignore and specific COPY commands',
                'implementation': 'Copy only necessary files to improve build cache efficiency'
            })
    
    def _generate_strategic_recommendations(self, analysis: Dict, app_type: str):
        """Generate strategic optimization recommendations based on application type"""
        strategy = self.optimization_strategies[app_type]
        
        # Priority-based recommendations
        for priority in strategy['priorities']:
            if priority == 'security':
                analysis['recommendations'].append(OptimizationRecommendation(
                    category='Security',
                    priority='High',
                    impact='Critical',
                    effort='Medium',
                    description='Implement security scanning and hardening',
                    implementation=self._get_security_implementation(app_type),
                    validation='Run Trivy and Hadolint scans'
                ))
            elif priority == 'size':
                analysis['recommendations'].append(OptimizationRecommendation(
                    category='Size Optimization',
                    priority='High',
                    impact='High',
                    effort='Low',
                    description=f'Use {strategy["recommended_base"]} base image',
                    implementation=self._get_size_implementation(app_type),
                    validation='Compare image sizes before/after'
                ))
            elif priority == 'startup_time':
                analysis['recommendations'].append(OptimizationRecommendation(
                    category='Startup Performance',
                    priority='Medium',
                    impact='High',
                    effort='Medium',
                    description='Optimize application startup time',
                    implementation=self._get_startup_implementation(app_type),
                    validation='Measure container startup time'
                ))
    
    def _estimate_size_savings(self, optimization_type: str) -> str:
        """Estimate potential size savings for optimization"""
        savings_map = {
            'large_base_images': '200-800MB',
            'apt_cleanup_missing': '50-200MB',
            'yum_cleanup_missing': '100-300MB',
            'apk_cleanup_missing': '20-100MB',
            'excessive_layers': '10-50MB',
            'multi_stage_optimization': '100-500MB'
        }
        return savings_map.get(optimization_type, '10-50MB')
    
    def _get_security_implementation(self, app_type: str) -> str:
        """Get security implementation based on app type"""
        implementations = {
            'web_application': 'Non-root user, security scanning, minimal packages',
            'microservice': 'Distroless base, static compilation, capability dropping',
            'data_processing': 'Secure data handling, encrypted volumes, network policies',
            'machine_learning': 'Model encryption, secure model serving, GPU security'
        }
        return implementations.get(app_type, 'Standard security hardening')
```

**Advanced Multi-Framework Dockerfile Generator**
```python
class FrameworkOptimizedDockerfileGenerator:
    def __init__(self):
        self.templates = {
            'node_express': self._generate_node_express_optimized,
            'python_fastapi': self._generate_python_fastapi_optimized,
            'python_django': self._generate_python_django_optimized,
            'golang_gin': self._generate_golang_optimized,
            'java_spring': self._generate_java_spring_optimized,
            'rust_actix': self._generate_rust_optimized,
            'dotnet_core': self._generate_dotnet_optimized
        }
    
    def generate_optimized_dockerfile(self, framework: str, config: Dict[str, Any]) -> str:
        """Generate highly optimized Dockerfile for specific framework"""
        if framework not in self.templates:
            raise ValueError(f"Unsupported framework: {framework}")
        
        return self.templates[framework](config)
    
    def _generate_node_express_optimized(self, config: Dict) -> str:
        """Generate optimized Node.js Express Dockerfile"""
        node_version = config.get('node_version', '20')
        use_bun = config.get('use_bun', False)
        
        if use_bun:
            return f"""
# Optimized Node.js with Bun - Ultra-fast builds and runtime
FROM oven/bun:{config.get('bun_version', 'latest')} AS base

# Install dependencies (bun is much faster than npm)
WORKDIR /app
COPY package.json bun.lockb* ./
RUN bun install --frozen-lockfile --production

# Build stage
FROM base AS build
COPY . .
RUN bun run build

# Production stage
FROM gcr.io/distroless/nodejs{node_version}-debian11
WORKDIR /app

# Copy built application
COPY --from=build --chown=nonroot:nonroot /app/dist ./dist
COPY --from=build --chown=nonroot:nonroot /app/node_modules ./node_modules
COPY --from=build --chown=nonroot:nonroot /app/package.json ./

# Security: Run as non-root
USER nonroot

# Health check
HEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \\
    CMD ["node", "-e", "require('http').get('http://localhost:3000/health', (res) => process.exit(res.statusCode === 200 ? 0 : 1)).on('error', () => process.exit(1))"]

EXPOSE 3000
CMD ["node", "dist/index.js"]
"""
        
        return f"""
# Optimized Node.js Express - Production-ready multi-stage build
FROM node:{node_version}-alpine AS deps

# Install dumb-init for proper signal handling
RUN apk add --no-cache dumb-init

# Create app directory with proper permissions
RUN addgroup -g 1001 -S nodejs && adduser -S nodejs -u 1001
WORKDIR /app
USER nodejs

# Copy package files and install dependencies
COPY --chown=nodejs:nodejs package*.json ./
RUN npm ci --only=production --no-audit --no-fund && npm cache clean --force

# Build stage
FROM node:{node_version}-alpine AS build
WORKDIR /app
COPY package*.json ./
RUN npm ci --no-audit --no-fund
COPY . .
RUN npm run build && npm run test

# Production stage
FROM node:{node_version}-alpine AS production

# Install dumb-init
RUN apk add --no-cache dumb-init

# Create user and app directory
RUN addgroup -g 1001 -S nodejs && adduser -S nodejs -u 1001
WORKDIR /app
USER nodejs

# Copy built application
COPY --from=build --chown=nodejs:nodejs /app/dist ./dist
COPY --from=deps --chown=nodejs:nodejs /app/node_modules ./node_modules
COPY --from=build --chown=nodejs:nodejs /app/package.json ./

# Health check
HEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \\
    CMD node healthcheck.js || exit 1

# Expose port
EXPOSE 3000

# Use dumb-init for proper signal handling
ENTRYPOINT ["dumb-init", "--"]
CMD ["node", "dist/index.js"]
"""
    
    def _generate_python_fastapi_optimized(self, config: Dict) -> str:
        """Generate optimized Python FastAPI Dockerfile"""
        python_version = config.get('python_version', '3.11')
        use_uv = config.get('use_uv', True)
        
        if use_uv:
            return f"""
# Ultra-fast Python with uv package manager
FROM python:{python_version}-slim AS base

# Install uv - the fastest Python package manager
RUN pip install uv

# Build dependencies
FROM base AS build
WORKDIR /app

# Copy requirements and install dependencies with uv
COPY requirements.txt ./
RUN uv venv /opt/venv && \\
    . /opt/venv/bin/activate && \\
    uv pip install --no-cache-dir -r requirements.txt

# Production stage
FROM python:{python_version}-slim AS production

# Install security updates
RUN apt-get update && apt-get upgrade -y && \\
    apt-get install -y --no-install-recommends dumb-init && \\
    rm -rf /var/lib/apt/lists/*

# Create non-root user
RUN useradd -m -u 1001 appuser
WORKDIR /app

# Copy virtual environment
COPY --from=build /opt/venv /opt/venv
ENV PATH="/opt/venv/bin:$PATH"

# Copy application
COPY --chown=appuser:appuser . .

# Security: Run as non-root
USER appuser

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \\
    CMD python -c "import requests; requests.get('http://localhost:8000/health', timeout=5)"

EXPOSE 8000

# Use dumb-init and Gunicorn for production
ENTRYPOINT ["dumb-init", "--"]
CMD ["gunicorn", "--bind", "0.0.0.0:8000", "--workers", "4", "--worker-class", "uvicorn.workers.UvicornWorker", "app.main:app"]
"""
        
        # Standard optimized Python Dockerfile
        return f"""
# Optimized Python FastAPI - Production-ready
FROM python:{python_version}-slim AS build

# Install build dependencies
RUN apt-get update && apt-get install -y --no-install-recommends \\
    build-essential \\
    && rm -rf /var/lib/apt/lists/*

# Create virtual environment
RUN python -m venv /opt/venv
ENV PATH="/opt/venv/bin:$PATH"

# Install Python dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir --upgrade pip && \\
    pip install --no-cache-dir -r requirements.txt

# Production stage
FROM python:{python_version}-slim AS production

# Install runtime dependencies and security updates
RUN apt-get update && apt-get install -y --no-install-recommends \\
    dumb-init \\
    && apt-get upgrade -y \\
    && rm -rf /var/lib/apt/lists/*

# Create non-root user
RUN useradd -m -u 1001 appuser
WORKDIR /app

# Copy virtual environment
COPY --from=build /opt/venv /opt/venv
ENV PATH="/opt/venv/bin:$PATH" \\
    PYTHONUNBUFFERED=1 \\
    PYTHONDONTWRITEBYTECODE=1 \\
    PYTHONOPTIMIZE=2

# Copy application
COPY --chown=appuser:appuser . .

# Security: Run as non-root
USER appuser

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \\
    CMD python -c "import urllib.request; urllib.request.urlopen('http://localhost:8000/health', timeout=5)"

EXPOSE 8000

# Production server with proper signal handling
ENTRYPOINT ["dumb-init", "--"]
CMD ["gunicorn", "--bind", "0.0.0.0:8000", "--workers", "4", "--worker-class", "uvicorn.workers.UvicornWorker", "app.main:app"]
"""
    
    def _generate_golang_optimized(self, config: Dict) -> str:
        """Generate optimized Go Dockerfile with minimal final image"""
        go_version = config.get('go_version', '1.21')
        
        return f"""
# Optimized Go build - Ultra-minimal final image
FROM golang:{go_version}-alpine AS build

# Install git for go modules
RUN apk add --no-cache git ca-certificates tzdata

# Create build directory
WORKDIR /build

# Copy go mod files and download dependencies
COPY go.mod go.sum ./
RUN go mod download && go mod verify

# Copy source code
COPY . .

# Build static binary with optimizations
RUN CGO_ENABLED=0 GOOS=linux GOARCH=amd64 go build \\
    -ldflags='-w -s -extldflags "-static"' \\
    -a -installsuffix cgo \\
    -o app .

# Final stage - minimal scratch image
FROM scratch

# Copy necessary files from build stage
COPY --from=build /etc/ssl/certs/ca-certificates.crt /etc/ssl/certs/
COPY --from=build /usr/share/zoneinfo /usr/share/zoneinfo
COPY --from=build /build/app /app

# Health check (using the app itself)
HEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \\
    CMD ["/app", "--health-check"]

EXPOSE 8080

# Run the binary
ENTRYPOINT ["/app"]
"""
    
    def _check_base_image(self, content, analysis):
        """Enhanced base image optimization analysis"""
        from_match = re.search(r'^FROM\s+(.+?)(?:\s+AS\s+\w+)?$', content, re.MULTILINE)
        if from_match:
            base_image = from_match.group(1)
            
            # Check for latest tag
            if ':latest' in base_image or not ':' in base_image:
                analysis['security_risks'].append({
                    'issue': 'Using latest or no tag',
                    'severity': 'HIGH',
                    'fix': 'Pin to specific version',
                    'example': f'FROM {base_image.split(":")[0]}:1.2.3',
                    'impact': 'Unpredictable builds, security vulnerabilities'
                })
            
            # Enhanced base image recommendations
            optimization_recommendations = {
                'ubuntu': {
                    'alternatives': ['ubuntu:22.04-slim', 'debian:bullseye-slim', 'alpine:3.18'],
                    'savings': '400-600MB',
                    'notes': 'Consider distroless for production'
                },
                'debian': {
                    'alternatives': ['debian:bullseye-slim', 'alpine:3.18', 'gcr.io/distroless/base'],
                    'savings': '300-500MB',
                    'notes': 'Distroless provides better security'
                },
                'centos': {
                    'alternatives': ['alpine:3.18', 'gcr.io/distroless/base', 'ubuntu:22.04-slim'],
                    'savings': '200-400MB',
                    'notes': 'CentOS is deprecated, migrate to alternatives'
                },
                'node': {
                    'alternatives': ['node:20-alpine', 'node:20-slim', 'gcr.io/distroless/nodejs20'],
                    'savings': '300-700MB',
                    'notes': 'Alpine is smallest, distroless is most secure'
                },
                'python': {
                    'alternatives': ['python:3.11-slim', 'python:3.11-alpine', 'gcr.io/distroless/python3'],
                    'savings': '400-800MB',
                    'notes': 'Slim balances size and compatibility'
                }
            }
            
            for base_name, config in optimization_recommendations.items():
                if base_name in base_image and 'slim' not in base_image and 'alpine' not in base_image:
                    analysis['size_impact'].append({
                        'issue': f'Large base image: {base_image}',
                        'impact': config['savings'],
                        'alternatives': config['alternatives'],
                        'recommendation': f"Switch to {config['alternatives'][0]} for optimal size/compatibility balance",
                        'notes': config['notes']
                    })
            
            # Check for deprecated or insecure base images
            deprecated_images = {
                'centos:7': 'EOL reached, migrate to Rocky Linux or Alpine',
                'ubuntu:18.04': 'LTS ended, upgrade to ubuntu:22.04',
                'node:14': 'Node 14 is EOL, upgrade to node:18 or node:20',
                'python:3.8': 'Python 3.8 will reach EOL soon, upgrade to 3.11+'
            }
            
            for deprecated, message in deprecated_images.items():
                if deprecated in base_image:
                    analysis['security_risks'].append({
                        'issue': f'Deprecated base image: {deprecated}',
                        'severity': 'MEDIUM',
                        'fix': message,
                        'impact': 'Security vulnerabilities, no security updates'
                    })
    
    def _check_layer_optimization(self, content, analysis):
        """Enhanced layer optimization analysis with modern best practices"""
        lines = content.split('\n')
        
        # Check for multiple RUN commands
        run_commands = [line for line in lines if line.strip().startswith('RUN')]
        if len(run_commands) > 5:
            analysis['build_performance'].append({
                'issue': f'Excessive RUN commands ({len(run_commands)})',
                'impact': f'Creates {len(run_commands)} unnecessary layers',
                'fix': 'Combine related RUN commands with && \\',
                'optimization': f'Could reduce to 2-3 layers, saving ~{len(run_commands) * 10}MB'
            })
        
        # Enhanced package manager cleanup checks
        package_managers = {
            'apt': {
                'install_pattern': r'RUN.*apt-get.*install',
                'cleanup_pattern': r'rm\s+-rf\s+/var/lib/apt/lists',
                'update_pattern': r'apt-get\s+update',
                'combined_check': r'RUN.*apt-get\s+update.*&&.*apt-get\s+install.*&&.*rm\s+-rf\s+/var/lib/apt/lists',
                'recommended_pattern': 'RUN apt-get update && apt-get install -y --no-install-recommends <packages> && rm -rf /var/lib/apt/lists/*'
            },
            'yum': {
                'install_pattern': r'RUN.*yum.*install',
                'cleanup_pattern': r'yum\s+clean\s+all',
                'recommended_pattern': 'RUN yum install -y <packages> && yum clean all'
            },
            'apk': {
                'install_pattern': r'RUN.*apk.*add',
                'cleanup_pattern': r'--no-cache|rm\s+-rf\s+/var/cache/apk',
                'recommended_pattern': 'RUN apk add --no-cache <packages>'
            },
            'pip': {
                'install_pattern': r'RUN.*pip.*install',
                'cleanup_pattern': r'--no-cache-dir|pip\s+cache\s+purge',
                'recommended_pattern': 'RUN pip install --no-cache-dir <packages>'
            }
        }
        
        for pm_name, patterns in package_managers.items():
            has_install = re.search(patterns['install_pattern'], content)
            has_cleanup = re.search(patterns['cleanup_pattern'], content)
            
            if has_install and not has_cleanup:
                potential_savings = {
                    'apt': '50-200MB',
                    'yum': '100-300MB', 
                    'apk': '5-50MB',
                    'pip': '20-100MB'
                }.get(pm_name, '10-50MB')
                
                analysis['size_impact'].append({
                    'issue': f'{pm_name} package manager without cleanup',
                    'impact': potential_savings,
                    'fix': f'Add cleanup in same RUN command',
                    'example': patterns['recommended_pattern'],
                    'severity': 'MEDIUM'
                })
        
        # Check for inefficient COPY operations
        copy_commands = [line for line in lines if line.strip().startswith(('COPY', 'ADD'))]
        for cmd in copy_commands:
            if 'COPY . .' in cmd or 'COPY ./ ./' in cmd:
                analysis['build_performance'].append({
                    'issue': 'Inefficient COPY command copying entire context',
                    'impact': 'Poor build cache efficiency, slower builds',
                    'fix': 'Use specific COPY commands and .dockerignore',
                    'example': 'COPY package*.json ./ && COPY src/ ./src/',
                    'note': 'Copy dependency files first for better caching'
                })
        
        # Check for BuildKit optimizations
        if '--mount=type=cache' not in content:
            analysis['build_performance'].append({
                'issue': 'Missing BuildKit cache mounts',
                'impact': 'Slower builds, no dependency caching',
                'fix': 'Use BuildKit cache mounts for package managers',
                'example': 'RUN --mount=type=cache,target=/root/.cache/pip pip install -r requirements.txt',
                'note': 'Requires DOCKER_BUILDKIT=1'
            })
        
        # Check for multi-stage build opportunities
        from_statements = re.findall(r'FROM\s+([^\s]+)', content)
        if len(from_statements) == 1 and any(keyword in content.lower() for keyword in ['build', 'compile', 'npm install', 'pip install']):
            analysis['size_impact'].append({
                'issue': 'Single-stage build with development dependencies',
                'impact': '100-500MB from build tools and dev dependencies',
                'fix': 'Implement multi-stage build',
                'example': 'Separate build and runtime stages',
                'potential_savings': '200-800MB'
            })
```

### 2. Advanced Multi-Stage Build Strategies

Implement sophisticated multi-stage builds with modern optimization techniques:

**Ultra-Optimized Multi-Stage Patterns**
```dockerfile
# Pattern 1: Node.js with Bun - Next-generation JavaScript runtime
# 5x faster installs, 4x faster runtime, 90% smaller images
FROM oven/bun:1.0-alpine AS base

# Stage 1: Dependency Resolution with Bun
FROM base AS deps
WORKDIR /app

# Bun lockfile for deterministic builds
COPY package.json bun.lockb* ./

# Ultra-fast dependency installation
RUN bun install --frozen-lockfile --production

# Stage 2: Build with development dependencies
FROM base AS build
WORKDIR /app

# Copy package files
COPY package.json bun.lockb* ./

# Install all dependencies (including dev)
RUN bun install --frozen-lockfile

# Copy source and build
COPY . .
RUN bun run build && bun test

# Stage 3: Security scanning (optional but recommended)
FROM build AS security-scan
RUN apk add --no-cache curl
# Download and run Trivy for vulnerability scanning
RUN curl -sfL https://raw.githubusercontent.com/aquasecurity/trivy/main/contrib/install.sh | sh -s -- -b /usr/local/bin && \
    trivy fs --exit-code 1 --no-progress --severity HIGH,CRITICAL /app

# Stage 4: Ultra-minimal production with distroless
FROM gcr.io/distroless/nodejs20-debian11 AS production

# Copy only what's needed for production
COPY --from=deps --chown=nonroot:nonroot /app/node_modules ./node_modules
COPY --from=build --chown=nonroot:nonroot /app/dist ./dist
COPY --from=build --chown=nonroot:nonroot /app/package.json ./

# Distroless already runs as non-root
USER nonroot

# Health check using Node.js built-in capabilities
HEALTHCHECK --interval=30s --timeout=5s --start-period=10s --retries=3 \
    CMD ["node", "-e", "require('http').get('http://localhost:3000/health',(r)=>process.exit(r.statusCode===200?0:1)).on('error',()=>process.exit(1))"]

EXPOSE 3000
CMD ["node", "dist/index.js"]
```

**Advanced Python Multi-Stage with UV Package Manager**
```dockerfile
# Pattern 2: Python with UV - 10-100x faster than pip
FROM python:3.11-slim AS base

# Install UV - next generation Python package manager
RUN pip install uv

# Stage 1: Dependency resolution with UV
FROM base AS deps
WORKDIR /app

# Copy requirements
COPY requirements.txt requirements-dev.txt ./

# Create virtual environment and install production dependencies with UV
RUN uv venv /opt/venv
ENV PATH="/opt/venv/bin:$PATH"
RUN uv pip install --no-cache -r requirements.txt

# Stage 2: Build and test
FROM base AS build
WORKDIR /app

# Install all dependencies including dev
RUN uv venv /opt/venv
ENV PATH="/opt/venv/bin:$PATH"
COPY requirements*.txt ./
RUN uv pip install --no-cache -r requirements.txt -r requirements-dev.txt

# Copy source and run tests
COPY . .
RUN python -m pytest tests/ --cov=src --cov-report=term-missing
RUN python -m black --check src/
RUN python -m isort --check-only src/
RUN python -m mypy src/

# Stage 3: Security and compliance scanning
FROM build AS security
RUN uv pip install safety bandit
RUN safety check
RUN bandit -r src/ -f json -o bandit-report.json

# Stage 4: Optimized production with distroless
FROM gcr.io/distroless/python3-debian11 AS production

# Copy virtual environment and application
COPY --from=deps /opt/venv /opt/venv
COPY --from=build /app/src ./src
COPY --from=build /app/requirements.txt ./

# Set environment for production
ENV PATH="/opt/venv/bin:$PATH" \
    PYTHONUNBUFFERED=1 \
    PYTHONDONTWRITEBYTECODE=1 \
    PYTHONOPTIMIZE=2

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=10s --retries=3 \
    CMD ["python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:8000/health', timeout=5)"]

EXPOSE 8000
CMD ["python", "-m", "gunicorn", "--bind", "0.0.0.0:8000", "--workers", "4", "--worker-class", "uvicorn.workers.UvicornWorker", "src.main:app"]
```

**Go Static Binary with Scratch Base**
```dockerfile
# Pattern 3: Go with ultra-minimal scratch base
FROM golang:1.21-alpine AS base

# Install build dependencies
RUN apk add --no-cache git ca-certificates tzdata upx

# Stage 1: Dependency download
FROM base AS deps
WORKDIR /src

# Copy go mod files
COPY go.mod go.sum ./

# Download dependencies with module cache
RUN --mount=type=cache,target=/go/pkg/mod \
    go mod download

# Stage 2: Build with optimizations
FROM base AS build
WORKDIR /src

# Copy dependencies from cache
COPY --from=deps /go/pkg /go/pkg
COPY . .

# Build static binary with extreme optimizations
RUN --mount=type=cache,target=/go/pkg/mod \
    --mount=type=cache,target=/root/.cache/go-build \
    CGO_ENABLED=0 GOOS=linux GOARCH=amd64 go build \
    -ldflags='-w -s -extldflags "-static"' \
    -a -installsuffix cgo \
    -trimpath \
    -o app ./cmd/server

# Compress binary with UPX (optional, 50-70% size reduction)
RUN upx --best --lzma app

# Stage 3: Testing
FROM build AS test
RUN go test -v ./...
RUN go vet ./...
RUN golangci-lint run

# Stage 4: Minimal scratch image (2-5MB final image)
FROM scratch AS production

# Copy essential files
COPY --from=build /etc/ssl/certs/ca-certificates.crt /etc/ssl/certs/
COPY --from=build /usr/share/zoneinfo /usr/share/zoneinfo
COPY --from=build /src/app /app

# Health check using app's built-in health endpoint
HEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \
    CMD ["/app", "-health-check"]

EXPOSE 8080
ENTRYPOINT ["/app"]
```

**Rust with Cross-Compilation and Security**
```dockerfile
# Pattern 4: Rust with musl for static linking
FROM rust:1.70-alpine AS base

# Install musl development tools
RUN apk add --no-cache musl-dev openssl-dev

# Stage 1: Dependency caching
FROM base AS deps
WORKDIR /app

# Copy Cargo files
COPY Cargo.toml Cargo.lock ./

# Create dummy main and build dependencies
RUN mkdir src && echo 'fn main() {}' > src/main.rs
RUN --mount=type=cache,target=/usr/local/cargo/registry \
    --mount=type=cache,target=/app/target \
    cargo build --release --target x86_64-unknown-linux-musl

# Stage 2: Build application
FROM base AS build
WORKDIR /app

# Copy dependencies from cache
COPY --from=deps /usr/local/cargo /usr/local/cargo
COPY . .

# Build optimized static binary
RUN --mount=type=cache,target=/usr/local/cargo/registry \
    --mount=type=cache,target=/app/target \
    cargo build --release --target x86_64-unknown-linux-musl && \
    cp target/x86_64-unknown-linux-musl/release/app /app/app

# Strip binary for smaller size
RUN strip /app/app

# Stage 3: Security scanning
FROM build AS security
RUN cargo audit
RUN cargo clippy -- -D warnings

# Stage 4: Minimal scratch image
FROM scratch AS production

# Copy static binary
COPY --from=build /app/app /app

# Health check
HEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \
    CMD ["/app", "--health"]

EXPOSE 8000
ENTRYPOINT ["/app"]
```

**Java Spring Boot with GraalVM Native Image**
```dockerfile
# Pattern 5: Java with GraalVM Native Image (sub-second startup)
FROM ghcr.io/graalvm/graalvm-ce:java17 AS base

# Install native-image component
RUN gu install native-image

# Stage 1: Dependencies
FROM base AS deps
WORKDIR /app

# Copy Maven/Gradle files
COPY pom.xml ./
COPY .mvn .mvn
COPY mvnw ./

# Download dependencies
RUN ./mvnw dependency:go-offline

# Stage 2: Build application
FROM base AS build
WORKDIR /app

# Copy dependencies and source
COPY --from=deps /root/.m2 /root/.m2
COPY . .

# Build JAR
RUN ./mvnw clean package -DskipTests

# Build native image
RUN native-image \
    -jar target/*.jar \
    --no-fallback \
    --static \
    --libc=musl \
    -H:+ReportExceptionStackTraces \
    -H:+AddAllCharsets \
    -H:IncludeResourceBundles=sun.util.resources.TimeZoneNames \
    app

# Stage 3: Testing
FROM build AS test
RUN ./mvnw test

# Stage 4: Ultra-minimal final image (20-50MB vs 200-300MB)
FROM scratch AS production

# Copy native binary
COPY --from=build /app/app /app

# Health check
HEALTHCHECK --interval=30s --timeout=3s --start-period=2s --retries=3 \
    CMD ["/app", "--health"]

EXPOSE 8080
ENTRYPOINT ["/app"]
```

**Python Multi-Stage Example**
```dockerfile
# Stage 1: Build dependencies
FROM python:3.11-slim AS builder

# Install build dependencies
RUN apt-get update && apt-get install -y --no-install-recommends \
    gcc \
    libc6-dev \
    && rm -rf /var/lib/apt/lists/*

# Create virtual environment
RUN python -m venv /opt/venv
ENV PATH="/opt/venv/bin:$PATH"

# Install Python dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Stage 2: Runtime
FROM python:3.11-slim AS runtime

# Copy virtual environment from builder
COPY --from=builder /opt/venv /opt/venv
ENV PATH="/opt/venv/bin:$PATH"

# Create non-root user
RUN useradd -m -u 1001 appuser

WORKDIR /app

# Copy application
COPY --chown=appuser:appuser . .

USER appuser

# Gunicorn for production
CMD ["gunicorn", "--bind", "0.0.0.0:8000", "--workers", "4", "app:application"]
```

### 3. Image Size Optimization

Minimize Docker image size:

**Size Reduction Techniques**
```dockerfile
# Alpine-based optimization
FROM alpine:3.18

# Install only necessary packages
RUN apk add --no-cache \
    python3 \
    py3-pip \
    && pip3 install --no-cache-dir --upgrade pip

# Use --no-cache-dir for pip
COPY requirements.txt .
RUN pip3 install --no-cache-dir -r requirements.txt

# Remove unnecessary files
RUN find /usr/local -type d -name __pycache__ -exec rm -rf {} + \
    && find /usr/local -type f -name '*.pyc' -delete

# Golang example with scratch image
FROM golang:1.21-alpine AS builder
WORKDIR /build
COPY . .
# Build static binary
RUN CGO_ENABLED=0 GOOS=linux go build -a -installsuffix cgo -ldflags '-s -w' -o app .

# Final stage: scratch
FROM scratch
# Copy only the binary
COPY --from=builder /build/app /app
# Copy SSL certificates for HTTPS
COPY --from=builder /etc/ssl/certs/ca-certificates.crt /etc/ssl/certs/
ENTRYPOINT ["/app"]
```

**Layer Optimization Script**
```python
def optimize_dockerfile_layers(dockerfile_content):
    """
    Optimize Dockerfile layers
    """
    optimizations = []
    
    # Combine RUN commands
    run_commands = re.findall(r'^RUN\s+(.+?)(?=^(?:RUN|FROM|COPY|ADD|ENV|EXPOSE|CMD|ENTRYPOINT|WORKDIR)|\Z)', 
                             dockerfile_content, re.MULTILINE | re.DOTALL)
    
    if len(run_commands) > 1:
        combined = ' && \\\n    '.join(cmd.strip() for cmd in run_commands)
        optimizations.append({
            'original': '\n'.join(f'RUN {cmd}' for cmd in run_commands),
            'optimized': f'RUN {combined}',
            'benefit': f'Reduces {len(run_commands)} layers to 1'
        })
    
    # Optimize package installation
    apt_install = re.search(r'RUN\s+apt-get\s+update.*?apt-get\s+install\s+(.+?)(?=^(?:RUN|FROM)|\Z)', 
                           dockerfile_content, re.MULTILINE | re.DOTALL)
    
    if apt_install:
        packages = apt_install.group(1)
        optimized = f"""RUN apt-get update && apt-get install -y --no-install-recommends \\
    {packages.strip()} \\
    && rm -rf /var/lib/apt/lists/*"""
        
        optimizations.append({
            'original': apt_install.group(0),
            'optimized': optimized,
            'benefit': 'Reduces image size by cleaning apt cache'
        })
    
    return optimizations
```

### 4. Build Performance Optimization

Speed up Docker builds:

**.dockerignore Optimization**
```
# .dockerignore
# Version control
.git
.gitignore

# Development
.vscode
.idea
*.swp
*.swo

# Dependencies
node_modules
vendor
venv
__pycache__

# Build artifacts
dist
build
*.egg-info
target

# Tests
test
tests
*.test.js
*.spec.js
coverage
.pytest_cache

# Documentation
docs
*.md
LICENSE

# Environment
.env
.env.*

# Logs
*.log
logs

# OS files
.DS_Store
Thumbs.db

# CI/CD
.github
.gitlab
.circleci
Jenkinsfile

# Docker
Dockerfile*
docker-compose*
.dockerignore
```

**Build Cache Optimization**
```dockerfile
# Optimize build cache
FROM node:18-alpine

WORKDIR /app

# Copy package files first (changes less frequently)
COPY package*.json ./

# Install dependencies (cached if package files haven't changed)
RUN npm ci --only=production

# Copy source code (changes frequently)
COPY . .

# Build application
RUN npm run build

# Use BuildKit cache mounts
FROM node:18-alpine AS builder
WORKDIR /app

# Mount cache for package manager
RUN --mount=type=cache,target=/root/.npm \
    npm ci --only=production

# Mount cache for build artifacts
RUN --mount=type=cache,target=/app/.cache \
    npm run build
```

### 5. Security Hardening

Implement security best practices:

**Security-Hardened Dockerfile**
```dockerfile
# Use specific version and minimal base image
FROM alpine:3.18.4

# Install security updates
RUN apk update && apk upgrade && apk add --no-cache \
    ca-certificates \
    && rm -rf /var/cache/apk/*

# Create non-root user
RUN addgroup -g 1001 -S appgroup && \
    adduser -S appuser -u 1001 -G appgroup

# Set secure permissions
RUN mkdir /app && chown -R appuser:appgroup /app
WORKDIR /app

# Copy with correct ownership
COPY --chown=appuser:appgroup . .

# Drop all capabilities
USER appuser

# Read-only root filesystem
# Add volumes for writable directories
VOLUME ["/tmp", "/app/logs"]

# Security labels
LABEL security.scan="trivy" \
      security.updates="auto"

# Health check with timeout
HEALTHCHECK --interval=30s --timeout=3s --retries=3 \
    CMD wget --no-verbose --tries=1 --spider http://localhost:8080/health || exit 1

# Run as PID 1 to handle signals properly
ENTRYPOINT ["dumb-init", "--"]
CMD ["./app"]
```

**Security Scanning Integration**
```yaml
# .github/workflows/docker-security.yml
name: Docker Security Scan

on:
  push:
    paths:
      - 'Dockerfile*'
      - '.dockerignore'

jobs:
  scan:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      
      - name: Run Trivy vulnerability scanner
        uses: aquasecurity/trivy-action@master
        with:
          image-ref: '${{ github.repository }}:${{ github.sha }}'
          format: 'sarif'
          output: 'trivy-results.sarif'
          severity: 'CRITICAL,HIGH'
          
      - name: Upload Trivy scan results
        uses: github/codeql-action/upload-sarif@v2
        with:
          sarif_file: 'trivy-results.sarif'
          
      - name: Run Hadolint
        uses: hadolint/hadolint-action@v3.1.0
        with:
          dockerfile: Dockerfile
          format: sarif
          output-file: hadolint-results.sarif
          
      - name: Upload Hadolint scan results
        uses: github/codeql-action/upload-sarif@v2
        with:
          sarif_file: hadolint-results.sarif
```

### 6. Runtime Optimization

Optimize container runtime performance:

**Runtime Configuration**
```dockerfile
# JVM optimization example
FROM eclipse-temurin:17-jre-alpine

# JVM memory settings based on container limits
ENV JAVA_OPTS="-XX:MaxRAMPercentage=75.0 \
    -XX:InitialRAMPercentage=50.0 \
    -XX:+UseContainerSupport \
    -XX:+OptimizeStringConcat \
    -XX:+UseStringDeduplication \
    -Djava.security.egd=file:/dev/./urandom"

# Node.js optimization
FROM node:18-alpine
ENV NODE_ENV=production \
    NODE_OPTIONS="--max-old-space-size=1024 --optimize-for-size"

# Python optimization
FROM python:3.11-slim
ENV PYTHONUNBUFFERED=1 \
    PYTHONDONTWRITEBYTECODE=1 \
    PYTHONOPTIMIZE=2

# Nginx optimization
FROM nginx:alpine
COPY nginx-optimized.conf /etc/nginx/nginx.conf
# Enable gzip, caching, and connection pooling
```

### 7. Docker Compose Optimization

Optimize multi-container applications:

```yaml
# docker-compose.yml
version: '3.9'

services:
  app:
    build:
      context: .
      dockerfile: Dockerfile
      cache_from:
        - ${REGISTRY}/app:latest
        - ${REGISTRY}/app:builder
      args:
        BUILDKIT_INLINE_CACHE: 1
    image: ${REGISTRY}/app:${VERSION:-latest}
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 512M
        reservations:
          cpus: '0.5'
          memory: 256M
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    restart: unless-stopped
    
  redis:
    image: redis:7-alpine
    command: redis-server --maxmemory 256mb --maxmemory-policy allkeys-lru
    deploy:
      resources:
        limits:
          memory: 256M
    volumes:
      - type: tmpfs
        target: /data
        tmpfs:
          size: 268435456 # 256MB
          
  nginx:
    image: nginx:alpine
    volumes:
      - type: bind
        source: ./nginx.conf
        target: /etc/nginx/nginx.conf
        read_only: true
    depends_on:
      app:
        condition: service_healthy
```

### 8. Build Automation

Automate optimized builds:

```bash
#!/bin/bash
# build-optimize.sh

set -euo pipefail

# Variables
IMAGE_NAME="${1:-myapp}"
VERSION="${2:-latest}"
PLATFORMS="${3:-linux/amd64,linux/arm64}"

echo "üèóÔ∏è  Building optimized Docker image..."

# Enable BuildKit
export DOCKER_BUILDKIT=1

# Build with cache
docker buildx build \
  --platform "${PLATFORMS}" \
  --cache-from "type=registry,ref=${IMAGE_NAME}:buildcache" \
  --cache-to "type=registry,ref=${IMAGE_NAME}:buildcache,mode=max" \
  --tag "${IMAGE_NAME}:${VERSION}" \
  --build-arg BUILDKIT_INLINE_CACHE=1 \
  --progress=plain \
  --push \
  .

# Analyze image size
echo "üìä Image analysis:"
docker run --rm -v /var/run/docker.sock:/var/run/docker.sock \
  wagoodman/dive:latest "${IMAGE_NAME}:${VERSION}"

# Security scan
echo "üîí Security scan:"
trivy image "${IMAGE_NAME}:${VERSION}"

# Size report
echo "üìè Size comparison:"
docker images "${IMAGE_NAME}" --format "table {{.Repository}}\t{{.Tag}}\t{{.Size}}"
```

### 9. Monitoring and Metrics

Track container performance:

```python
# container-metrics.py
import docker
import json
from datetime import datetime

class ContainerMonitor:
    def __init__(self):
        self.client = docker.from_env()
        
    def collect_metrics(self, container_name):
        """Collect container performance metrics"""
        container = self.client.containers.get(container_name)
        stats = container.stats(stream=False)
        
        metrics = {
            'timestamp': datetime.now().isoformat(),
            'container': container_name,
            'cpu': self._calculate_cpu_percent(stats),
            'memory': self._calculate_memory_usage(stats),
            'network': self._calculate_network_io(stats),
            'disk': self._calculate_disk_io(stats)
        }
        
        return metrics
    
    def _calculate_cpu_percent(self, stats):
        """Calculate CPU usage percentage"""
        cpu_delta = stats['cpu_stats']['cpu_usage']['total_usage'] - \
                   stats['precpu_stats']['cpu_usage']['total_usage']
        system_delta = stats['cpu_stats']['system_cpu_usage'] - \
                      stats['precpu_stats']['system_cpu_usage']
        
        if system_delta > 0 and cpu_delta > 0:
            cpu_percent = (cpu_delta / system_delta) * \
                         len(stats['cpu_stats']['cpu_usage']['percpu_usage']) * 100.0
            return round(cpu_percent, 2)
        return 0.0
    
    def _calculate_memory_usage(self, stats):
        """Calculate memory usage"""
        usage = stats['memory_stats']['usage']
        limit = stats['memory_stats']['limit']
        
        return {
            'usage_bytes': usage,
            'limit_bytes': limit,
            'percent': round((usage / limit) * 100, 2)
        }
```

### 10. Best Practices Checklist

```python
def generate_dockerfile_checklist():
    """Generate Dockerfile best practices checklist"""
    checklist = """
## Dockerfile Best Practices Checklist

### Base Image
- [ ] Use specific version tags (not :latest)
- [ ] Use minimal base images (alpine, slim, distroless)
- [ ] Keep base images updated
- [ ] Use official images when possible

### Layers & Caching
- [ ] Order commands from least to most frequently changing
- [ ] Combine RUN commands where appropriate
- [ ] Clean up in the same layer (apt cache, pip cache)
- [ ] Use .dockerignore to exclude unnecessary files

### Security
- [ ] Run as non-root user
- [ ] Don't store secrets in images
- [ ] Scan images for vulnerabilities
- [ ] Use COPY instead of ADD
- [ ] Set read-only root filesystem where possible

### Size Optimization
- [ ] Use multi-stage builds
- [ ] Remove unnecessary dependencies
- [ ] Clear package manager caches
- [ ] Remove temporary files and build artifacts
- [ ] Use --no-install-recommends for apt

### Performance
- [ ] Set appropriate resource limits
- [ ] Use health checks
- [ ] Optimize for startup time
- [ ] Configure logging appropriately
- [ ] Use BuildKit for faster builds

### Maintainability
- [ ] Include LABEL metadata
- [ ] Document exposed ports with EXPOSE
- [ ] Use ARG for build-time variables
- [ ] Include meaningful comments
- [ ] Version your Dockerfiles
"""
    return checklist
```

## Output Format

1. **Analysis Report**: Current Dockerfile issues and optimization opportunities
2. **Optimized Dockerfile**: Rewritten Dockerfile with all optimizations
3. **Size Comparison**: Before/after image size analysis
4. **Build Performance**: Build time improvements and caching strategy
5. **Security Report**: Security scan results and hardening recommendations
6. **Runtime Config**: Optimized runtime settings for the application
7. **Monitoring Setup**: Container metrics and performance tracking
8. **Migration Guide**: Step-by-step guide to implement optimizations

## Cross-Command Integration

### Complete Container-First Development Workflow

**Containerized Development Pipeline**
```bash
# 1. Generate containerized API scaffolding
/api-scaffold
framework: "fastapi"
deployment_target: "kubernetes"
containerization: true
monitoring: true

# 2. Optimize containers for production
/docker-optimize
optimization_level: "production"
security_hardening: true
multi_stage_build: true

# 3. Security scan container images
/security-scan
scan_types: ["container", "dockerfile", "runtime"]
image_name: "app:optimized"
generate_sbom: true

# 4. Generate K8s manifests for optimized containers
/k8s-manifest
container_security: "strict"
resource_optimization: true
horizontal_scaling: true
```

**Integrated Container Configuration**
```python
# container-config.py - Shared across all commands
class IntegratedContainerConfig:
    def __init__(self):
        self.api_config = self.load_api_config()           # From /api-scaffold
        self.security_config = self.load_security_config() # From /security-scan
        self.k8s_config = self.load_k8s_config()          # From /k8s-manifest
        self.test_config = self.load_test_config()         # From /test-harness
        
    def generate_optimized_dockerfile(self):
        """Generate Dockerfile optimized for the specific application"""
        framework = self.api_config.get('framework', 'python')
        security_level = self.security_config.get('level', 'standard')
        deployment_target = self.k8s_config.get('platform', 'kubernetes')
        
        if framework == 'fastapi':
            return self.generate_fastapi_dockerfile(security_level, deployment_target)
        elif framework == 'express':
            return self.generate_express_dockerfile(security_level, deployment_target)
        elif framework == 'django':
            return self.generate_django_dockerfile(security_level, deployment_target)
            
    def generate_fastapi_dockerfile(self, security_level, deployment_target):
        """Generate optimized FastAPI Dockerfile"""
        dockerfile_content = {
            'base_image': self.select_base_image('python', security_level),
            'build_stages': self.configure_build_stages(),
            'security_configs': self.apply_security_configurations(security_level),
            'runtime_optimizations': self.configure_runtime_optimizations(),
            'monitoring_setup': self.configure_monitoring_setup(),
            'health_checks': self.configure_health_checks()
        }
        return dockerfile_content
    
    def select_base_image(self, language, security_level):
        """Select optimal base image based on security and size requirements"""
        base_images = {
            'python': {
                'minimal': 'python:3.11-alpine',
                'standard': 'python:3.11-slim-bookworm',
                'secure': 'chainguard/python:latest-dev',
                'distroless': 'gcr.io/distroless/python3-debian12'
            }
        }
        
        if security_level == 'strict':
            return base_images[language]['distroless']
        elif security_level == 'enhanced':
            return base_images[language]['secure']
        else:
            return base_images[language]['standard']
    
    def configure_build_stages(self):
        """Configure multi-stage build optimization"""
        return {
            'dependencies_stage': {
                'name': 'dependencies',
                'base': 'python:3.11-slim-bookworm',
                'actions': [
                    'COPY requirements.txt .',
                    'RUN pip install --no-cache-dir --user -r requirements.txt'
                ]
            },
            'security_stage': {
                'name': 'security-scan',
                'base': 'dependencies',
                'actions': [
                    'RUN pip-audit --format=json --output=/tmp/security-report.json',
                    'RUN safety check --json --output=/tmp/safety-report.json'
                ]
            },
            'runtime_stage': {
                'name': 'runtime',
                'base': 'python:3.11-slim-bookworm',
                'actions': [
                    'COPY --from=dependencies /root/.local /root/.local',
                    'COPY --from=security-scan /tmp/*-report.json /security-reports/'
                ]
            }
        }
```

**API Container Integration**
```dockerfile
# Dockerfile.api - Generated from /api-scaffold + /docker-optimize
# Multi-stage build optimized for FastAPI applications
FROM python:3.11-slim-bookworm AS base

# Set environment variables for optimization
ENV PYTHONUNBUFFERED=1 \
    PYTHONDONTWRITEBYTECODE=1 \
    PIP_NO_CACHE_DIR=1 \
    PIP_DISABLE_PIP_VERSION_CHECK=1

# Stage 1: Dependencies
FROM base AS dependencies
WORKDIR /app

# Install system dependencies for building Python packages
RUN apt-get update && apt-get install -y --no-install-recommends \
    build-essential \
    && rm -rf /var/lib/apt/lists/*

# Copy and install Python dependencies
COPY requirements.txt .
RUN pip install --user --no-warn-script-location -r requirements.txt

# Stage 2: Security scanning
FROM dependencies AS security-scan
RUN pip install --user pip-audit safety bandit

# Copy source code for security scanning
COPY . .

# Run security scans during build
RUN python -m bandit -r . -f json -o /tmp/bandit-report.json || true
RUN python -m safety check --json --output /tmp/safety-report.json || true
RUN python -m pip_audit --format=json --output=/tmp/pip-audit-report.json || true

# Stage 3: Testing (optional, can be skipped in production builds)
FROM security-scan AS testing
RUN pip install --user pytest pytest-cov

# Run tests during build (from /test-harness integration)
RUN python -m pytest tests/ --cov=src --cov-report=json --cov-report=term

# Stage 4: Production runtime
FROM base AS runtime

# Create non-root user for security
RUN groupadd -r appuser && useradd -r -g appuser appuser

# Set up application directory
WORKDIR /app
RUN chown appuser:appuser /app

# Copy Python packages from dependencies stage
COPY --from=dependencies --chown=appuser:appuser /root/.local /home/appuser/.local

# Copy security reports from security stage
COPY --from=security-scan /tmp/*-report.json /app/security-reports/

# Copy application code
COPY --chown=appuser:appuser . .

# Update PATH to include user packages
ENV PATH=/home/appuser/.local/bin:$PATH

# Switch to non-root user
USER appuser

# Configure health check (integrates with K8s health checks)
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
    CMD python -c "import requests; requests.get('http://localhost:8000/health', timeout=5)"

# Expose port (configured from API scaffold)
EXPOSE 8000

# Set optimal startup command
CMD ["python", "-m", "uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000", "--workers", "4"]
```

**Database Container Integration**
```dockerfile
# Dockerfile.db - Generated for database migrations from /db-migrate
FROM postgres:15-alpine AS base

# Install migration tools
RUN apk add --no-cache python3 py3-pip
RUN pip3 install alembic psycopg2-binary

# Create migration user
RUN addgroup -g 1001 migration && adduser -D -u 1001 -G migration migration

# Stage 1: Migration preparation
FROM base AS migration-prep
WORKDIR /migrations

# Copy migration scripts from /db-migrate output
COPY --chown=migration:migration migrations/ ./
COPY --chown=migration:migration alembic.ini ./

# Validate migration scripts
USER migration
RUN alembic check || echo "Migration validation completed"

# Stage 2: Production database
FROM postgres:15-alpine AS production

# Copy validated migrations
COPY --from=migration-prep --chown=postgres:postgres /migrations /docker-entrypoint-initdb.d/

# Configure PostgreSQL for production
RUN echo "shared_preload_libraries = 'pg_stat_statements'" >> /usr/local/share/postgresql/postgresql.conf.sample
RUN echo "track_activity_query_size = 2048" >> /usr/local/share/postgresql/postgresql.conf.sample
RUN echo "log_min_duration_statement = 1000" >> /usr/local/share/postgresql/postgresql.conf.sample

# Security configurations from /security-scan
RUN echo "ssl = on" >> /usr/local/share/postgresql/postgresql.conf.sample
RUN echo "log_connections = on" >> /usr/local/share/postgresql/postgresql.conf.sample
RUN echo "log_disconnections = on" >> /usr/local/share/postgresql/postgresql.conf.sample

EXPOSE 5432
```

**Frontend Container Integration**
```dockerfile
# Dockerfile.frontend - Generated from /frontend-optimize + /docker-optimize
# Multi-stage build for React/Vue applications
FROM node:18-alpine AS base

# Set environment variables
ENV NODE_ENV=production \
    NPM_CONFIG_CACHE=/tmp/.npm

# Stage 1: Dependencies
FROM base AS dependencies
WORKDIR /app

# Copy package files
COPY package*.json ./

# Install dependencies with optimization
RUN npm ci --only=production --silent

# Stage 2: Build application
FROM base AS build
WORKDIR /app

# Copy dependencies
COPY --from=dependencies /app/node_modules ./node_modules

# Copy source code
COPY . .

# Build application with optimizations from /frontend-optimize
RUN npm run build

# Run security audit
RUN npm audit --audit-level high --production

# Stage 3: Security scanning
FROM build AS security-scan

# Install security scanning tools
RUN npm install -g retire snyk

# Run security scans
RUN retire --outputformat json --outputpath /tmp/retire-report.json || true
RUN snyk test --json > /tmp/snyk-report.json || true

# Stage 4: Production server
FROM nginx:alpine AS production

# Install security updates
RUN apk update && apk upgrade && apk add --no-cache dumb-init

# Create non-root user
RUN addgroup -g 1001 www && adduser -D -u 1001 -G www www

# Copy built application
COPY --from=build --chown=www:www /app/dist /usr/share/nginx/html

# Copy security reports
COPY --from=security-scan /tmp/*-report.json /var/log/security/

# Copy optimized nginx configuration
COPY nginx.conf /etc/nginx/nginx.conf

# Configure proper file permissions
RUN chown -R www:www /usr/share/nginx/html
RUN chmod -R 755 /usr/share/nginx/html

# Use non-root user
USER www

# Health check for frontend
HEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \
    CMD wget --no-verbose --tries=1 --spider http://localhost:80/ || exit 1

EXPOSE 80

# Use dumb-init for proper signal handling
ENTRYPOINT ["dumb-init", "--"]
CMD ["nginx", "-g", "daemon off;"]
```

**Kubernetes Container Integration**
```yaml
# k8s-optimized-deployment.yaml - From /k8s-manifest + /docker-optimize
apiVersion: v1
kind: ConfigMap
metadata:
  name: container-config
  namespace: production
data:
  optimization-level: "production"
  security-level: "strict"
  monitoring-enabled: "true"

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: optimized-api
  namespace: production
  labels:
    app: api
    optimization: enabled
spec:
  replicas: 3
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1
      maxSurge: 1
  selector:
    matchLabels:
      app: api
  template:
    metadata:
      labels:
        app: api
      annotations:
        # Container optimization annotations
        container.seccomp.security.alpha.kubernetes.io/defaultProfileName: runtime/default
        container.apparmor.security.beta.kubernetes.io/api: runtime/default
    spec:
      # Optimized pod configuration
      securityContext:
        runAsNonRoot: true
        runAsUser: 1001
        runAsGroup: 1001
        fsGroup: 1001
        seccompProfile:
          type: RuntimeDefault
      
      # Resource optimization from container analysis
      containers:
      - name: api
        image: registry.company.com/api:optimized-latest
        imagePullPolicy: Always
        
        # Optimized resource allocation
        resources:
          requests:
            memory: "128Mi"     # Optimized based on actual usage
            cpu: "100m"         # Optimized based on load testing
            ephemeral-storage: "1Gi"
          limits:
            memory: "512Mi"     # Prevents OOM, allows burst
            cpu: "500m"         # Allows processing spikes
            ephemeral-storage: "2Gi"
        
        # Container security optimization
        securityContext:
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
          runAsNonRoot: true
          runAsUser: 1001
          capabilities:
            drop:
              - ALL
            add:
              - NET_BIND_SERVICE
        
        # Optimized startup and health checks
        ports:
        - containerPort: 8000
          protocol: TCP
          
        # Fast startup probe
        startupProbe:
          httpGet:
            path: /startup
            port: 8000
          failureThreshold: 30
          periodSeconds: 1
          
        # Optimized health checks
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 5
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3
          
        readinessProbe:
          httpGet:
            path: /ready
            port: 8000
          initialDelaySeconds: 2
          periodSeconds: 5
          timeoutSeconds: 3
          
        # Environment variables from container optimization
        env:
        - name: OPTIMIZATION_LEVEL
          valueFrom:
            configMapKeyRef:
              name: container-config
              key: optimization-level
        - name: PYTHONUNBUFFERED
          value: "1"
        - name: WORKERS
          value: "4"
        
        # Optimized volume mounts
        volumeMounts:
        - name: tmp-volume
          mountPath: /tmp
        - name: cache-volume
          mountPath: /app/cache
        - name: security-reports
          mountPath: /app/security-reports
          readOnly: true
      
      # Optimized volumes
      volumes:
      - name: tmp-volume
        emptyDir:
          sizeLimit: 100Mi
      - name: cache-volume
        emptyDir:
          sizeLimit: 500Mi
      - name: security-reports
        configMap:
          name: security-reports

---
# Horizontal Pod Autoscaler with container metrics
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: api-hpa
  namespace: production
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: optimized-api
  minReplicas: 2
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 50
        periodSeconds: 60
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
      - type: Percent
        value: 100
        periodSeconds: 15

---
# Pod Disruption Budget for rolling updates
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: api-pdb
  namespace: production
spec:
  minAvailable: 1
  selector:
    matchLabels:
      app: api
```

**CI/CD Container Integration**
```yaml
# .github/workflows/container-pipeline.yml
name: Optimized Container Pipeline

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]

env:
  REGISTRY: ghcr.io
  IMAGE_NAME: ${{ github.repository }}

jobs:
  build-and-optimize:
    runs-on: ubuntu-latest
    permissions:
      contents: read
      packages: write
      security-events: write
    
    strategy:
      matrix:
        service: [api, frontend, database]
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
    
    # 1. Build multi-stage container
    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3
      with:
        driver-opts: network=host
    
    - name: Log in to Container Registry
      uses: docker/login-action@v3
      with:
        registry: ${{ env.REGISTRY }}
        username: ${{ github.actor }}
        password: ${{ secrets.GITHUB_TOKEN }}
    
    # 2. Build optimized images
    - name: Build and push container images
      uses: docker/build-push-action@v5
      with:
        context: .
        file: Dockerfile.${{ matrix.service }}
        push: true
        tags: |
          ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}/${{ matrix.service }}:${{ github.sha }}
          ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}/${{ matrix.service }}:latest
        cache-from: type=gha
        cache-to: type=gha,mode=max
        platforms: linux/amd64,linux/arm64
    
    # 3. Container security scanning
    - name: Run Trivy vulnerability scanner
      uses: aquasecurity/trivy-action@master
      with:
        image-ref: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}/${{ matrix.service }}:${{ github.sha }}
        format: 'sarif'
        output: 'trivy-results-${{ matrix.service }}.sarif'
    
    # 4. Container optimization analysis
    - name: Analyze container optimization
      run: |
        docker images --format "table {{.Repository}}\t{{.Tag}}\t{{.Size}}" | \
        grep ${{ matrix.service }} > container-analysis-${{ matrix.service }}.txt
        
        # Compare with baseline
        if [ -f baseline-sizes.txt ]; then
          echo "Size comparison for ${{ matrix.service }}:" >> size-comparison.txt
          echo "Previous: $(grep ${{ matrix.service }} baseline-sizes.txt || echo 'N/A')" >> size-comparison.txt
          echo "Current: $(grep ${{ matrix.service }} container-analysis-${{ matrix.service }}.txt)" >> size-comparison.txt
        fi
    
    # 5. Performance testing
    - name: Container performance testing
      run: |
        # Start container for performance testing
        docker run -d --name test-${{ matrix.service }} \
          ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}/${{ matrix.service }}:${{ github.sha }}
        
        # Wait for startup
        sleep 30
        
        # Run basic performance tests
        if [ "${{ matrix.service }}" = "api" ]; then
          docker exec test-${{ matrix.service }} \
            python -c "import requests; print(requests.get('http://localhost:8000/health').status_code)"
        fi
        
        # Cleanup
        docker stop test-${{ matrix.service }}
        docker rm test-${{ matrix.service }}
    
    # 6. Upload security results
    - name: Upload Trivy scan results to GitHub Security tab
      uses: github/codeql-action/upload-sarif@v2
      with:
        sarif_file: 'trivy-results-${{ matrix.service }}.sarif'
    
    # 7. Generate optimization report
    - name: Generate optimization report
      run: |
        cat > optimization-report-${{ matrix.service }}.md << EOF
        # Container Optimization Report - ${{ matrix.service }}
        
        ## Build Information
        - **Image**: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}/${{ matrix.service }}:${{ github.sha }}
        - **Build Date**: $(date)
        - **Platforms**: linux/amd64, linux/arm64
        
        ## Size Analysis
        $(cat container-analysis-${{ matrix.service }}.txt)
        
        ## Security Scan
        - **Scanner**: Trivy
        - **Results**: See Security tab for detailed findings
        
        ## Optimizations Applied
        - Multi-stage build for minimal image size
        - Security hardening with non-root user
        - Layer caching for faster builds
        - Health checks for reliability
        EOF
    
    - name: Upload optimization report
      uses: actions/upload-artifact@v3
      with:
        name: optimization-report-${{ matrix.service }}
        path: optimization-report-${{ matrix.service }}.md

  deploy-to-staging:
    needs: build-and-optimize
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/develop'
    
    steps:
    - name: Deploy to staging
      run: |
        # Update K8s manifests with new image tags
        # Apply optimized K8s configurations
        kubectl set image deployment/optimized-api \
          api=${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}/api:${{ github.sha }} \
          --namespace=staging
```

**Monitoring Integration**
```python
# container_monitoring.py - Integrated container monitoring
import docker
import psutil
from prometheus_client import CollectorRegistry, Gauge, Counter, Histogram
from typing import Dict, Any

class ContainerOptimizationMonitor:
    """Monitor container performance and optimization metrics"""
    
    def __init__(self):
        self.docker_client = docker.from_env()
        self.registry = CollectorRegistry()
        
        # Metrics from container optimization
        self.container_size_gauge = Gauge(
            'container_image_size_bytes', 
            'Container image size in bytes',
            ['service', 'optimization_level'],
            registry=self.registry
        )
        
        self.container_startup_time = Histogram(
            'container_startup_seconds',
            'Container startup time in seconds',
            ['service'],
            registry=self.registry
        )
        
        self.resource_usage_gauge = Gauge(
            'container_resource_usage_ratio',
            'Container resource usage ratio (used/limit)',
            ['service', 'resource_type'],
            registry=self.registry
        )
    
    def monitor_optimization_metrics(self):
        """Monitor container optimization effectiveness"""
        containers = self.docker_client.containers.list()
        
        optimization_metrics = {}
        
        for container in containers:
            service_name = container.labels.get('app', 'unknown')
            
            # Monitor image size efficiency
            image = container.image
            size_mb = self.get_image_size(image.id) / (1024 * 1024)
            
            # Monitor resource efficiency
            stats = container.stats(stream=False)
            memory_usage = self.calculate_memory_efficiency(stats)
            cpu_usage = self.calculate_cpu_efficiency(stats)
            
            # Monitor startup performance
            startup_time = self.get_container_startup_time(container)
            
            optimization_metrics[service_name] = {
                'image_size_mb': size_mb,
                'memory_efficiency': memory_usage,
                'cpu_efficiency': cpu_usage,
                'startup_time_seconds': startup_time,
                'optimization_score': self.calculate_optimization_score(
                    size_mb, memory_usage, cpu_usage, startup_time
                )
            }
            
            # Update Prometheus metrics
            self.container_size_gauge.labels(
                service=service_name,
                optimization_level='production'
            ).set(size_mb)
            
            self.container_startup_time.labels(
                service=service_name
            ).observe(startup_time)
        
        return optimization_metrics
    
    def calculate_optimization_score(self, size_mb, memory_eff, cpu_eff, startup_time):
        """Calculate overall optimization score (0-100)"""
        size_score = max(0, 100 - (size_mb / 10))  # Penalty for large images
        memory_score = (1 - memory_eff) * 100      # Reward for efficient memory use
        cpu_score = (1 - cpu_eff) * 100           # Reward for efficient CPU use
        startup_score = max(0, 100 - startup_time * 10)  # Penalty for slow startup
        
        return (size_score + memory_score + cpu_score + startup_score) / 4
```

This comprehensive integration ensures containers are optimized across the entire development lifecycle, from build-time optimization through runtime monitoring and Kubernetes deployment.
````

## File: tools/error-analysis.md

````markdown
# Error Analysis and Resolution

Analyze and resolve errors in: $ARGUMENTS

Perform comprehensive error analysis:

1. **Error Pattern Analysis**:
   - Categorize error types
   - Identify root causes
   - Trace error propagation
   - Analyze error frequency
   - Correlate with system events

2. **Debugging Strategy**:
   - Stack trace analysis
   - Variable state inspection
   - Execution flow tracing
   - Memory dump analysis
   - Race condition detection

3. **Error Handling Improvements**:
   - Custom exception classes
   - Error boundary implementation
   - Retry logic with backoff
   - Circuit breaker patterns
   - Graceful degradation

4. **Logging Enhancement**:
   - Structured logging setup
   - Correlation ID implementation
   - Log aggregation strategy
   - Debug vs production logging
   - Sensitive data masking

5. **Monitoring Integration**:
   - Sentry/Rollbar setup
   - Error alerting rules
   - Error dashboards
   - Trend analysis
   - SLA impact assessment

6. **Recovery Mechanisms**:
   - Automatic recovery procedures
   - Data consistency checks
   - Rollback strategies
   - State recovery
   - Compensation logic

7. **Prevention Strategies**:
   - Input validation
   - Type safety improvements
   - Contract testing
   - Defensive programming
   - Code review checklist

Provide specific fixes, preventive measures, and long-term reliability improvements. Include test cases for each error scenario.
````

## File: tools/error-trace.md

````markdown
# Error Tracking and Monitoring

You are an error tracking and observability expert specializing in implementing comprehensive error monitoring solutions. Set up error tracking systems, configure alerts, implement structured logging, and ensure teams can quickly identify and resolve production issues.

## Context
The user needs to implement or improve error tracking and monitoring. Focus on real-time error detection, meaningful alerts, error grouping, performance monitoring, and integration with popular error tracking services.

## Requirements
$ARGUMENTS

## Instructions

### 1. Error Tracking Analysis

Analyze current error handling and tracking:

**Error Analysis Script**
```python
import os
import re
import ast
from pathlib import Path
from collections import defaultdict

class ErrorTrackingAnalyzer:
    def analyze_codebase(self, project_path):
        """
        Analyze error handling patterns in codebase
        """
        analysis = {
            'error_handling': self._analyze_error_handling(project_path),
            'logging_usage': self._analyze_logging(project_path),
            'monitoring_setup': self._check_monitoring_setup(project_path),
            'error_patterns': self._identify_error_patterns(project_path),
            'recommendations': []
        }
        
        self._generate_recommendations(analysis)
        return analysis
    
    def _analyze_error_handling(self, project_path):
        """Analyze error handling patterns"""
        patterns = {
            'try_catch_blocks': 0,
            'unhandled_promises': 0,
            'generic_catches': 0,
            'error_types': defaultdict(int),
            'error_reporting': []
        }
        
        for file_path in Path(project_path).rglob('*.{js,ts,py,java,go}'):
            content = file_path.read_text(errors='ignore')
            
            # JavaScript/TypeScript patterns
            if file_path.suffix in ['.js', '.ts']:
                patterns['try_catch_blocks'] += len(re.findall(r'try\s*{', content))
                patterns['generic_catches'] += len(re.findall(r'catch\s*\([^)]*\)\s*{\s*}', content))
                patterns['unhandled_promises'] += len(re.findall(r'\.then\([^)]+\)(?!\.catch)', content))
            
            # Python patterns
            elif file_path.suffix == '.py':
                try:
                    tree = ast.parse(content)
                    for node in ast.walk(tree):
                        if isinstance(node, ast.Try):
                            patterns['try_catch_blocks'] += 1
                            for handler in node.handlers:
                                if handler.type is None:
                                    patterns['generic_catches'] += 1
                except:
                    pass
        
        return patterns
    
    def _analyze_logging(self, project_path):
        """Analyze logging patterns"""
        logging_patterns = {
            'console_logs': 0,
            'structured_logging': False,
            'log_levels_used': set(),
            'logging_frameworks': []
        }
        
        # Check for logging frameworks
        package_files = ['package.json', 'requirements.txt', 'go.mod', 'pom.xml']
        for pkg_file in package_files:
            pkg_path = Path(project_path) / pkg_file
            if pkg_path.exists():
                content = pkg_path.read_text()
                if 'winston' in content or 'bunyan' in content:
                    logging_patterns['logging_frameworks'].append('winston/bunyan')
                if 'pino' in content:
                    logging_patterns['logging_frameworks'].append('pino')
                if 'logging' in content:
                    logging_patterns['logging_frameworks'].append('python-logging')
                if 'logrus' in content or 'zap' in content:
                    logging_patterns['logging_frameworks'].append('logrus/zap')
        
        return logging_patterns
```

### 2. Error Tracking Service Integration

Implement integrations with popular error tracking services:

**Sentry Integration**
```javascript
// sentry-setup.js
import * as Sentry from "@sentry/node";
import { ProfilingIntegration } from "@sentry/profiling-node";

class SentryErrorTracker {
    constructor(config) {
        this.config = config;
        this.initialized = false;
    }
    
    initialize() {
        Sentry.init({
            dsn: this.config.dsn,
            environment: this.config.environment,
            release: this.config.release,
            
            // Performance Monitoring
            tracesSampleRate: this.config.tracesSampleRate || 0.1,
            profilesSampleRate: this.config.profilesSampleRate || 0.1,
            
            // Integrations
            integrations: [
                // HTTP integration
                new Sentry.Integrations.Http({ tracing: true }),
                
                // Express integration
                new Sentry.Integrations.Express({
                    app: this.config.app,
                    router: true,
                    methods: ['GET', 'POST', 'PUT', 'DELETE', 'PATCH']
                }),
                
                // Database integration
                new Sentry.Integrations.Postgres(),
                new Sentry.Integrations.Mysql(),
                new Sentry.Integrations.Mongo(),
                
                // Profiling
                new ProfilingIntegration(),
                
                // Custom integrations
                ...this.getCustomIntegrations()
            ],
            
            // Filtering
            beforeSend: (event, hint) => {
                // Filter sensitive data
                if (event.request?.cookies) {
                    delete event.request.cookies;
                }
                
                // Filter out specific errors
                if (this.shouldFilterError(event, hint)) {
                    return null;
                }
                
                // Enhance error context
                return this.enhanceErrorEvent(event, hint);
            },
            
            // Breadcrumbs
            beforeBreadcrumb: (breadcrumb, hint) => {
                // Filter sensitive breadcrumbs
                if (breadcrumb.category === 'console' && breadcrumb.level === 'debug') {
                    return null;
                }
                
                return breadcrumb;
            },
            
            // Options
            attachStacktrace: true,
            shutdownTimeout: 5000,
            maxBreadcrumbs: 100,
            debug: this.config.debug || false,
            
            // Tags
            initialScope: {
                tags: {
                    component: this.config.component,
                    version: this.config.version
                },
                user: {
                    id: this.config.userId,
                    segment: this.config.userSegment
                }
            }
        });
        
        this.initialized = true;
        this.setupErrorHandlers();
    }
    
    setupErrorHandlers() {
        // Global error handler
        process.on('uncaughtException', (error) => {
            console.error('Uncaught Exception:', error);
            Sentry.captureException(error, {
                tags: { type: 'uncaught_exception' },
                level: 'fatal'
            });
            
            // Graceful shutdown
            this.gracefulShutdown();
        });
        
        // Promise rejection handler
        process.on('unhandledRejection', (reason, promise) => {
            console.error('Unhandled Rejection:', reason);
            Sentry.captureException(reason, {
                tags: { type: 'unhandled_rejection' },
                extra: { promise: promise.toString() }
            });
        });
    }
    
    enhanceErrorEvent(event, hint) {
        // Add custom context
        event.extra = {
            ...event.extra,
            memory: process.memoryUsage(),
            uptime: process.uptime(),
            nodeVersion: process.version
        };
        
        // Add user context
        if (this.config.getUserContext) {
            event.user = this.config.getUserContext();
        }
        
        // Add custom fingerprinting
        if (hint.originalException) {
            event.fingerprint = this.generateFingerprint(hint.originalException);
        }
        
        return event;
    }
    
    generateFingerprint(error) {
        // Custom fingerprinting logic
        const fingerprint = [];
        
        // Group by error type
        fingerprint.push(error.name || 'Error');
        
        // Group by error location
        if (error.stack) {
            const match = error.stack.match(/at\s+(.+?)\s+\(/);
            if (match) {
                fingerprint.push(match[1]);
            }
        }
        
        // Group by custom properties
        if (error.code) {
            fingerprint.push(error.code);
        }
        
        return fingerprint;
    }
}

// Express middleware
export const sentryMiddleware = {
    requestHandler: Sentry.Handlers.requestHandler(),
    tracingHandler: Sentry.Handlers.tracingHandler(),
    errorHandler: Sentry.Handlers.errorHandler({
        shouldHandleError(error) {
            // Capture 4xx and 5xx errors
            if (error.status >= 400) {
                return true;
            }
            return false;
        }
    })
};
```

**Custom Error Tracking Service**
```typescript
// error-tracker.ts
interface ErrorEvent {
    timestamp: Date;
    level: 'debug' | 'info' | 'warning' | 'error' | 'fatal';
    message: string;
    stack?: string;
    context: {
        user?: any;
        request?: any;
        environment: string;
        release: string;
        tags: Record<string, string>;
        extra: Record<string, any>;
    };
    fingerprint: string[];
}

class ErrorTracker {
    private queue: ErrorEvent[] = [];
    private batchSize = 10;
    private flushInterval = 5000;
    
    constructor(private config: ErrorTrackerConfig) {
        this.startBatchProcessor();
    }
    
    captureException(error: Error, context?: Partial<ErrorEvent['context']>) {
        const event: ErrorEvent = {
            timestamp: new Date(),
            level: 'error',
            message: error.message,
            stack: error.stack,
            context: {
                environment: this.config.environment,
                release: this.config.release,
                tags: {},
                extra: {},
                ...context
            },
            fingerprint: this.generateFingerprint(error)
        };
        
        this.addToQueue(event);
    }
    
    captureMessage(message: string, level: ErrorEvent['level'] = 'info') {
        const event: ErrorEvent = {
            timestamp: new Date(),
            level,
            message,
            context: {
                environment: this.config.environment,
                release: this.config.release,
                tags: {},
                extra: {}
            },
            fingerprint: [message]
        };
        
        this.addToQueue(event);
    }
    
    private addToQueue(event: ErrorEvent) {
        // Apply sampling
        if (Math.random() > this.config.sampleRate) {
            return;
        }
        
        // Filter sensitive data
        event = this.sanitizeEvent(event);
        
        // Add to queue
        this.queue.push(event);
        
        // Flush if queue is full
        if (this.queue.length >= this.batchSize) {
            this.flush();
        }
    }
    
    private sanitizeEvent(event: ErrorEvent): ErrorEvent {
        // Remove sensitive data
        const sensitiveKeys = ['password', 'token', 'secret', 'api_key'];
        
        const sanitize = (obj: any): any => {
            if (!obj || typeof obj !== 'object') return obj;
            
            const cleaned = Array.isArray(obj) ? [] : {};
            
            for (const [key, value] of Object.entries(obj)) {
                if (sensitiveKeys.some(k => key.toLowerCase().includes(k))) {
                    cleaned[key] = '[REDACTED]';
                } else if (typeof value === 'object') {
                    cleaned[key] = sanitize(value);
                } else {
                    cleaned[key] = value;
                }
            }
            
            return cleaned;
        };
        
        return {
            ...event,
            context: sanitize(event.context)
        };
    }
    
    private async flush() {
        if (this.queue.length === 0) return;
        
        const events = this.queue.splice(0, this.batchSize);
        
        try {
            await this.sendEvents(events);
        } catch (error) {
            console.error('Failed to send error events:', error);
            // Re-queue events
            this.queue.unshift(...events);
        }
    }
    
    private async sendEvents(events: ErrorEvent[]) {
        const response = await fetch(this.config.endpoint, {
            method: 'POST',
            headers: {
                'Content-Type': 'application/json',
                'Authorization': `Bearer ${this.config.apiKey}`
            },
            body: JSON.stringify({ events })
        });
        
        if (!response.ok) {
            throw new Error(`Error tracking API returned ${response.status}`);
        }
    }
}
```

### 3. Structured Logging Implementation

Implement comprehensive structured logging:

**Advanced Logger**
```typescript
// structured-logger.ts
import winston from 'winston';
import { ElasticsearchTransport } from 'winston-elasticsearch';

class StructuredLogger {
    private logger: winston.Logger;
    
    constructor(config: LoggerConfig) {
        this.logger = winston.createLogger({
            level: config.level || 'info',
            format: winston.format.combine(
                winston.format.timestamp(),
                winston.format.errors({ stack: true }),
                winston.format.metadata(),
                winston.format.json()
            ),
            defaultMeta: {
                service: config.service,
                environment: config.environment,
                version: config.version
            },
            transports: this.createTransports(config)
        });
    }
    
    private createTransports(config: LoggerConfig): winston.transport[] {
        const transports: winston.transport[] = [];
        
        // Console transport for development
        if (config.environment === 'development') {
            transports.push(new winston.transports.Console({
                format: winston.format.combine(
                    winston.format.colorize(),
                    winston.format.simple()
                )
            }));
        }
        
        // File transport for all environments
        transports.push(new winston.transports.File({
            filename: 'logs/error.log',
            level: 'error',
            maxsize: 5242880, // 5MB
            maxFiles: 5
        }));
        
        transports.push(new winston.transports.File({
            filename: 'logs/combined.log',
            maxsize: 5242880,
            maxFiles: 5
        });
        
        // Elasticsearch transport for production
        if (config.elasticsearch) {
            transports.push(new ElasticsearchTransport({
                level: 'info',
                clientOpts: config.elasticsearch,
                index: `logs-${config.service}`,
                transformer: (logData) => {
                    return {
                        '@timestamp': logData.timestamp,
                        severity: logData.level,
                        message: logData.message,
                        fields: {
                            ...logData.metadata,
                            ...logData.defaultMeta
                        }
                    };
                }
            }));
        }
        
        return transports;
    }
    
    // Logging methods with context
    error(message: string, error?: Error, context?: any) {
        this.logger.error(message, {
            error: {
                message: error?.message,
                stack: error?.stack,
                name: error?.name
            },
            ...context
        });
    }
    
    warn(message: string, context?: any) {
        this.logger.warn(message, context);
    }
    
    info(message: string, context?: any) {
        this.logger.info(message, context);
    }
    
    debug(message: string, context?: any) {
        this.logger.debug(message, context);
    }
    
    // Performance logging
    startTimer(label: string): () => void {
        const start = Date.now();
        return () => {
            const duration = Date.now() - start;
            this.info(`Timer ${label}`, { duration, label });
        };
    }
    
    // Audit logging
    audit(action: string, userId: string, details: any) {
        this.info('Audit Event', {
            type: 'audit',
            action,
            userId,
            timestamp: new Date().toISOString(),
            details
        });
    }
}

// Request logging middleware
export function requestLoggingMiddleware(logger: StructuredLogger) {
    return (req: Request, res: Response, next: NextFunction) => {
        const start = Date.now();
        
        // Log request
        logger.info('Incoming request', {
            method: req.method,
            url: req.url,
            ip: req.ip,
            userAgent: req.get('user-agent')
        });
        
        // Log response
        res.on('finish', () => {
            const duration = Date.now() - start;
            logger.info('Request completed', {
                method: req.method,
                url: req.url,
                status: res.statusCode,
                duration,
                contentLength: res.get('content-length')
            });
        });
        
        next();
    };
}
```

### 4. Error Alerting Configuration

Set up intelligent alerting:

**Alert Manager**
```python
# alert_manager.py
from dataclasses import dataclass
from typing import List, Dict, Optional
from datetime import datetime, timedelta
import asyncio

@dataclass
class AlertRule:
    name: str
    condition: str
    threshold: float
    window: timedelta
    severity: str
    channels: List[str]
    cooldown: timedelta = timedelta(minutes=15)

class AlertManager:
    def __init__(self, config):
        self.config = config
        self.rules = self._load_rules()
        self.alert_history = {}
        self.channels = self._setup_channels()
    
    def _load_rules(self):
        """Load alert rules from configuration"""
        return [
            AlertRule(
                name="High Error Rate",
                condition="error_rate",
                threshold=0.05,  # 5% error rate
                window=timedelta(minutes=5),
                severity="critical",
                channels=["slack", "pagerduty"]
            ),
            AlertRule(
                name="Response Time Degradation",
                condition="response_time_p95",
                threshold=1000,  # 1 second
                window=timedelta(minutes=10),
                severity="warning",
                channels=["slack"]
            ),
            AlertRule(
                name="Memory Usage Critical",
                condition="memory_usage_percent",
                threshold=90,
                window=timedelta(minutes=5),
                severity="critical",
                channels=["slack", "pagerduty"]
            ),
            AlertRule(
                name="Disk Space Low",
                condition="disk_free_percent",
                threshold=10,
                window=timedelta(minutes=15),
                severity="warning",
                channels=["slack", "email"]
            )
        ]
    
    async def evaluate_rules(self, metrics: Dict):
        """Evaluate all alert rules against current metrics"""
        for rule in self.rules:
            if await self._should_alert(rule, metrics):
                await self._send_alert(rule, metrics)
    
    async def _should_alert(self, rule: AlertRule, metrics: Dict) -> bool:
        """Check if alert should be triggered"""
        # Check if metric exists
        if rule.condition not in metrics:
            return False
        
        # Check threshold
        value = metrics[rule.condition]
        if not self._check_threshold(value, rule.threshold, rule.condition):
            return False
        
        # Check cooldown
        last_alert = self.alert_history.get(rule.name)
        if last_alert and datetime.now() - last_alert < rule.cooldown:
            return False
        
        return True
    
    async def _send_alert(self, rule: AlertRule, metrics: Dict):
        """Send alert through configured channels"""
        alert_data = {
            "rule": rule.name,
            "severity": rule.severity,
            "value": metrics[rule.condition],
            "threshold": rule.threshold,
            "timestamp": datetime.now().isoformat(),
            "environment": self.config.environment,
            "service": self.config.service
        }
        
        # Send to all channels
        tasks = []
        for channel_name in rule.channels:
            if channel_name in self.channels:
                channel = self.channels[channel_name]
                tasks.append(channel.send(alert_data))
        
        await asyncio.gather(*tasks)
        
        # Update alert history
        self.alert_history[rule.name] = datetime.now()

# Alert channels
class SlackAlertChannel:
    def __init__(self, webhook_url):
        self.webhook_url = webhook_url
    
    async def send(self, alert_data):
        """Send alert to Slack"""
        color = {
            "critical": "danger",
            "warning": "warning",
            "info": "good"
        }.get(alert_data["severity"], "danger")
        
        payload = {
            "attachments": [{
                "color": color,
                "title": f"üö® {alert_data['rule']}",
                "fields": [
                    {
                        "title": "Severity",
                        "value": alert_data["severity"].upper(),
                        "short": True
                    },
                    {
                        "title": "Environment",
                        "value": alert_data["environment"],
                        "short": True
                    },
                    {
                        "title": "Current Value",
                        "value": str(alert_data["value"]),
                        "short": True
                    },
                    {
                        "title": "Threshold",
                        "value": str(alert_data["threshold"]),
                        "short": True
                    }
                ],
                "footer": alert_data["service"],
                "ts": int(datetime.now().timestamp())
            }]
        }
        
        # Send to Slack
        async with aiohttp.ClientSession() as session:
            await session.post(self.webhook_url, json=payload)
```

### 5. Error Grouping and Deduplication

Implement intelligent error grouping:

**Error Grouping Algorithm**
```python
import hashlib
import re
from difflib import SequenceMatcher

class ErrorGrouper:
    def __init__(self):
        self.groups = {}
        self.patterns = self._compile_patterns()
    
    def _compile_patterns(self):
        """Compile regex patterns for normalization"""
        return {
            'numbers': re.compile(r'\b\d+\b'),
            'uuids': re.compile(r'[a-f0-9]{8}-[a-f0-9]{4}-[a-f0-9]{4}-[a-f0-9]{4}-[a-f0-9]{12}'),
            'urls': re.compile(r'https?://[^\s]+'),
            'file_paths': re.compile(r'(/[^/\s]+)+'),
            'memory_addresses': re.compile(r'0x[0-9a-fA-F]+'),
            'timestamps': re.compile(r'\d{4}-\d{2}-\d{2}[T\s]\d{2}:\d{2}:\d{2}')
        }
    
    def group_error(self, error):
        """Group error with similar errors"""
        fingerprint = self.generate_fingerprint(error)
        
        # Find existing group
        group = self.find_similar_group(fingerprint, error)
        
        if group:
            group['count'] += 1
            group['last_seen'] = error['timestamp']
            group['instances'].append(error)
        else:
            # Create new group
            self.groups[fingerprint] = {
                'fingerprint': fingerprint,
                'first_seen': error['timestamp'],
                'last_seen': error['timestamp'],
                'count': 1,
                'instances': [error],
                'pattern': self.extract_pattern(error)
            }
        
        return fingerprint
    
    def generate_fingerprint(self, error):
        """Generate unique fingerprint for error"""
        # Normalize error message
        normalized = self.normalize_message(error['message'])
        
        # Include error type and location
        components = [
            error.get('type', 'Unknown'),
            normalized,
            self.extract_location(error.get('stack', ''))
        ]
        
        # Generate hash
        fingerprint = hashlib.sha256(
            '|'.join(components).encode()
        ).hexdigest()[:16]
        
        return fingerprint
    
    def normalize_message(self, message):
        """Normalize error message for grouping"""
        # Replace dynamic values
        normalized = message
        for pattern_name, pattern in self.patterns.items():
            normalized = pattern.sub(f'<{pattern_name}>', normalized)
        
        return normalized.strip()
    
    def extract_location(self, stack):
        """Extract error location from stack trace"""
        if not stack:
            return 'unknown'
        
        lines = stack.split('\n')
        for line in lines:
            # Look for file references
            if ' at ' in line:
                # Extract file and line number
                match = re.search(r'at\s+(.+?)\s*\((.+?):(\d+):(\d+)\)', line)
                if match:
                    file_path = match.group(2)
                    # Normalize file path
                    file_path = re.sub(r'.*/(?=src/|lib/|app/)', '', file_path)
                    return f"{file_path}:{match.group(3)}"
        
        return 'unknown'
    
    def find_similar_group(self, fingerprint, error):
        """Find similar error group using fuzzy matching"""
        if fingerprint in self.groups:
            return self.groups[fingerprint]
        
        # Try fuzzy matching
        normalized_message = self.normalize_message(error['message'])
        
        for group_fp, group in self.groups.items():
            similarity = SequenceMatcher(
                None,
                normalized_message,
                group['pattern']
            ).ratio()
            
            if similarity > 0.85:  # 85% similarity threshold
                return group
        
        return None
```

### 6. Performance Impact Tracking

Monitor performance impact of errors:

**Performance Monitor**
```typescript
// performance-monitor.ts
interface PerformanceMetrics {
    responseTime: number;
    errorRate: number;
    throughput: number;
    apdex: number;
    resourceUsage: {
        cpu: number;
        memory: number;
        disk: number;
    };
}

class PerformanceMonitor {
    private metrics: Map<string, PerformanceMetrics[]> = new Map();
    private intervals: Map<string, NodeJS.Timer> = new Map();
    
    startMonitoring(service: string, interval: number = 60000) {
        const timer = setInterval(() => {
            this.collectMetrics(service);
        }, interval);
        
        this.intervals.set(service, timer);
    }
    
    private async collectMetrics(service: string) {
        const metrics: PerformanceMetrics = {
            responseTime: await this.getResponseTime(service),
            errorRate: await this.getErrorRate(service),
            throughput: await this.getThroughput(service),
            apdex: await this.calculateApdex(service),
            resourceUsage: await this.getResourceUsage()
        };
        
        // Store metrics
        if (!this.metrics.has(service)) {
            this.metrics.set(service, []);
        }
        
        const serviceMetrics = this.metrics.get(service)!;
        serviceMetrics.push(metrics);
        
        // Keep only last 24 hours
        const dayAgo = Date.now() - 24 * 60 * 60 * 1000;
        const filtered = serviceMetrics.filter(m => m.timestamp > dayAgo);
        this.metrics.set(service, filtered);
        
        // Check for anomalies
        this.detectAnomalies(service, metrics);
    }
    
    private detectAnomalies(service: string, current: PerformanceMetrics) {
        const history = this.metrics.get(service) || [];
        if (history.length < 10) return; // Need history for comparison
        
        // Calculate baselines
        const baseline = this.calculateBaseline(history.slice(-60)); // Last hour
        
        // Check for anomalies
        const anomalies = [];
        
        if (current.responseTime > baseline.responseTime * 2) {
            anomalies.push({
                type: 'response_time_spike',
                severity: 'warning',
                value: current.responseTime,
                baseline: baseline.responseTime
            });
        }
        
        if (current.errorRate > baseline.errorRate + 0.05) {
            anomalies.push({
                type: 'error_rate_increase',
                severity: 'critical',
                value: current.errorRate,
                baseline: baseline.errorRate
            });
        }
        
        if (anomalies.length > 0) {
            this.reportAnomalies(service, anomalies);
        }
    }
    
    private calculateBaseline(history: PerformanceMetrics[]) {
        const sum = history.reduce((acc, m) => ({
            responseTime: acc.responseTime + m.responseTime,
            errorRate: acc.errorRate + m.errorRate,
            throughput: acc.throughput + m.throughput,
            apdex: acc.apdex + m.apdex
        }), {
            responseTime: 0,
            errorRate: 0,
            throughput: 0,
            apdex: 0
        });
        
        return {
            responseTime: sum.responseTime / history.length,
            errorRate: sum.errorRate / history.length,
            throughput: sum.throughput / history.length,
            apdex: sum.apdex / history.length
        };
    }
    
    async calculateApdex(service: string, threshold: number = 500) {
        // Apdex = (Satisfied + Tolerating/2) / Total
        const satisfied = await this.countRequests(service, 0, threshold);
        const tolerating = await this.countRequests(service, threshold, threshold * 4);
        const total = await this.getTotalRequests(service);
        
        if (total === 0) return 1;
        
        return (satisfied + tolerating / 2) / total;
    }
}
```

### 7. Error Recovery Strategies

Implement automatic error recovery:

**Recovery Manager**
```javascript
// recovery-manager.js
class RecoveryManager {
    constructor(config) {
        this.strategies = new Map();
        this.retryPolicies = config.retryPolicies || {};
        this.circuitBreakers = new Map();
        this.registerDefaultStrategies();
    }
    
    registerStrategy(errorType, strategy) {
        this.strategies.set(errorType, strategy);
    }
    
    registerDefaultStrategies() {
        // Network errors
        this.registerStrategy('NetworkError', async (error, context) => {
            return this.retryWithBackoff(
                context.operation,
                this.retryPolicies.network || {
                    maxRetries: 3,
                    baseDelay: 1000,
                    maxDelay: 10000
                }
            );
        });
        
        // Database errors
        this.registerStrategy('DatabaseError', async (error, context) => {
            // Try read replica if available
            if (context.operation.type === 'read' && context.readReplicas) {
                return this.tryReadReplica(context);
            }
            
            // Otherwise retry with backoff
            return this.retryWithBackoff(
                context.operation,
                this.retryPolicies.database || {
                    maxRetries: 2,
                    baseDelay: 500,
                    maxDelay: 5000
                }
            );
        });
        
        // Rate limit errors
        this.registerStrategy('RateLimitError', async (error, context) => {
            const retryAfter = error.retryAfter || 60;
            await this.delay(retryAfter * 1000);
            return context.operation();
        });
        
        // Circuit breaker for external services
        this.registerStrategy('ExternalServiceError', async (error, context) => {
            const breaker = this.getCircuitBreaker(context.service);
            
            try {
                return await breaker.execute(context.operation);
            } catch (error) {
                // Fallback to cache or default
                if (context.fallback) {
                    return context.fallback();
                }
                throw error;
            }
        });
    }
    
    async recover(error, context) {
        const errorType = this.classifyError(error);
        const strategy = this.strategies.get(errorType);
        
        if (!strategy) {
            // No recovery strategy, rethrow
            throw error;
        }
        
        try {
            const result = await strategy(error, context);
            
            // Log recovery success
            this.logRecovery(error, errorType, 'success');
            
            return result;
        } catch (recoveryError) {
            // Log recovery failure
            this.logRecovery(error, errorType, 'failure', recoveryError);
            
            // Throw original error
            throw error;
        }
    }
    
    async retryWithBackoff(operation, policy) {
        let lastError;
        let delay = policy.baseDelay;
        
        for (let attempt = 0; attempt < policy.maxRetries; attempt++) {
            try {
                return await operation();
            } catch (error) {
                lastError = error;
                
                if (attempt < policy.maxRetries - 1) {
                    await this.delay(delay);
                    delay = Math.min(delay * 2, policy.maxDelay);
                }
            }
        }
        
        throw lastError;
    }
    
    getCircuitBreaker(service) {
        if (!this.circuitBreakers.has(service)) {
            this.circuitBreakers.set(service, new CircuitBreaker({
                timeout: 3000,
                errorThresholdPercentage: 50,
                resetTimeout: 30000,
                rollingCountTimeout: 10000,
                rollingCountBuckets: 10,
                volumeThreshold: 10
            }));
        }
        
        return this.circuitBreakers.get(service);
    }
    
    classifyError(error) {
        // Classify by error code
        if (error.code === 'ECONNREFUSED' || error.code === 'ETIMEDOUT') {
            return 'NetworkError';
        }
        
        if (error.code === 'ER_LOCK_DEADLOCK' || error.code === 'SQLITE_BUSY') {
            return 'DatabaseError';
        }
        
        if (error.status === 429) {
            return 'RateLimitError';
        }
        
        if (error.isExternalService) {
            return 'ExternalServiceError';
        }
        
        // Default
        return 'UnknownError';
    }
}

// Circuit breaker implementation
class CircuitBreaker {
    constructor(options) {
        this.options = options;
        this.state = 'CLOSED';
        this.failures = 0;
        this.successes = 0;
        this.nextAttempt = Date.now();
    }
    
    async execute(operation) {
        if (this.state === 'OPEN') {
            if (Date.now() < this.nextAttempt) {
                throw new Error('Circuit breaker is OPEN');
            }
            
            // Try half-open
            this.state = 'HALF_OPEN';
        }
        
        try {
            const result = await Promise.race([
                operation(),
                this.timeout(this.options.timeout)
            ]);
            
            this.onSuccess();
            return result;
        } catch (error) {
            this.onFailure();
            throw error;
        }
    }
    
    onSuccess() {
        this.failures = 0;
        
        if (this.state === 'HALF_OPEN') {
            this.successes++;
            if (this.successes >= this.options.volumeThreshold) {
                this.state = 'CLOSED';
                this.successes = 0;
            }
        }
    }
    
    onFailure() {
        this.failures++;
        
        if (this.state === 'HALF_OPEN') {
            this.state = 'OPEN';
            this.nextAttempt = Date.now() + this.options.resetTimeout;
        } else if (this.failures >= this.options.volumeThreshold) {
            this.state = 'OPEN';
            this.nextAttempt = Date.now() + this.options.resetTimeout;
        }
    }
}
```

### 8. Error Dashboard

Create comprehensive error dashboard:

**Dashboard Component**
```typescript
// error-dashboard.tsx
import React from 'react';
import { LineChart, BarChart, PieChart } from 'recharts';

const ErrorDashboard: React.FC = () => {
    const [metrics, setMetrics] = useState<DashboardMetrics>();
    const [timeRange, setTimeRange] = useState('1h');
    
    useEffect(() => {
        const fetchMetrics = async () => {
            const data = await getErrorMetrics(timeRange);
            setMetrics(data);
        };
        
        fetchMetrics();
        const interval = setInterval(fetchMetrics, 30000); // Update every 30s
        
        return () => clearInterval(interval);
    }, [timeRange]);
    
    if (!metrics) return <Loading />;
    
    return (
        <div className="error-dashboard">
            <Header>
                <h1>Error Tracking Dashboard</h1>
                <TimeRangeSelector
                    value={timeRange}
                    onChange={setTimeRange}
                    options={['1h', '6h', '24h', '7d', '30d']}
                />
            </Header>
            
            <MetricCards>
                <MetricCard
                    title="Error Rate"
                    value={`${(metrics.errorRate * 100).toFixed(2)}%`}
                    trend={metrics.errorRateTrend}
                    status={metrics.errorRate > 0.05 ? 'critical' : 'ok'}
                />
                <MetricCard
                    title="Total Errors"
                    value={metrics.totalErrors.toLocaleString()}
                    trend={metrics.errorsTrend}
                />
                <MetricCard
                    title="Affected Users"
                    value={metrics.affectedUsers.toLocaleString()}
                    trend={metrics.usersTrend}
                />
                <MetricCard
                    title="MTTR"
                    value={formatDuration(metrics.mttr)}
                    trend={metrics.mttrTrend}
                />
            </MetricCards>
            
            <ChartGrid>
                <ChartCard title="Error Trend">
                    <LineChart data={metrics.errorTrend}>
                        <Line
                            type="monotone"
                            dataKey="errors"
                            stroke="#ff6b6b"
                            strokeWidth={2}
                        />
                        <Line
                            type="monotone"
                            dataKey="warnings"
                            stroke="#ffd93d"
                            strokeWidth={2}
                        />
                    </LineChart>
                </ChartCard>
                
                <ChartCard title="Error Distribution">
                    <PieChart data={metrics.errorDistribution}>
                        <Pie
                            dataKey="count"
                            nameKey="type"
                            cx="50%"
                            cy="50%"
                            outerRadius={80}
                        />
                    </PieChart>
                </ChartCard>
                
                <ChartCard title="Top Errors">
                    <BarChart data={metrics.topErrors}>
                        <Bar dataKey="count" fill="#ff6b6b" />
                    </BarChart>
                </ChartCard>
                
                <ChartCard title="Error Heatmap">
                    <ErrorHeatmap data={metrics.errorHeatmap} />
                </ChartCard>
            </ChartGrid>
            
            <ErrorList>
                <h2>Recent Errors</h2>
                <ErrorTable
                    errors={metrics.recentErrors}
                    onErrorClick={handleErrorClick}
                />
            </ErrorList>
            
            <AlertsSection>
                <h2>Active Alerts</h2>
                <AlertsList alerts={metrics.activeAlerts} />
            </AlertsSection>
        </div>
    );
};

// Real-time error stream
const ErrorStream: React.FC = () => {
    const [errors, setErrors] = useState<ErrorEvent[]>([]);
    
    useEffect(() => {
        const eventSource = new EventSource('/api/errors/stream');
        
        eventSource.onmessage = (event) => {
            const error = JSON.parse(event.data);
            setErrors(prev => [error, ...prev].slice(0, 100));
        };
        
        return () => eventSource.close();
    }, []);
    
    return (
        <div className="error-stream">
            <h3>Live Error Stream</h3>
            <div className="stream-container">
                {errors.map((error, index) => (
                    <ErrorStreamItem
                        key={error.id}
                        error={error}
                        isNew={index === 0}
                    />
                ))}
            </div>
        </div>
    );
};
```

## Output Format

1. **Error Tracking Analysis**: Current error handling assessment
2. **Integration Configuration**: Setup for error tracking services
3. **Logging Implementation**: Structured logging setup
4. **Alert Rules**: Intelligent alerting configuration
5. **Error Grouping**: Deduplication and grouping logic
6. **Recovery Strategies**: Automatic error recovery implementation
7. **Dashboard Setup**: Real-time error monitoring dashboard
8. **Documentation**: Implementation and troubleshooting guide

Focus on providing comprehensive error visibility, intelligent alerting, and quick error resolution capabilities.
````

## File: tools/issue.md

````markdown
Please analyze and fix the GitHub issue: $ARGUMENTS.

Follow these steps:

# PLAN
1. Use 'gh issue view' to get the issue details
2. Understand the problem described in the issue
3. Ask clarifying questions if necessary
4. Understand the prior art for this issue
- Search the scratchpads for previous thoughts related to the issue
- Search PRs to see if you can find history on this issue
- Search the codebase for relevant files
5. Think harder about how to break the issue down into a seriers of small, manageable tasks.
6. Document your plan in a new scratchpad
  - include the issue name in the filename
  - include a link to the issue in the scratchpad.

# CREATE
- Create a new branch for the issue
- Solve the issue in small, manageable steps, according to your plan.
- Commit your changes after each step.

# TEST
- Use playwright via MCP to test the changes if you have made changes to the UI
- Write tests to describe the expected behavior of your code
- Run the full test suite to ensure you haven't broken anything
- If the tests are failing, fix them
- Ensure that all tests are passing before moving on to the next step

# DEPLOY
- Open a PR and request a review.

Remember to use the GitHub CLI (`gh`) for all GitHub-related tasks.
````

## File: tools/k8s-manifest.md

````markdown
# Kubernetes Manifest Generation

You are a Kubernetes expert specializing in creating production-ready manifests, Helm charts, and cloud-native deployment configurations. Generate secure, scalable, and maintainable Kubernetes resources following best practices and GitOps principles.

## Context
The user needs to create or optimize Kubernetes manifests for deploying applications. Focus on production readiness, security hardening, resource optimization, observability, and multi-environment configurations.

## Requirements
$ARGUMENTS

## Instructions

### 1. Application Analysis

Analyze the application to determine Kubernetes requirements:

**Framework-Specific Analysis**
```python
import yaml
import json
from pathlib import Path
from typing import Dict, List, Any

class AdvancedK8sAnalyzer:
    def __init__(self):
        self.framework_patterns = {
            'react': {
                'files': ['package.json', 'src/App.js', 'src/index.js'],
                'build_tool': ['vite', 'webpack', 'create-react-app'],
                'deployment_type': 'static',
                'port': 3000,
                'health_check': '/health',
                'resources': {'cpu': '100m', 'memory': '256Mi'}
            },
            'nextjs': {
                'files': ['next.config.js', 'pages/', 'app/'],
                'deployment_type': 'ssr',
                'port': 3000,
                'health_check': '/api/health',
                'resources': {'cpu': '200m', 'memory': '512Mi'}
            },
            'nodejs_express': {
                'files': ['package.json', 'server.js', 'app.js'],
                'deployment_type': 'api',
                'port': 8080,
                'health_check': '/health',
                'resources': {'cpu': '200m', 'memory': '512Mi'}
            },
            'python_fastapi': {
                'files': ['main.py', 'requirements.txt', 'pyproject.toml'],
                'deployment_type': 'api',
                'port': 8000,
                'health_check': '/health',
                'resources': {'cpu': '250m', 'memory': '512Mi'}
            },
            'python_django': {
                'files': ['manage.py', 'settings.py', 'wsgi.py'],
                'deployment_type': 'web',
                'port': 8000,
                'health_check': '/health/',
                'resources': {'cpu': '300m', 'memory': '1Gi'}
            },
            'go': {
                'files': ['main.go', 'go.mod', 'go.sum'],
                'deployment_type': 'api',
                'port': 8080,
                'health_check': '/health',
                'resources': {'cpu': '100m', 'memory': '128Mi'}
            },
            'java_spring': {
                'files': ['pom.xml', 'build.gradle', 'src/main/java'],
                'deployment_type': 'api',
                'port': 8080,
                'health_check': '/actuator/health',
                'resources': {'cpu': '500m', 'memory': '1Gi'}
            },
            'dotnet': {
                'files': ['*.csproj', 'Program.cs', 'Startup.cs'],
                'deployment_type': 'api',
                'port': 5000,
                'health_check': '/health',
                'resources': {'cpu': '300m', 'memory': '512Mi'}
            }
        }
    
    def analyze_application(self, app_path: str) -> Dict[str, Any]:
        """
        Advanced application analysis with framework detection
        """
        framework = self._detect_framework(app_path)
        analysis = {
            'framework': framework,
            'app_type': self._detect_app_type(app_path),
            'services': self._identify_services(app_path),
            'dependencies': self._find_dependencies(app_path),
            'storage_needs': self._analyze_storage(app_path),
            'networking': self._analyze_networking(app_path),
            'resource_requirements': self._estimate_resources(app_path, framework),
            'security_requirements': self._analyze_security_needs(app_path),
            'observability_needs': self._analyze_observability(app_path),
            'scaling_strategy': self._recommend_scaling(app_path, framework)
        }
        
        return analysis
    
    def _detect_framework(self, app_path: str) -> str:
        """Detect application framework for optimized deployments"""
        app_path = Path(app_path)
        
        for framework, config in self.framework_patterns.items():
            if all((app_path / f).exists() for f in config['files'][:1]):
                if any((app_path / f).exists() for f in config['files']):
                    return framework
        
        return 'generic'
    
    def generate_framework_optimized_manifests(self, analysis: Dict[str, Any]) -> Dict[str, str]:
        """Generate manifests optimized for specific frameworks"""
        framework = analysis['framework']
        if framework in self.framework_patterns:
            return self._generate_specialized_manifests(framework, analysis)
        return self._generate_generic_manifests(analysis)
    
    def _detect_app_type(self, app_path):
        """Detect application type and stack"""
        indicators = {
            'web': ['nginx.conf', 'httpd.conf', 'index.html'],
            'api': ['app.py', 'server.js', 'main.go'],
            'database': ['postgresql.conf', 'my.cnf', 'mongod.conf'],
            'worker': ['worker.py', 'consumer.js', 'processor.go'],
            'frontend': ['package.json', 'webpack.config.js', 'angular.json']
        }
        
        detected_types = []
        for app_type, files in indicators.items():
            if any((Path(app_path) / f).exists() for f in files):
                detected_types.append(app_type)
                
        return detected_types
    
    def _identify_services(self, app_path):
        """Identify microservices structure"""
        services = []
        
        # Check docker-compose.yml
        compose_file = Path(app_path) / 'docker-compose.yml'
        if compose_file.exists():
            with open(compose_file) as f:
                compose = yaml.safe_load(f)
                for service_name, config in compose.get('services', {}).items():
                    services.append({
                        'name': service_name,
                        'image': config.get('image', 'custom'),
                        'ports': config.get('ports', []),
                        'environment': config.get('environment', {}),
                        'volumes': config.get('volumes', [])
                    })
        
        return services
```

### 2. Deployment Manifest Generation

Create production-ready Deployment manifests:

**Deployment Template**
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ${APP_NAME}
  namespace: ${NAMESPACE}
  labels:
    app: ${APP_NAME}
    version: ${VERSION}
    component: ${COMPONENT}
    managed-by: kubectl
spec:
  replicas: ${REPLICAS}
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  selector:
    matchLabels:
      app: ${APP_NAME}
      component: ${COMPONENT}
  template:
    metadata:
      labels:
        app: ${APP_NAME}
        version: ${VERSION}
        component: ${COMPONENT}
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "${METRICS_PORT}"
        prometheus.io/path: "/metrics"
    spec:
      serviceAccountName: ${APP_NAME}
      securityContext:
        runAsNonRoot: true
        runAsUser: 1000
        fsGroup: 1000
        seccompProfile:
          type: RuntimeDefault
      containers:
      - name: ${APP_NAME}
        image: ${IMAGE}:${TAG}
        imagePullPolicy: IfNotPresent
        ports:
        - name: http
          containerPort: ${PORT}
          protocol: TCP
        - name: metrics
          containerPort: ${METRICS_PORT}
          protocol: TCP
        env:
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        envFrom:
        - configMapRef:
            name: ${APP_NAME}-config
        - secretRef:
            name: ${APP_NAME}-secrets
        resources:
          requests:
            memory: "${MEMORY_REQUEST}"
            cpu: "${CPU_REQUEST}"
          limits:
            memory: "${MEMORY_LIMIT}"
            cpu: "${CPU_LIMIT}"
        livenessProbe:
          httpGet:
            path: /health
            port: http
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3
        readinessProbe:
          httpGet:
            path: /ready
            port: http
          initialDelaySeconds: 5
          periodSeconds: 5
          timeoutSeconds: 3
          failureThreshold: 3
        startupProbe:
          httpGet:
            path: /startup
            port: http
          initialDelaySeconds: 0
          periodSeconds: 10
          timeoutSeconds: 3
          failureThreshold: 30
        securityContext:
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
          runAsNonRoot: true
          runAsUser: 1000
          capabilities:
            drop:
            - ALL
        volumeMounts:
        - name: tmp
          mountPath: /tmp
        - name: cache
          mountPath: /app/cache
      volumes:
      - name: tmp
        emptyDir: {}
      - name: cache
        emptyDir: {}
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - ${APP_NAME}
              topologyKey: kubernetes.io/hostname
      topologySpreadConstraints:
      - maxSkew: 1
        topologyKey: topology.kubernetes.io/zone
        whenUnsatisfiable: DoNotSchedule
        labelSelector:
          matchLabels:
            app: ${APP_NAME}
```

### 3. Service and Networking

Generate Service and networking resources:

**Service Configuration**
```yaml
apiVersion: v1
kind: Service
metadata:
  name: ${APP_NAME}
  namespace: ${NAMESPACE}
  labels:
    app: ${APP_NAME}
    component: ${COMPONENT}
  annotations:
    service.beta.kubernetes.io/aws-load-balancer-type: "nlb"
spec:
  type: ClusterIP
  selector:
    app: ${APP_NAME}
    component: ${COMPONENT}
  ports:
  - name: http
    port: 80
    targetPort: http
    protocol: TCP
  - name: grpc
    port: 9090
    targetPort: grpc
    protocol: TCP
---
apiVersion: v1
kind: Service
metadata:
  name: ${APP_NAME}-headless
  namespace: ${NAMESPACE}
  labels:
    app: ${APP_NAME}
spec:
  type: ClusterIP
  clusterIP: None
  selector:
    app: ${APP_NAME}
  ports:
  - name: http
    port: 80
    targetPort: http
```

**Ingress Configuration**
```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: ${APP_NAME}
  namespace: ${NAMESPACE}
  annotations:
    cert-manager.io/cluster-issuer: letsencrypt-prod
    nginx.ingress.kubernetes.io/rate-limit: "100"
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
    nginx.ingress.kubernetes.io/force-ssl-redirect: "true"
spec:
  ingressClassName: nginx
  tls:
  - hosts:
    - ${DOMAIN}
    secretName: ${APP_NAME}-tls
  rules:
  - host: ${DOMAIN}
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: ${APP_NAME}
            port:
              name: http
```

### 4. Configuration Management

Create ConfigMaps and Secrets:

**ConfigMap Generator**
```python
def generate_configmap(app_name, config_data):
    """
    Generate ConfigMap manifest
    """
    configmap = {
        'apiVersion': 'v1',
        'kind': 'ConfigMap',
        'metadata': {
            'name': f'{app_name}-config',
            'namespace': 'default',
            'labels': {
                'app': app_name
            }
        },
        'data': {}
    }
    
    # Handle different config formats
    for key, value in config_data.items():
        if isinstance(value, dict):
            # Nested config as YAML
            configmap['data'][key] = yaml.dump(value)
        elif isinstance(value, list):
            # List as JSON
            configmap['data'][key] = json.dumps(value)
        else:
            # Plain string
            configmap['data'][key] = str(value)
    
    return yaml.dump(configmap)

def generate_secret(app_name, secret_data):
    """
    Generate Secret manifest
    """
    import base64
    
    secret = {
        'apiVersion': 'v1',
        'kind': 'Secret',
        'metadata': {
            'name': f'{app_name}-secrets',
            'namespace': 'default',
            'labels': {
                'app': app_name
            }
        },
        'type': 'Opaque',
        'data': {}
    }
    
    # Base64 encode all values
    for key, value in secret_data.items():
        encoded = base64.b64encode(value.encode()).decode()
        secret['data'][key] = encoded
    
    return yaml.dump(secret)
```

### 5. Persistent Storage

Configure persistent volumes:

**StatefulSet with Storage**
```yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: ${APP_NAME}
  namespace: ${NAMESPACE}
spec:
  serviceName: ${APP_NAME}-headless
  replicas: ${REPLICAS}
  selector:
    matchLabels:
      app: ${APP_NAME}
  template:
    metadata:
      labels:
        app: ${APP_NAME}
    spec:
      containers:
      - name: ${APP_NAME}
        image: ${IMAGE}:${TAG}
        ports:
        - containerPort: ${PORT}
          name: http
        volumeMounts:
        - name: data
          mountPath: /data
        - name: config
          mountPath: /etc/config
          readOnly: true
      volumes:
      - name: config
        configMap:
          name: ${APP_NAME}-config
  volumeClaimTemplates:
  - metadata:
      name: data
      labels:
        app: ${APP_NAME}
    spec:
      accessModes: ["ReadWriteOnce"]
      storageClassName: ${STORAGE_CLASS}
      resources:
        requests:
          storage: ${STORAGE_SIZE}
```

### 6. Helm Chart Generation

Create production Helm charts:

**Chart Structure**
```bash
#!/bin/bash
# generate-helm-chart.sh

create_helm_chart() {
    local chart_name="$1"
    
    mkdir -p "$chart_name"/{templates,charts}
    
    # Chart.yaml
    cat > "$chart_name/Chart.yaml" << EOF
apiVersion: v2
name: $chart_name
description: A Helm chart for $chart_name
type: application
version: 0.1.0
appVersion: "1.0.0"
keywords:
  - $chart_name
home: https://github.com/org/$chart_name
sources:
  - https://github.com/org/$chart_name
maintainers:
  - name: Team Name
    email: team@example.com
dependencies: []
EOF

    # values.yaml
    cat > "$chart_name/values.yaml" << 'EOF'
# Default values for the application
replicaCount: 2

image:
  repository: myapp
  pullPolicy: IfNotPresent
  tag: ""

imagePullSecrets: []
nameOverride: ""
fullnameOverride: ""

serviceAccount:
  create: true
  annotations: {}
  name: ""

podAnnotations: {}
podSecurityContext:
  fsGroup: 2000
  runAsNonRoot: true
  runAsUser: 1000

securityContext:
  allowPrivilegeEscalation: false
  capabilities:
    drop:
    - ALL
  readOnlyRootFilesystem: true
  runAsNonRoot: true
  runAsUser: 1000

service:
  type: ClusterIP
  port: 80
  targetPort: 8080

ingress:
  enabled: false
  className: "nginx"
  annotations:
    cert-manager.io/cluster-issuer: letsencrypt-prod
  hosts:
    - host: chart-example.local
      paths:
        - path: /
          pathType: ImplementationSpecific
  tls: []

resources:
  limits:
    cpu: 500m
    memory: 512Mi
  requests:
    cpu: 250m
    memory: 256Mi

autoscaling:
  enabled: true
  minReplicas: 2
  maxReplicas: 10
  targetCPUUtilizationPercentage: 80
  targetMemoryUtilizationPercentage: 80

persistence:
  enabled: false
  storageClass: ""
  accessMode: ReadWriteOnce
  size: 8Gi

nodeSelector: {}
tolerations: []
affinity: {}

# Application config
config:
  logLevel: info
  debug: false

# Secrets - use external secrets in production
secrets: {}

# Health check paths
healthcheck:
  liveness:
    path: /health
    initialDelaySeconds: 30
    periodSeconds: 10
  readiness:
    path: /ready
    initialDelaySeconds: 5
    periodSeconds: 5
EOF

    # _helpers.tpl
    cat > "$chart_name/templates/_helpers.tpl" << 'EOF'
{{/*
Expand the name of the chart.
*/}}
{{- define "app.name" -}}
{{- default .Chart.Name .Values.nameOverride | trunc 63 | trimSuffix "-" }}
{{- end }}

{{/*
Create a default fully qualified app name.
*/}}
{{- define "app.fullname" -}}
{{- if .Values.fullnameOverride }}
{{- .Values.fullnameOverride | trunc 63 | trimSuffix "-" }}
{{- else }}
{{- $name := default .Chart.Name .Values.nameOverride }}
{{- if contains $name .Release.Name }}
{{- .Release.Name | trunc 63 | trimSuffix "-" }}
{{- else }}
{{- printf "%s-%s" .Release.Name $name | trunc 63 | trimSuffix "-" }}
{{- end }}
{{- end }}
{{- end }}

{{/*
Create chart name and version as used by the chart label.
*/}}
{{- define "app.chart" -}}
{{- printf "%s-%s" .Chart.Name .Chart.Version | replace "+" "_" | trunc 63 | trimSuffix "-" }}
{{- end }}

{{/*
Common labels
*/}}
{{- define "app.labels" -}}
helm.sh/chart: {{ include "app.chart" . }}
{{ include "app.selectorLabels" . }}
{{- if .Chart.AppVersion }}
app.kubernetes.io/version: {{ .Chart.AppVersion | quote }}
{{- end }}
app.kubernetes.io/managed-by: {{ .Release.Service }}
{{- end }}

{{/*
Selector labels
*/}}
{{- define "app.selectorLabels" -}}
app.kubernetes.io/name: {{ include "app.name" . }}
app.kubernetes.io/instance: {{ .Release.Name }}
{{- end }}

{{/*
Create the name of the service account to use
*/}}
{{- define "app.serviceAccountName" -}}
{{- if .Values.serviceAccount.create }}
{{- default (include "app.fullname" .) .Values.serviceAccount.name }}
{{- else }}
{{- default "default" .Values.serviceAccount.name }}
{{- end }}
{{- end }}
EOF
}
```

### 7. Advanced Multi-Environment Configuration

Handle environment-specific configurations with GitOps:

**FluxCD GitOps Setup**
```yaml
# infrastructure/flux-system/gotk-sync.yaml
apiVersion: source.toolkit.fluxcd.io/v1beta2
kind: GitRepository
metadata:
  name: flux-system
  namespace: flux-system
spec:
  interval: 1m0s
  ref:
    branch: main
  url: https://github.com/org/k8s-gitops
---
apiVersion: kustomize.toolkit.fluxcd.io/v1beta2
kind: Kustomization
metadata:
  name: flux-system
  namespace: flux-system
spec:
  interval: 10m0s
  path: "./clusters/production"
  prune: true
  sourceRef:
    kind: GitRepository
    name: flux-system
  validation: client
  healthChecks:
    - apiVersion: apps/v1
      kind: Deployment
      name: myapp
      namespace: production

---
# Advanced Kustomization with Helm
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization

resources:
  - namespace.yaml
  - ../../base
  - monitoring/
  - security/

namespace: production

helmCharts:
  - name: prometheus
    repo: https://prometheus-community.github.io/helm-charts
    version: 15.0.0
    releaseName: prometheus
    namespace: monitoring
    valuesInline:
      server:
        persistentVolume:
          size: 50Gi
      alertmanager:
        enabled: true

patchesStrategicMerge:
  - deployment-patch.yaml
  - service-patch.yaml

patchesJson6902:
  - target:
      version: v1
      kind: Deployment
      name: myapp
    patch: |
      - op: replace
        path: /spec/replicas
        value: 10
      - op: add
        path: /spec/template/spec/containers/0/resources/limits/nvidia.com~1gpu
        value: "1"

configMapGenerator:
  - name: app-config
    behavior: merge
    literals:
      - ENV=production
      - LOG_LEVEL=warn
      - DATABASE_POOL_SIZE=20
      - CACHE_TTL=3600
    files:
      - config/production.yaml

secretGenerator:
  - name: app-secrets
    behavior: replace
    type: Opaque
    options:
      disableNameSuffixHash: true
    files:
      - .env.production

replicas:
  - name: myapp
    count: 10

images:
  - name: myapp
    newTag: v1.2.3

commonLabels:
  app: myapp
  env: production
  version: v1.2.3

commonAnnotations:
  deployment.kubernetes.io/revision: "1"
  prometheus.io/scrape: "true"

resources:
  - hpa.yaml
  - vpa.yaml
  - pdb.yaml
  - networkpolicy.yaml
  - servicemonitor.yaml
  - backup.yaml
```

### 8. Advanced Security Manifests

Create comprehensive security-focused resources:

**Pod Security Standards (PSS)**
```yaml
# Namespace with Pod Security Standards
apiVersion: v1
kind: Namespace
metadata:
  name: ${NAMESPACE}
  labels:
    pod-security.kubernetes.io/enforce: restricted
    pod-security.kubernetes.io/audit: restricted
    pod-security.kubernetes.io/warn: restricted
    name: ${NAMESPACE}
---
# Advanced Network Policy
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: ${APP_NAME}-netpol
  namespace: ${NAMESPACE}
spec:
  podSelector:
    matchLabels:
      app: ${APP_NAME}
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          name: ingress-nginx
      podSelector: {}
    - namespaceSelector:
        matchLabels:
          name: monitoring
      podSelector:
        matchLabels:
          app: prometheus
    ports:
    - protocol: TCP
      port: 8080
    - protocol: TCP
      port: 9090  # metrics
  egress:
  # Database access
  - to:
    - namespaceSelector:
        matchLabels:
          name: database
    ports:
    - protocol: TCP
      port: 5432
    - protocol: TCP
      port: 6379  # Redis
  # External API access
  - to: []
    ports:
    - protocol: TCP
      port: 443
    - protocol: TCP
      port: 80
  # DNS resolution
  - to:
    - namespaceSelector:
        matchLabels:
          name: kube-system
      podSelector:
        matchLabels:
          k8s-app: kube-dns
    ports:
    - protocol: UDP
      port: 53
    - protocol: TCP
      port: 53
---
# Open Policy Agent Gatekeeper Constraints
apiVersion: templates.gatekeeper.sh/v1beta1
kind: ConstraintTemplate
metadata:
  name: requiredlabels
spec:
  crd:
    spec:
      names:
        kind: RequiredLabels
      validation:
        properties:
          labels:
            type: array
            items:
              type: string
  targets:
    - target: admission.k8s.gatekeeper.sh
      rego: |
        package requiredlabels
        
        violation[{"msg": msg}] {
          required := input.parameters.labels
          provided := input.review.object.metadata.labels
          missing := required[_]
          not provided[missing]
          msg := sprintf("Missing required label: %v", [missing])
        }
---
apiVersion: constraints.gatekeeper.sh/v1beta1
kind: RequiredLabels
metadata:
  name: must-have-app-label
spec:
  match:
    kinds:
      - apiGroups: ["apps"]
        kinds: ["Deployment"]
  parameters:
    labels: ["app", "version", "component"]
---
# Falco Security Rules
apiVersion: v1
kind: ConfigMap
metadata:
  name: falco-rules
  namespace: falco
data:
  application_rules.yaml: |
    - rule: Suspicious Network Activity
      desc: Detect suspicious network connections
      condition: >
        (spawned_process and container and
         ((proc.name in (nc, ncat, netcat, netcat.traditional) and
           proc.args contains "-l") or
          (proc.name = socat and proc.args contains "TCP-LISTEN")))
      output: >
        Suspicious network tool launched in container
        (user=%user.name command=%proc.cmdline image=%container.image.repository)
      priority: WARNING
      tags: [network, mitre_lateral_movement]
    
    - rule: Unexpected Outbound Connection
      desc: An unexpected outbound connection was established
      condition: >
        outbound and not proc.name in (known_outbound_processes) and
        not fd.sip in (allowed_external_ips)
      output: >
        Unexpected outbound connection
        (command=%proc.cmdline connection=%fd.name user=%user.name)
      priority: WARNING
      tags: [network, mitre_exfiltration]
---
# Service Mesh Security (Istio)
apiVersion: security.istio.io/v1beta1
kind: AuthorizationPolicy
metadata:
  name: ${APP_NAME}-authz
  namespace: ${NAMESPACE}
spec:
  selector:
    matchLabels:
      app: ${APP_NAME}
  rules:
  - from:
    - source:
        principals: ["cluster.local/ns/frontend/sa/frontend"]
    to:
    - operation:
        methods: ["GET", "POST"]
        paths: ["/api/*"]
  - from:
    - source:
        principals: ["cluster.local/ns/monitoring/sa/prometheus"]
    to:
    - operation:
        methods: ["GET"]
        paths: ["/metrics"]
---
apiVersion: security.istio.io/v1beta1
kind: PeerAuthentication
metadata:
  name: ${APP_NAME}-peer-authn
  namespace: ${NAMESPACE}
spec:
  selector:
    matchLabels:
      app: ${APP_NAME}
  mtls:
    mode: STRICT
```

### 9. Advanced Observability Setup

Configure comprehensive monitoring, logging, and tracing:

**OpenTelemetry Integration**
```yaml
# OpenTelemetry Collector
apiVersion: opentelemetry.io/v1alpha1
kind: OpenTelemetryCollector
metadata:
  name: otel-collector
  namespace: ${NAMESPACE}
spec:
  mode: daemonset
  serviceAccount: otel-collector
  config: |
    receivers:
      otlp:
        protocols:
          grpc:
            endpoint: 0.0.0.0:4317
          http:
            endpoint: 0.0.0.0:4318
      prometheus:
        config:
          scrape_configs:
            - job_name: 'kubernetes-pods'
              kubernetes_sd_configs:
                - role: pod
      k8s_cluster:
        auth_type: serviceAccount
      kubeletstats:
        collection_interval: 20s
        auth_type: "serviceAccount"
        endpoint: "${env:K8S_NODE_NAME}:10250"
        insecure_skip_verify: true
    
    processors:
      batch:
        timeout: 1s
        send_batch_size: 1024
      memory_limiter:
        limit_mib: 512
      k8sattributes:
        auth_type: "serviceAccount"
        passthrough: false
        filter:
          node_from_env_var: KUBE_NODE_NAME
        extract:
          metadata:
            - k8s.pod.name
            - k8s.pod.uid
            - k8s.deployment.name
            - k8s.namespace.name
            - k8s.node.name
            - k8s.pod.start_time
    
    exporters:
      prometheus:
        endpoint: "0.0.0.0:8889"
      jaeger:
        endpoint: jaeger-collector:14250
        tls:
          insecure: true
      loki:
        endpoint: http://loki:3100/loki/api/v1/push
    
    service:
      pipelines:
        traces:
          receivers: [otlp]
          processors: [memory_limiter, k8sattributes, batch]
          exporters: [jaeger]
        metrics:
          receivers: [otlp, prometheus, k8s_cluster, kubeletstats]
          processors: [memory_limiter, k8sattributes, batch]
          exporters: [prometheus]
        logs:
          receivers: [otlp]
          processors: [memory_limiter, k8sattributes, batch]
          exporters: [loki]
---
# Enhanced ServiceMonitor
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: ${APP_NAME}
  namespace: ${NAMESPACE}
  labels:
    app: ${APP_NAME}
    prometheus: kube-prometheus
spec:
  selector:
    matchLabels:
      app: ${APP_NAME}
  endpoints:
  - port: metrics
    interval: 15s
    path: /metrics
    honorLabels: true
    metricRelabelings:
    - sourceLabels: [__name__]
      regex: 'go_.*'
      action: drop
    - sourceLabels: [__name__]
      regex: 'promhttp_.*'
      action: drop
    relabelings:
    - sourceLabels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
      action: keep
      regex: true
    - sourceLabels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
      action: replace
      targetLabel: __metrics_path__
      regex: (.+)
    - sourceLabels: [__meta_kubernetes_pod_ip]
      action: replace
      targetLabel: __address__
      regex: (.*)
      replacement: $1:9090
---
# Custom Prometheus Rules
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: ${APP_NAME}-rules
  namespace: ${NAMESPACE}
spec:
  groups:
  - name: ${APP_NAME}.rules
    rules:
    - alert: HighErrorRate
      expr: |
        (
          rate(http_requests_total{job="${APP_NAME}",status=~"5.."}[5m])
          /
          rate(http_requests_total{job="${APP_NAME}"}[5m])
        ) > 0.05
      for: 5m
      labels:
        severity: warning
        service: ${APP_NAME}
      annotations:
        summary: "High error rate detected"
        description: "Error rate is {{ $value | humanizePercentage }} for {{ $labels.job }}"
    
    - alert: HighResponseTime
      expr: |
        histogram_quantile(0.95,
          rate(http_request_duration_seconds_bucket{job="${APP_NAME}"}[5m])
        ) > 0.5
      for: 5m
      labels:
        severity: warning
        service: ${APP_NAME}
      annotations:
        summary: "High response time detected"
        description: "95th percentile response time is {{ $value }}s for {{ $labels.job }}"
    
    - alert: PodCrashLooping
      expr: |
        increase(kube_pod_container_status_restarts_total{pod=~"${APP_NAME}-.*"}[1h]) > 5
      for: 5m
      labels:
        severity: critical
        service: ${APP_NAME}
      annotations:
        summary: "Pod is crash looping"
        description: "Pod {{ $labels.pod }} has restarted {{ $value }} times in the last hour"
---
# Grafana Dashboard ConfigMap
apiVersion: v1
kind: ConfigMap
metadata:
  name: ${APP_NAME}-dashboard
  namespace: monitoring
  labels:
    grafana_dashboard: "1"
data:
  dashboard.json: |
    {
      "dashboard": {
        "title": "${APP_NAME} Dashboard",
        "panels": [
          {
            "title": "Request Rate",
            "type": "graph",
            "targets": [
              {
                "expr": "rate(http_requests_total{job=\"${APP_NAME}\"}[5m])",
                "legendFormat": "{{ $labels.method }} {{ $labels.status }}"
              }
            ]
          },
          {
            "title": "Response Time",
            "type": "graph", 
            "targets": [
              {
                "expr": "histogram_quantile(0.95, rate(http_request_duration_seconds_bucket{job=\"${APP_NAME}\"}[5m]))",
                "legendFormat": "95th percentile"
              },
              {
                "expr": "histogram_quantile(0.50, rate(http_request_duration_seconds_bucket{job=\"${APP_NAME}\"}[5m]))",
                "legendFormat": "50th percentile"
              }
            ]
          }
        ]
      }
    }
```

### 10. Advanced GitOps Integration

Prepare manifests for enterprise GitOps:

**Multi-Cluster ArgoCD Application**
```yaml
# Application Set for Multi-Environment Deployment
apiVersion: argoproj.io/v1alpha1
kind: ApplicationSet
metadata:
  name: ${APP_NAME}-appset
  namespace: argocd
spec:
  generators:
  - clusters:
      selector:
        matchLabels:
          argocd.argoproj.io/secret-type: cluster
  - git:
      repoURL: https://github.com/org/k8s-manifests
      revision: HEAD
      directories:
      - path: apps/${APP_NAME}/overlays/*
  template:
    metadata:
      name: '${APP_NAME}-{{path.basename}}'
      labels:
        app: ${APP_NAME}
        env: '{{path.basename}}'
    spec:
      project: default
      source:
        repoURL: https://github.com/org/k8s-manifests
        targetRevision: HEAD
        path: 'apps/${APP_NAME}/overlays/{{path.basename}}'
      destination:
        server: '{{server}}'
        namespace: '${APP_NAME}-{{path.basename}}'
      syncPolicy:
        automated:
          prune: true
          selfHeal: true
          allowEmpty: false
        syncOptions:
        - CreateNamespace=true
        - PrunePropagationPolicy=foreground
        - RespectIgnoreDifferences=true
        - ApplyOutOfSyncOnly=true
        managedNamespaceMetadata:
          labels:
            pod-security.kubernetes.io/enforce: restricted
            managed-by: argocd
        retry:
          limit: 5
          backoff:
            duration: 5s
            factor: 2
            maxDuration: 3m
      ignoreDifferences:
      - group: apps
        kind: Deployment
        jsonPointers:
        - /spec/replicas
      - group: autoscaling
        kind: HorizontalPodAutoscaler
        jsonPointers:
        - /spec/minReplicas
        - /spec/maxReplicas
---
# Progressive Rollout with Argo Rollouts
apiVersion: argoproj.io/v1alpha1
kind: Rollout
metadata:
  name: ${APP_NAME}
  namespace: ${NAMESPACE}
spec:
  replicas: 10
  strategy:
    canary:
      maxSurge: "25%"
      maxUnavailable: 0
      analysis:
        templates:
        - templateName: success-rate
        startingStep: 2
        args:
        - name: service-name
          value: ${APP_NAME}
      steps:
      - setWeight: 10
      - pause: {duration: 60s}
      - setWeight: 20
      - pause: {duration: 60s}
      - analysis:
          templates:
          - templateName: success-rate
          args:
          - name: service-name
            value: ${APP_NAME}
      - setWeight: 40
      - pause: {duration: 60s}
      - setWeight: 60
      - pause: {duration: 60s}
      - setWeight: 80
      - pause: {duration: 60s}
      trafficRouting:
        istio:
          virtualService:
            name: ${APP_NAME}
            routes:
            - primary
          destinationRule:
            name: ${APP_NAME}
            canarySubsetName: canary
            stableSubsetName: stable
  selector:
    matchLabels:
      app: ${APP_NAME}
  template:
    metadata:
      labels:
        app: ${APP_NAME}
    spec:
      containers:
      - name: ${APP_NAME}
        image: ${IMAGE}:${TAG}
        ports:
        - containerPort: 8080
        resources:
          requests:
            memory: "256Mi"
            cpu: "250m"
          limits:
            memory: "512Mi"
            cpu: "500m"
---
# Analysis Template for Rollouts
apiVersion: argoproj.io/v1alpha1
kind: AnalysisTemplate
metadata:
  name: success-rate
  namespace: ${NAMESPACE}
spec:
  args:
  - name: service-name
  metrics:
  - name: success-rate
    interval: 60s
    count: 5
    successCondition: result[0] >= 0.95
    failureLimit: 3
    provider:
      prometheus:
        address: http://prometheus:9090
        query: |
          sum(
            rate(http_requests_total{job="{{args.service-name}}",status!~"5.."}[5m])
          ) /
          sum(
            rate(http_requests_total{job="{{args.service-name}}"}[5m])
          )
---
# Multi-Cluster Service Mirror (Linkerd)
apiVersion: linkerd.io/v1alpha2
kind: Link
metadata:
  name: ${APP_NAME}-west
  namespace: ${NAMESPACE}
  annotations:
    multicluster.linkerd.io/target-cluster-name: west
spec:
  targetClusterName: west
  targetClusterDomain: cluster.local
  selector:
    matchLabels:
      app: ${APP_NAME}
      mirror.linkerd.io/exported: "true"
```

### 11. Validation and Testing

Validate generated manifests:

**Manifest Validation Script**
```python
#!/usr/bin/env python3
import yaml
import sys
from kubernetes import client, config
from kubernetes.client.rest import ApiException

class ManifestValidator:
    def __init__(self):
        try:
            config.load_incluster_config()
        except:
            config.load_kube_config()
        
        self.api_client = client.ApiClient()
    
    def validate_manifest(self, manifest_file):
        """
        Validate Kubernetes manifest
        """
        with open(manifest_file) as f:
            manifests = list(yaml.safe_load_all(f))
        
        results = []
        for manifest in manifests:
            result = {
                'kind': manifest.get('kind'),
                'name': manifest.get('metadata', {}).get('name'),
                'valid': False,
                'errors': []
            }
            
            # Dry run validation
            try:
                self._dry_run_apply(manifest)
                result['valid'] = True
            except ApiException as e:
                result['errors'].append(str(e))
            
            # Security checks
            security_issues = self._check_security(manifest)
            if security_issues:
                result['errors'].extend(security_issues)
            
            # Best practices checks
            bp_issues = self._check_best_practices(manifest)
            if bp_issues:
                result['errors'].extend(bp_issues)
            
            results.append(result)
        
        return results
    
    def _check_security(self, manifest):
        """Check security best practices"""
        issues = []
        
        if manifest.get('kind') == 'Deployment':
            spec = manifest.get('spec', {}).get('template', {}).get('spec', {})
            
            # Check security context
            if not spec.get('securityContext'):
                issues.append("Missing pod security context")
            
            # Check container security
            for container in spec.get('containers', []):
                if not container.get('securityContext'):
                    issues.append(f"Container {container['name']} missing security context")
                
                sec_ctx = container.get('securityContext', {})
                if not sec_ctx.get('runAsNonRoot'):
                    issues.append(f"Container {container['name']} not configured to run as non-root")
                
                if not sec_ctx.get('readOnlyRootFilesystem'):
                    issues.append(f"Container {container['name']} has writable root filesystem")
        
        return issues
```

### 11. Advanced Scaling and Performance

Implement intelligent scaling strategies:

**KEDA Autoscaling**
```yaml
apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
  name: ${APP_NAME}-scaler
  namespace: ${NAMESPACE}
spec:
  scaleTargetRef:
    name: ${APP_NAME}
  pollingInterval: 30
  cooldownPeriod: 300
  idleReplicaCount: 2
  minReplicaCount: 2
  maxReplicaCount: 50
  fallback:
    failureThreshold: 3
    replicas: 5
  triggers:
  - type: prometheus
    metadata:
      serverAddress: http://prometheus:9090
      metricName: http_requests_per_second
      threshold: '100'
      query: sum(rate(http_requests_total{job="${APP_NAME}"}[2m]))
  - type: memory
    metadata:
      type: Utilization
      value: "70"
  - type: cpu
    metadata:
      type: Utilization
      value: "70"
---
# Vertical Pod Autoscaler
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: ${APP_NAME}-vpa
  namespace: ${NAMESPACE}
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: ${APP_NAME}
  updatePolicy:
    updateMode: "Auto"
    minReplicas: 2
  resourcePolicy:
    containerPolicies:
    - containerName: ${APP_NAME}
      minAllowed:
        cpu: 100m
        memory: 128Mi
      maxAllowed:
        cpu: 2
        memory: 4Gi
      controlledResources: ["cpu", "memory"]
      controlledValues: RequestsAndLimits
---
# Pod Disruption Budget
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: ${APP_NAME}-pdb
  namespace: ${NAMESPACE}
spec:
  minAvailable: 2
  selector:
    matchLabels:
      app: ${APP_NAME}
```

### 12. CI/CD Integration

Modern deployment pipeline integration:

**GitHub Actions Workflow**
```yaml
# .github/workflows/deploy.yml
name: Deploy to Kubernetes

on:
  push:
    branches: [main]
    paths: ['src/**', 'k8s/**', 'Dockerfile']
  pull_request:
    branches: [main]

env:
  REGISTRY: ghcr.io
  IMAGE_NAME: ${{ github.repository }}

jobs:
  build-and-deploy:
    runs-on: ubuntu-latest
    permissions:
      contents: read
      packages: write
      id-token: write
    
    steps:
    - name: Checkout
      uses: actions/checkout@v4
      with:
        fetch-depth: 0
    
    - name: Setup GitVersion
      uses: gittools/actions/gitversion/setup@v0.9.15
      with:
        versionSpec: '5.x'
    
    - name: Determine Version
      uses: gittools/actions/gitversion/execute@v0.9.15
      id: gitversion
    
    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3
    
    - name: Log in to Container Registry
      uses: docker/login-action@v3
      with:
        registry: ${{ env.REGISTRY }}
        username: ${{ github.actor }}
        password: ${{ secrets.GITHUB_TOKEN }}
    
    - name: Build and push Docker image
      uses: docker/build-push-action@v5
      with:
        context: .
        platforms: linux/amd64,linux/arm64
        push: true
        tags: |
          ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:${{ steps.gitversion.outputs.semVer }}
          ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:latest
        cache-from: type=gha
        cache-to: type=gha,mode=max
        build-args: |
          VERSION=${{ steps.gitversion.outputs.semVer }}
          COMMIT_SHA=${{ github.sha }}
    
    - name: Run Trivy vulnerability scanner
      uses: aquasecurity/trivy-action@master
      with:
        image-ref: '${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:${{ steps.gitversion.outputs.semVer }}'
        format: 'sarif'
        output: 'trivy-results.sarif'
    
    - name: Upload Trivy scan results
      uses: github/codeql-action/upload-sarif@v2
      with:
        sarif_file: 'trivy-results.sarif'
    
    - name: Install kubectl and kustomize
      run: |
        curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
        chmod +x kubectl && sudo mv kubectl /usr/local/bin/
        curl -s "https://raw.githubusercontent.com/kubernetes-sigs/kustomize/master/hack/install_kustomize.sh" | bash
        sudo mv kustomize /usr/local/bin/
    
    - name: Validate Kubernetes manifests
      run: |
        kubectl --dry-run=client --validate=true apply -k k8s/overlays/staging
    
    - name: Deploy to staging
      if: github.ref == 'refs/heads/main'
      run: |
        cd k8s/overlays/staging
        kustomize edit set image app=${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:${{ steps.gitversion.outputs.semVer }}
        kubectl apply -k .
        kubectl rollout status deployment/${APP_NAME} -n staging --timeout=300s
    
    - name: Run integration tests
      if: github.ref == 'refs/heads/main'
      run: |
        # Wait for deployment to be ready
        kubectl wait --for=condition=available --timeout=300s deployment/${APP_NAME} -n staging
        # Run tests
        npm run test:integration
    
    - name: Deploy to production
      if: github.ref == 'refs/heads/main' && success()
      run: |
        cd k8s/overlays/production
        kustomize edit set image app=${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:${{ steps.gitversion.outputs.semVer }}
        kubectl apply -k .
        kubectl rollout status deployment/${APP_NAME} -n production --timeout=600s
```

## Output Format

1. **Framework-Optimized Manifests**: Tailored deployment configurations
2. **Advanced Security Bundle**: PSS, OPA, Falco, service mesh policies
3. **GitOps Repository Structure**: Multi-environment with FluxCD/ArgoCD
4. **Observability Stack**: OpenTelemetry, Prometheus, Grafana, Jaeger
5. **Progressive Delivery Setup**: Argo Rollouts with canary deployment
6. **Auto-scaling Configuration**: HPA, VPA, KEDA for intelligent scaling
7. **Multi-Cluster Setup**: Service mesh and cross-cluster communication
8. **CI/CD Pipeline**: Complete GitHub Actions workflow with security scanning
9. **Disaster Recovery Plan**: Backup strategies and restoration procedures
10. **Performance Benchmarks**: Load testing and optimization recommendations

## Cross-Command Integration

### Complete Cloud-Native Deployment Workflow

**Enterprise Kubernetes Pipeline**
```bash
# 1. Generate cloud-native API scaffolding
/api-scaffold
framework: "fastapi"
deployment_target: "kubernetes"
cloud_native: true
observability: ["prometheus", "jaeger", "grafana"]

# 2. Optimize containers for Kubernetes
/docker-optimize
optimization_level: "kubernetes"
multi_arch_build: true
security_hardening: true

# 3. Comprehensive security scanning
/security-scan
scan_types: ["k8s", "container", "iac", "rbac"]
compliance: ["cis", "nsa", "pci"]

# 4. Generate production K8s manifests
/k8s-manifest
environment: "production"
security_level: "enterprise"
auto_scaling: true
service_mesh: true
```

**Integrated Kubernetes Configuration**
```python
# k8s-integration-config.py - Shared across all commands
class IntegratedKubernetesConfig:
    def __init__(self):
        self.api_config = self.load_api_config()           # From /api-scaffold
        self.container_config = self.load_container_config() # From /docker-optimize
        self.security_config = self.load_security_config() # From /security-scan
        self.test_config = self.load_test_config()         # From /test-harness
        
    def generate_application_manifests(self):
        """Generate complete K8s manifests for the application stack"""
        manifests = {
            'namespace': self.generate_namespace_manifest(),
            'secrets': self.generate_secrets_manifests(),
            'configmaps': self.generate_configmap_manifests(),
            'deployments': self.generate_deployment_manifests(),
            'services': self.generate_service_manifests(),
            'ingress': self.generate_ingress_manifests(),
            'security': self.generate_security_manifests(),
            'monitoring': self.generate_monitoring_manifests(),
            'autoscaling': self.generate_autoscaling_manifests()
        }
        return manifests
    
    def generate_deployment_manifests(self):
        """Generate deployment manifests from API and container configs"""
        deployments = []
        
        # API deployment
        if self.api_config.get('framework'):
            api_deployment = {
                'apiVersion': 'apps/v1',
                'kind': 'Deployment',
                'metadata': {
                    'name': f"{self.api_config['name']}-api",
                    'namespace': self.api_config.get('namespace', 'default'),
                    'labels': {
                        'app': f"{self.api_config['name']}-api",
                        'framework': self.api_config['framework'],
                        'version': self.api_config.get('version', 'v1.0.0'),
                        'component': 'backend'
                    }
                },
                'spec': {
                    'replicas': self.calculate_replica_count(),
                    'selector': {
                        'matchLabels': {
                            'app': f"{self.api_config['name']}-api"
                        }
                    },
                    'template': {
                        'metadata': {
                            'labels': {
                                'app': f"{self.api_config['name']}-api"
                            },
                            'annotations': self.generate_pod_annotations()
                        },
                        'spec': self.generate_pod_spec()
                    }
                }
            }
            deployments.append(api_deployment)
        
        return deployments
    
    def generate_pod_spec(self):
        """Generate optimized pod specification"""
        containers = []
        
        # Main application container
        app_container = {
            'name': 'app',
            'image': self.container_config.get('image_name', 'app:latest'),
            'imagePullPolicy': 'Always',
            'ports': [
                {
                    'name': 'http',
                    'containerPort': self.api_config.get('port', 8000),
                    'protocol': 'TCP'
                }
            ],
            'env': self.generate_environment_variables(),
            'resources': self.calculate_resource_requirements(),
            'securityContext': self.generate_security_context(),
            'livenessProbe': self.generate_health_probes('liveness'),
            'readinessProbe': self.generate_health_probes('readiness'),
            'startupProbe': self.generate_health_probes('startup'),
            'volumeMounts': self.generate_volume_mounts()
        }
        containers.append(app_container)
        
        # Sidecar containers (monitoring, security, etc.)
        if self.should_include_monitoring_sidecar():
            containers.append(self.generate_monitoring_sidecar())
        
        if self.should_include_security_sidecar():
            containers.append(self.generate_security_sidecar())
        
        pod_spec = {
            'serviceAccountName': f"{self.api_config['name']}-sa",
            'securityContext': self.generate_pod_security_context(),
            'containers': containers,
            'volumes': self.generate_volumes(),
            'initContainers': self.generate_init_containers(),
            'nodeSelector': self.generate_node_selector(),
            'tolerations': self.generate_tolerations(),
            'affinity': self.generate_affinity_rules(),
            'topologySpreadConstraints': self.generate_topology_constraints()
        }
        
        return pod_spec
    
    def generate_security_context(self):
        """Generate container security context from security scan results"""
        security_level = self.security_config.get('level', 'standard')
        
        base_context = {
            'allowPrivilegeEscalation': False,
            'readOnlyRootFilesystem': True,
            'runAsNonRoot': True,
            'runAsUser': 1001,
            'capabilities': {
                'drop': ['ALL']
            }
        }
        
        if security_level == 'enterprise':
            base_context.update({
                'seccompProfile': {'type': 'RuntimeDefault'},
                'capabilities': {
                    'drop': ['ALL'],
                    'add': ['NET_BIND_SERVICE'] if self.api_config.get('privileged_port') else []
                }
            })
        
        return base_context
```

**Database Integration with Kubernetes**
```yaml
# database-k8s-manifests.yaml - From /db-migrate + /k8s-manifest
apiVersion: v1
kind: Secret
metadata:
  name: database-credentials
  namespace: production
type: Opaque
data:
  username: cG9zdGdyZXM=  # postgres (base64)
  password: <ENCODED_PASSWORD>
  database: YXBwX2Ri  # app_db (base64)

---
apiVersion: v1
kind: ConfigMap
metadata:
  name: database-config
  namespace: production
data:
  postgresql.conf: |
    # Performance tuning from /db-migrate analysis
    shared_buffers = 256MB
    effective_cache_size = 1GB
    work_mem = 4MB
    maintenance_work_mem = 64MB
    
    # Security settings from /security-scan
    ssl = on
    log_connections = on
    log_disconnections = on
    log_statement = 'all'
    
    # Monitoring settings
    shared_preload_libraries = 'pg_stat_statements'
    track_activity_query_size = 2048

---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: database-pvc
  namespace: production
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: fast-ssd
  resources:
    requests:
      storage: 100Gi

---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: database
  namespace: production
spec:
  serviceName: database-headless
  replicas: 3  # High availability setup
  selector:
    matchLabels:
      app: database
  template:
    metadata:
      labels:
        app: database
    spec:
      serviceAccountName: database-sa
      securityContext:
        runAsUser: 999
        runAsGroup: 999
        fsGroup: 999
      containers:
      - name: postgresql
        image: postgres:15-alpine
        ports:
        - name: postgresql
          containerPort: 5432
        env:
        - name: POSTGRES_USER
          valueFrom:
            secretKeyRef:
              name: database-credentials
              key: username
        - name: POSTGRES_PASSWORD
          valueFrom:
            secretKeyRef:
              name: database-credentials
              key: password
        - name: POSTGRES_DB
          valueFrom:
            secretKeyRef:
              name: database-credentials
              key: database
        - name: PGDATA
          value: /var/lib/postgresql/data/pgdata
        volumeMounts:
        - name: database-storage
          mountPath: /var/lib/postgresql/data
        - name: database-config
          mountPath: /etc/postgresql/postgresql.conf
          subPath: postgresql.conf
        resources:
          requests:
            memory: "512Mi"
            cpu: "500m"
          limits:
            memory: "2Gi"
            cpu: "2000m"
        livenessProbe:
          exec:
            command:
            - pg_isready
            - -U
            - postgres
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          exec:
            command:
            - pg_isready
            - -U
            - postgres
          initialDelaySeconds: 5
          periodSeconds: 5
      # Migration init container from /db-migrate
      initContainers:
      - name: migration
        image: migration-runner:latest
        env:
        - name: DATABASE_URL
          value: "postgresql://$(POSTGRES_USER):$(POSTGRES_PASSWORD)@localhost:5432/$(POSTGRES_DB)"
        envFrom:
        - secretRef:
            name: database-credentials
        command:
        - sh
        - -c
        - |
          echo "Running database migrations..."
          alembic upgrade head
          echo "Migrations completed successfully"
      volumes:
      - name: database-config
        configMap:
          name: database-config
  volumeClaimTemplates:
  - metadata:
      name: database-storage
    spec:
      accessModes: ["ReadWriteOnce"]
      storageClassName: fast-ssd
      resources:
        requests:
          storage: 100Gi

---
apiVersion: v1
kind: Service
metadata:
  name: database-headless
  namespace: production
spec:
  clusterIP: None
  selector:
    app: database
  ports:
  - name: postgresql
    port: 5432
    targetPort: 5432

---
apiVersion: v1
kind: Service
metadata:
  name: database
  namespace: production
spec:
  selector:
    app: database
  ports:
  - name: postgresql
    port: 5432
    targetPort: 5432
  type: ClusterIP
```

**Frontend + Backend Integration**
```yaml
# fullstack-k8s-deployment.yaml - Integration across all commands
apiVersion: v1
kind: Namespace
metadata:
  name: fullstack-app
  labels:
    name: fullstack-app
    security-policy: strict

---
# API deployment (from /api-scaffold + optimizations)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: api-deployment
  namespace: fullstack-app
spec:
  replicas: 3
  selector:
    matchLabels:
      app: api
      tier: backend
  template:
    metadata:
      labels:
        app: api
        tier: backend
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "8000"
        prometheus.io/path: "/metrics"
    spec:
      serviceAccountName: api-service-account
      containers:
      - name: api
        image: registry.company.com/api:optimized-latest
        ports:
        - containerPort: 8000
          name: http
        env:
        - name: DATABASE_URL
          valueFrom:
            secretKeyRef:
              name: database-credentials
              key: url
        - name: REDIS_URL
          valueFrom:
            configMapKeyRef:
              name: app-config
              key: redis-url
        resources:
          requests:
            memory: "256Mi"
            cpu: "250m"
          limits:
            memory: "512Mi"
            cpu: "500m"
        securityContext:
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
          runAsNonRoot: true
          runAsUser: 1001
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /ready
            port: 8000
          initialDelaySeconds: 5
          periodSeconds: 5

---
# Frontend deployment (from /frontend-optimize + container optimization)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend-deployment
  namespace: fullstack-app
spec:
  replicas: 2
  selector:
    matchLabels:
      app: frontend
      tier: frontend
  template:
    metadata:
      labels:
        app: frontend
        tier: frontend
    spec:
      containers:
      - name: frontend
        image: registry.company.com/frontend:optimized-latest
        ports:
        - containerPort: 80
          name: http
        env:
        - name: API_URL
          value: "http://api-service:8000"
        - name: NODE_ENV
          value: "production"
        resources:
          requests:
            memory: "128Mi"
            cpu: "100m"
          limits:
            memory: "256Mi"
            cpu: "200m"
        securityContext:
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
          runAsNonRoot: true
          runAsUser: 1001
        livenessProbe:
          httpGet:
            path: /
            port: 80
          initialDelaySeconds: 10
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /
            port: 80
          initialDelaySeconds: 5
          periodSeconds: 5

---
# Services
apiVersion: v1
kind: Service
metadata:
  name: api-service
  namespace: fullstack-app
spec:
  selector:
    app: api
    tier: backend
  ports:
  - name: http
    port: 8000
    targetPort: 8000
  type: ClusterIP

---
apiVersion: v1
kind: Service
metadata:
  name: frontend-service
  namespace: fullstack-app
spec:
  selector:
    app: frontend
    tier: frontend
  ports:
  - name: http
    port: 80
    targetPort: 80
  type: ClusterIP

---
# Ingress with security configurations from /security-scan
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: app-ingress
  namespace: fullstack-app
  annotations:
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
    nginx.ingress.kubernetes.io/force-ssl-redirect: "true"
    nginx.ingress.kubernetes.io/rate-limit: "100"
    nginx.ingress.kubernetes.io/rate-limit-window: "1m"
    cert-manager.io/cluster-issuer: "letsencrypt-prod"
    nginx.ingress.kubernetes.io/add-base-url: "true"
    nginx.ingress.kubernetes.io/proxy-buffer-size: "8k"
spec:
  ingressClassName: nginx
  tls:
  - hosts:
    - app.company.com
    secretName: app-tls-secret
  rules:
  - host: app.company.com
    http:
      paths:
      - path: /api
        pathType: Prefix
        backend:
          service:
            name: api-service
            port:
              number: 8000
      - path: /
        pathType: Prefix
        backend:
          service:
            name: frontend-service
            port:
              number: 80
```

**Security Integration**
```yaml
# security-k8s-manifests.yaml - From /security-scan integration
apiVersion: v1
kind: ServiceAccount
metadata:
  name: api-service-account
  namespace: fullstack-app
automountServiceAccountToken: false

---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: api-role
  namespace: fullstack-app
rules:
- apiGroups: [""]
  resources: ["secrets", "configmaps"]
  verbs: ["get", "list"]
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "list", "watch"]

---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: api-role-binding
  namespace: fullstack-app
subjects:
- kind: ServiceAccount
  name: api-service-account
  namespace: fullstack-app
roleRef:
  kind: Role
  name: api-role
  apiGroup: rbac.authorization.k8s.io

---
# Network policies for security isolation
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: api-network-policy
  namespace: fullstack-app
spec:
  podSelector:
    matchLabels:
      app: api
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - podSelector:
        matchLabels:
          app: frontend
    - namespaceSelector:
        matchLabels:
          name: ingress-nginx
    ports:
    - protocol: TCP
      port: 8000
  egress:
  - to:
    - podSelector:
        matchLabels:
          app: database
    ports:
    - protocol: TCP
      port: 5432
  - to: []  # Allow DNS
    ports:
    - protocol: UDP
      port: 53

---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: frontend-network-policy
  namespace: fullstack-app
spec:
  podSelector:
    matchLabels:
      app: frontend
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          name: ingress-nginx
    ports:
    - protocol: TCP
      port: 80
  egress:
  - to:
    - podSelector:
        matchLabels:
          app: api
    ports:
    - protocol: TCP
      port: 8000

---
# Pod Security Standards
apiVersion: v1
kind: LimitRange
metadata:
  name: resource-limits
  namespace: fullstack-app
spec:
  limits:
  - default:
      cpu: "500m"
      memory: "512Mi"
      ephemeral-storage: "1Gi"
    defaultRequest:
      cpu: "100m"
      memory: "128Mi"
      ephemeral-storage: "500Mi"
    type: Container
  - max:
      cpu: "2"
      memory: "4Gi"
      ephemeral-storage: "10Gi"
    type: Container

---
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: api-pdb
  namespace: fullstack-app
spec:
  minAvailable: 1
  selector:
    matchLabels:
      app: api

---
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: frontend-pdb
  namespace: fullstack-app
spec:
  minAvailable: 1
  selector:
    matchLabels:
      app: frontend
```

**Monitoring and Observability Integration**
```yaml
# monitoring-k8s-manifests.yaml - Complete observability stack
apiVersion: v1
kind: ServiceMonitor
metadata:
  name: api-monitor
  namespace: fullstack-app
  labels:
    app: api
spec:
  selector:
    matchLabels:
      app: api
  endpoints:
  - port: http
    path: /metrics
    interval: 30s

---
apiVersion: v1
kind: ConfigMap
metadata:
  name: grafana-dashboards
  namespace: monitoring
data:
  app-dashboard.json: |
    {
      "dashboard": {
        "title": "Application Metrics",
        "panels": [
          {
            "title": "API Response Time",
            "targets": [
              {
                "expr": "http_request_duration_seconds{job=\"api-service\"}",
                "legendFormat": "Response Time"
              }
            ]
          },
          {
            "title": "Error Rate",
            "targets": [
              {
                "expr": "rate(http_requests_total{job=\"api-service\",status=~\"5..\"}[5m])",
                "legendFormat": "5xx Errors"
              }
            ]
          }
        ]
      }
    }

---
# Jaeger tracing configuration
apiVersion: v1
kind: ConfigMap
metadata:
  name: jaeger-config
  namespace: fullstack-app
data:
  jaeger.yaml: |
    sampling:
      type: probabilistic
      param: 0.1
    reporter:
      logSpans: true
      localAgentHostPort: jaeger-agent:6831

---
# Application logging configuration
apiVersion: v1
kind: ConfigMap
metadata:
  name: fluent-bit-config
  namespace: fullstack-app
data:
  fluent-bit.conf: |
    [SERVICE]
        Flush         1
        Log_Level     info
        Daemon        off
        Parsers_File  parsers.conf
    
    [INPUT]
        Name              tail
        Path              /var/log/containers/*.log
        Parser            docker
        Tag               kube.*
        Refresh_Interval  5
        Mem_Buf_Limit     5MB
        Skip_Long_Lines   On
    
    [FILTER]
        Name                kubernetes
        Match               kube.*
        Kube_URL            https://kubernetes.default.svc:443
        Kube_CA_File        /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        Kube_Token_File     /var/run/secrets/kubernetes.io/serviceaccount/token
        Merge_Log           On
    
    [OUTPUT]
        Name  es
        Match *
        Host  elasticsearch.logging.svc.cluster.local
        Port  9200
        Index app-logs
```

**Auto-scaling Integration**
```yaml
# autoscaling-k8s-manifests.yaml - Intelligent scaling based on multiple metrics
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: api-hpa
  namespace: fullstack-app
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: api-deployment
  minReplicas: 2
  maxReplicas: 20
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  - type: Pods
    pods:
      metric:
        name: http_requests_per_second
      target:
        type: AverageValue
        averageValue: "1000"
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 25
        periodSeconds: 60
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
      - type: Percent
        value: 50
        periodSeconds: 15

---
apiVersion: autoscaling/v1
kind: VerticalPodAutoscaler
metadata:
  name: api-vpa
  namespace: fullstack-app
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: api-deployment
  updatePolicy:
    updateMode: "Auto"
  resourcePolicy:
    containerPolicies:
    - containerName: api
      minAllowed:
        cpu: 100m
        memory: 128Mi
      maxAllowed:
        cpu: 2
        memory: 4Gi
      controlledResources: ["cpu", "memory"]

---
# KEDA for advanced autoscaling based on external metrics
apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
  name: api-scaled-object
  namespace: fullstack-app
spec:
  scaleTargetRef:
    name: api-deployment
  minReplicaCount: 2
  maxReplicaCount: 50
  triggers:
  - type: prometheus
    metadata:
      serverAddress: http://prometheus.monitoring.svc.cluster.local:9090
      metricName: http_requests_per_second
      threshold: '1000'
      query: sum(rate(http_requests_total{job="api-service"}[1m]))
  - type: redis
    metadata:
      address: redis.fullstack-app.svc.cluster.local:6379
      listName: task_queue
      listLength: '10'
```

**CI/CD Integration Pipeline**
```yaml
# .github/workflows/k8s-deployment.yml
name: Kubernetes Deployment Pipeline

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]

env:
  REGISTRY: ghcr.io
  CLUSTER_NAME: production-cluster

jobs:
  deploy-to-kubernetes:
    runs-on: ubuntu-latest
    permissions:
      contents: read
      packages: read
      id-token: write
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
    
    # 1. Setup kubectl and helm
    - name: Setup kubectl
      uses: azure/setup-kubectl@v3
      with:
        version: 'v1.28.0'
    
    - name: Setup Helm
      uses: azure/setup-helm@v3
      with:
        version: 'v3.12.0'
    
    # 2. Authenticate with cluster
    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v2
      with:
        role-to-assume: ${{ secrets.AWS_ROLE_TO_ASSUME }}
        aws-region: us-west-2
    
    - name: Update kubeconfig
      run: |
        aws eks update-kubeconfig --region us-west-2 --name ${{ env.CLUSTER_NAME }}
    
    # 3. Validate manifests
    - name: Validate Kubernetes manifests
      run: |
        # Validate syntax
        kubectl --dry-run=client apply -f k8s/
        
        # Security validation with kubesec
        docker run --rm -v $(pwd):/workspace kubesec/kubesec:latest scan /workspace/k8s/*.yaml
        
        # Policy validation with OPA Gatekeeper
        conftest verify --policy opa-policies/ k8s/
    
    # 4. Deploy to staging
    - name: Deploy to staging
      if: github.ref == 'refs/heads/develop'
      run: |
        # Update image tags
        sed -i "s|registry.company.com/api:.*|registry.company.com/api:${{ github.sha }}|g" k8s/api-deployment.yaml
        sed -i "s|registry.company.com/frontend:.*|registry.company.com/frontend:${{ github.sha }}|g" k8s/frontend-deployment.yaml
        
        # Apply manifests to staging namespace
        kubectl apply -f k8s/ --namespace=staging
        
        # Wait for rollout to complete
        kubectl rollout status deployment/api-deployment --namespace=staging --timeout=300s
        kubectl rollout status deployment/frontend-deployment --namespace=staging --timeout=300s
    
    # 5. Run integration tests
    - name: Run integration tests
      if: github.ref == 'refs/heads/develop'
      run: |
        # Wait for services to be ready
        kubectl wait --for=condition=ready pod -l app=api --namespace=staging --timeout=300s
        
        # Get service URLs
        API_URL=$(kubectl get service api-service --namespace=staging -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')
        
        # Run tests from /test-harness
        pytest tests/integration/ --api-url="http://${API_URL}:8000" -v
    
    # 6. Deploy to production (on main branch)
    - name: Deploy to production
      if: github.ref == 'refs/heads/main'
      run: |
        # Update image tags
        sed -i "s|registry.company.com/api:.*|registry.company.com/api:${{ github.sha }}|g" k8s/api-deployment.yaml
        sed -i "s|registry.company.com/frontend:.*|registry.company.com/frontend:${{ github.sha }}|g" k8s/frontend-deployment.yaml
        
        # Apply manifests to production namespace with rolling update
        kubectl apply -f k8s/ --namespace=production
        
        # Monitor rollout
        kubectl rollout status deployment/api-deployment --namespace=production --timeout=600s
        kubectl rollout status deployment/frontend-deployment --namespace=production --timeout=600s
        
        # Verify deployment health
        kubectl get pods --namespace=production -l app=api
        kubectl get pods --namespace=production -l app=frontend
    
    # 7. Post-deployment verification
    - name: Post-deployment verification
      if: github.ref == 'refs/heads/main'
      run: |
        # Health checks
        kubectl exec -n production deployment/api-deployment -- curl -f http://localhost:8000/health
        
        # Performance baseline check
        kubectl run --rm -i --tty load-test --image=loadimpact/k6:latest --restart=Never -- run - <<EOF
        import http from 'k6/http';
        import { check } from 'k6';
        
        export let options = {
          stages: [
            { duration: '2m', target: 100 },
            { duration: '5m', target: 100 },
            { duration: '2m', target: 0 },
          ],
        };
        
        export default function () {
          let response = http.get('http://api-service.production.svc.cluster.local:8000/health');
          check(response, {
            'status is 200': (r) => r.status === 200,
            'response time < 500ms': (r) => r.timings.duration < 500,
          });
        }
        EOF
    
    # 8. Cleanup on failure
    - name: Rollback on failure
      if: failure()
      run: |
        # Rollback to previous version
        kubectl rollout undo deployment/api-deployment --namespace=production
        kubectl rollout undo deployment/frontend-deployment --namespace=production
        
        # Notify team
        echo "Deployment failed and rolled back" >> $GITHUB_STEP_SUMMARY
```

This comprehensive integration ensures that Kubernetes deployments leverage all optimizations from container builds, security hardening, database migrations, and monitoring configurations while providing enterprise-grade reliability and observability.

Focus on creating enterprise-grade, cloud-native deployments with zero-downtime deployment capabilities and comprehensive observability.
````

## File: tools/langchain-agent.md

````markdown
# LangChain/LangGraph Agent Scaffold

Create a production-ready LangChain/LangGraph agent for: $ARGUMENTS

Implement a complete agent system including:

1. **Agent Architecture**:
   - LangGraph state machine
   - Tool selection logic
   - Memory management
   - Context window optimization
   - Multi-agent coordination

2. **Tool Implementation**:
   - Custom tool creation
   - Tool validation
   - Error handling in tools
   - Tool composition
   - Async tool execution

3. **Memory Systems**:
   - Short-term memory
   - Long-term storage (vector DB)
   - Conversation summarization
   - Entity tracking
   - Memory retrieval strategies

4. **Prompt Engineering**:
   - System prompts
   - Few-shot examples
   - Chain-of-thought reasoning
   - Output formatting
   - Prompt templates

5. **RAG Integration**:
   - Document loading pipeline
   - Chunking strategies
   - Embedding generation
   - Vector store setup
   - Retrieval optimization

6. **Production Features**:
   - Streaming responses
   - Token counting
   - Cost tracking
   - Rate limiting
   - Fallback strategies

7. **Observability**:
   - LangSmith integration
   - Custom callbacks
   - Performance metrics
   - Decision tracking
   - Debug mode

Include error handling, testing strategies, and deployment considerations. Use the latest LangChain/LangGraph best practices.
````

## File: tools/monitor-setup.md

````markdown
# Monitoring and Observability Setup

You are a monitoring and observability expert specializing in implementing comprehensive monitoring solutions. Set up metrics collection, distributed tracing, log aggregation, and create insightful dashboards that provide full visibility into system health and performance.

## Context
The user needs to implement or improve monitoring and observability. Focus on the three pillars of observability (metrics, logs, traces), setting up monitoring infrastructure, creating actionable dashboards, and establishing effective alerting strategies.

## Requirements
$ARGUMENTS

## Instructions

### 1. Monitoring Requirements Analysis

Analyze monitoring needs and current state:

**Monitoring Assessment**
```python
import yaml
from pathlib import Path
from collections import defaultdict

class MonitoringAssessment:
    def analyze_infrastructure(self, project_path):
        """
        Analyze infrastructure and determine monitoring needs
        """
        assessment = {
            'infrastructure': self._detect_infrastructure(project_path),
            'services': self._identify_services(project_path),
            'current_monitoring': self._check_existing_monitoring(project_path),
            'metrics_needed': self._determine_metrics(project_path),
            'compliance_requirements': self._check_compliance_needs(project_path),
            'recommendations': []
        }
        
        self._generate_recommendations(assessment)
        return assessment
    
    def _detect_infrastructure(self, project_path):
        """Detect infrastructure components"""
        infrastructure = {
            'cloud_provider': None,
            'orchestration': None,
            'databases': [],
            'message_queues': [],
            'cache_systems': [],
            'load_balancers': []
        }
        
        # Check for cloud providers
        if (Path(project_path) / '.aws').exists():
            infrastructure['cloud_provider'] = 'AWS'
        elif (Path(project_path) / 'azure-pipelines.yml').exists():
            infrastructure['cloud_provider'] = 'Azure'
        elif (Path(project_path) / '.gcloud').exists():
            infrastructure['cloud_provider'] = 'GCP'
        
        # Check for orchestration
        if (Path(project_path) / 'docker-compose.yml').exists():
            infrastructure['orchestration'] = 'docker-compose'
        elif (Path(project_path) / 'k8s').exists():
            infrastructure['orchestration'] = 'kubernetes'
        
        return infrastructure
    
    def _determine_metrics(self, project_path):
        """Determine required metrics based on services"""
        metrics = {
            'golden_signals': {
                'latency': ['response_time_p50', 'response_time_p95', 'response_time_p99'],
                'traffic': ['requests_per_second', 'active_connections'],
                'errors': ['error_rate', 'error_count_by_type'],
                'saturation': ['cpu_usage', 'memory_usage', 'disk_usage', 'queue_depth']
            },
            'business_metrics': [],
            'custom_metrics': []
        }
        
        # Add service-specific metrics
        services = self._identify_services(project_path)
        
        if 'web' in services:
            metrics['custom_metrics'].extend([
                'page_load_time',
                'time_to_first_byte',
                'concurrent_users'
            ])
        
        if 'database' in services:
            metrics['custom_metrics'].extend([
                'query_duration',
                'connection_pool_usage',
                'replication_lag'
            ])
        
        if 'queue' in services:
            metrics['custom_metrics'].extend([
                'message_processing_time',
                'queue_length',
                'dead_letter_queue_size'
            ])
        
        return metrics
```

### 2. Prometheus Setup

Implement Prometheus-based monitoring:

**Prometheus Configuration**
```yaml
# prometheus.yml
global:
  scrape_interval: 15s
  evaluation_interval: 15s
  external_labels:
    cluster: 'production'
    region: 'us-east-1'

# Alertmanager configuration
alerting:
  alertmanagers:
    - static_configs:
        - targets:
            - alertmanager:9093

# Rule files
rule_files:
  - "alerts/*.yml"
  - "recording_rules/*.yml"

# Scrape configurations
scrape_configs:
  # Prometheus self-monitoring
  - job_name: 'prometheus'
    static_configs:
      - targets: ['localhost:9090']

  # Node exporter for system metrics
  - job_name: 'node'
    static_configs:
      - targets: 
          - 'node-exporter:9100'
    relabel_configs:
      - source_labels: [__address__]
        regex: '([^:]+)(?::\d+)?'
        target_label: instance
        replacement: '${1}'

  # Application metrics
  - job_name: 'application'
    kubernetes_sd_configs:
      - role: pod
    relabel_configs:
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
        action: keep
        regex: true
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
        action: replace
        target_label: __metrics_path__
        regex: (.+)
      - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
        action: replace
        regex: ([^:]+)(?::\d+)?;(\d+)
        replacement: $1:$2
        target_label: __address__
      - action: labelmap
        regex: __meta_kubernetes_pod_label_(.+)
      - source_labels: [__meta_kubernetes_namespace]
        action: replace
        target_label: kubernetes_namespace
      - source_labels: [__meta_kubernetes_pod_name]
        action: replace
        target_label: kubernetes_pod_name

  # Database monitoring
  - job_name: 'postgres'
    static_configs:
      - targets: ['postgres-exporter:9187']
    params:
      query: ['pg_stat_database', 'pg_stat_replication']

  # Redis monitoring
  - job_name: 'redis'
    static_configs:
      - targets: ['redis-exporter:9121']

  # Custom service discovery
  - job_name: 'custom-services'
    consul_sd_configs:
      - server: 'consul:8500'
        services: []
    relabel_configs:
      - source_labels: [__meta_consul_service]
        target_label: service_name
      - source_labels: [__meta_consul_tags]
        regex: '.*,metrics,.*'
        action: keep
```

**Custom Metrics Implementation**
```typescript
// metrics.ts
import { Counter, Histogram, Gauge, Registry } from 'prom-client';

export class MetricsCollector {
    private registry: Registry;
    
    // HTTP metrics
    private httpRequestDuration: Histogram<string>;
    private httpRequestTotal: Counter<string>;
    private httpRequestsInFlight: Gauge<string>;
    
    // Business metrics
    private userRegistrations: Counter<string>;
    private activeUsers: Gauge<string>;
    private revenue: Counter<string>;
    
    // System metrics
    private queueDepth: Gauge<string>;
    private cacheHitRatio: Gauge<string>;
    
    constructor() {
        this.registry = new Registry();
        this.initializeMetrics();
    }
    
    private initializeMetrics() {
        // HTTP metrics
        this.httpRequestDuration = new Histogram({
            name: 'http_request_duration_seconds',
            help: 'Duration of HTTP requests in seconds',
            labelNames: ['method', 'route', 'status_code'],
            buckets: [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 2, 5]
        });
        
        this.httpRequestTotal = new Counter({
            name: 'http_requests_total',
            help: 'Total number of HTTP requests',
            labelNames: ['method', 'route', 'status_code']
        });
        
        this.httpRequestsInFlight = new Gauge({
            name: 'http_requests_in_flight',
            help: 'Number of HTTP requests currently being processed',
            labelNames: ['method', 'route']
        });
        
        // Business metrics
        this.userRegistrations = new Counter({
            name: 'user_registrations_total',
            help: 'Total number of user registrations',
            labelNames: ['source', 'plan']
        });
        
        this.activeUsers = new Gauge({
            name: 'active_users',
            help: 'Number of active users',
            labelNames: ['timeframe']
        });
        
        this.revenue = new Counter({
            name: 'revenue_total_cents',
            help: 'Total revenue in cents',
            labelNames: ['product', 'currency']
        });
        
        // Register all metrics
        this.registry.registerMetric(this.httpRequestDuration);
        this.registry.registerMetric(this.httpRequestTotal);
        this.registry.registerMetric(this.httpRequestsInFlight);
        this.registry.registerMetric(this.userRegistrations);
        this.registry.registerMetric(this.activeUsers);
        this.registry.registerMetric(this.revenue);
    }
    
    // Middleware for Express
    httpMetricsMiddleware() {
        return (req: Request, res: Response, next: NextFunction) => {
            const start = Date.now();
            const route = req.route?.path || req.path;
            
            // Increment in-flight gauge
            this.httpRequestsInFlight.inc({ method: req.method, route });
            
            res.on('finish', () => {
                const duration = (Date.now() - start) / 1000;
                const labels = {
                    method: req.method,
                    route,
                    status_code: res.statusCode.toString()
                };
                
                // Record metrics
                this.httpRequestDuration.observe(labels, duration);
                this.httpRequestTotal.inc(labels);
                this.httpRequestsInFlight.dec({ method: req.method, route });
            });
            
            next();
        };
    }
    
    // Business metric helpers
    recordUserRegistration(source: string, plan: string) {
        this.userRegistrations.inc({ source, plan });
    }
    
    updateActiveUsers(timeframe: string, count: number) {
        this.activeUsers.set({ timeframe }, count);
    }
    
    recordRevenue(product: string, currency: string, amountCents: number) {
        this.revenue.inc({ product, currency }, amountCents);
    }
    
    // Export metrics endpoint
    async getMetrics(): Promise<string> {
        return this.registry.metrics();
    }
}

// Recording rules for Prometheus
export const recordingRules = `
groups:
  - name: aggregations
    interval: 30s
    rules:
      # Request rate
      - record: http_request_rate_5m
        expr: rate(http_requests_total[5m])
      
      # Error rate
      - record: http_error_rate_5m
        expr: |
          sum(rate(http_requests_total{status_code=~"5.."}[5m]))
          /
          sum(rate(http_requests_total[5m]))
      
      # P95 latency
      - record: http_request_duration_p95_5m
        expr: |
          histogram_quantile(0.95,
            sum(rate(http_request_duration_seconds_bucket[5m])) by (le, route)
          )
      
      # Business metrics
      - record: user_registration_rate_1h
        expr: rate(user_registrations_total[1h])
      
      - record: revenue_rate_1d
        expr: rate(revenue_total_cents[1d]) / 100
`;
```

### 3. Grafana Dashboard Setup

Create comprehensive dashboards:

**Dashboard Configuration**
```json
{
  "dashboard": {
    "title": "Application Overview",
    "tags": ["production", "overview"],
    "timezone": "browser",
    "panels": [
      {
        "title": "Request Rate",
        "type": "graph",
        "gridPos": { "x": 0, "y": 0, "w": 12, "h": 8 },
        "targets": [
          {
            "expr": "sum(rate(http_requests_total[5m])) by (method)",
            "legendFormat": "{{method}}"
          }
        ]
      },
      {
        "title": "Error Rate",
        "type": "graph",
        "gridPos": { "x": 12, "y": 0, "w": 12, "h": 8 },
        "targets": [
          {
            "expr": "sum(rate(http_requests_total{status_code=~\"5..\"}[5m])) / sum(rate(http_requests_total[5m]))",
            "legendFormat": "Error Rate"
          }
        ],
        "alert": {
          "conditions": [
            {
              "evaluator": { "params": [0.05], "type": "gt" },
              "query": { "params": ["A", "5m", "now"] },
              "reducer": { "type": "avg" },
              "type": "query"
            }
          ],
          "name": "High Error Rate"
        }
      },
      {
        "title": "Response Time",
        "type": "graph",
        "gridPos": { "x": 0, "y": 8, "w": 12, "h": 8 },
        "targets": [
          {
            "expr": "histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket[5m])) by (le))",
            "legendFormat": "p95"
          },
          {
            "expr": "histogram_quantile(0.99, sum(rate(http_request_duration_seconds_bucket[5m])) by (le))",
            "legendFormat": "p99"
          }
        ]
      },
      {
        "title": "Active Users",
        "type": "stat",
        "gridPos": { "x": 12, "y": 8, "w": 6, "h": 4 },
        "targets": [
          {
            "expr": "active_users{timeframe=\"realtime\"}"
          }
        ]
      }
    ]
  }
}
```

**Dashboard as Code**
```typescript
// dashboards/service-dashboard.ts
import { Dashboard, Panel, Target } from '@grafana/toolkit';

export const createServiceDashboard = (serviceName: string): Dashboard => {
    return new Dashboard({
        title: `${serviceName} Service Dashboard`,
        uid: `${serviceName}-overview`,
        tags: ['service', serviceName],
        time: { from: 'now-6h', to: 'now' },
        refresh: '30s',
        
        panels: [
            // Row 1: Golden Signals
            new Panel.Graph({
                title: 'Request Rate',
                gridPos: { x: 0, y: 0, w: 6, h: 8 },
                targets: [
                    new Target({
                        expr: `sum(rate(http_requests_total{service="${serviceName}"}[5m])) by (method)`,
                        legendFormat: '{{method}}'
                    })
                ]
            }),
            
            new Panel.Graph({
                title: 'Error Rate',
                gridPos: { x: 6, y: 0, w: 6, h: 8 },
                targets: [
                    new Target({
                        expr: `sum(rate(http_requests_total{service="${serviceName}",status_code=~"5.."}[5m])) / sum(rate(http_requests_total{service="${serviceName}"}[5m]))`,
                        legendFormat: 'Error %'
                    })
                ],
                yaxes: [{ format: 'percentunit' }]
            }),
            
            new Panel.Graph({
                title: 'Latency Percentiles',
                gridPos: { x: 12, y: 0, w: 12, h: 8 },
                targets: [
                    new Target({
                        expr: `histogram_quantile(0.50, sum(rate(http_request_duration_seconds_bucket{service="${serviceName}"}[5m])) by (le))`,
                        legendFormat: 'p50'
                    }),
                    new Target({
                        expr: `histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket{service="${serviceName}"}[5m])) by (le))`,
                        legendFormat: 'p95'
                    }),
                    new Target({
                        expr: `histogram_quantile(0.99, sum(rate(http_request_duration_seconds_bucket{service="${serviceName}"}[5m])) by (le))`,
                        legendFormat: 'p99'
                    })
                ],
                yaxes: [{ format: 's' }]
            }),
            
            // Row 2: Resource Usage
            new Panel.Graph({
                title: 'CPU Usage',
                gridPos: { x: 0, y: 8, w: 8, h: 8 },
                targets: [
                    new Target({
                        expr: `avg(rate(container_cpu_usage_seconds_total{pod=~"${serviceName}-.*"}[5m])) by (pod)`,
                        legendFormat: '{{pod}}'
                    })
                ],
                yaxes: [{ format: 'percentunit' }]
            }),
            
            new Panel.Graph({
                title: 'Memory Usage',
                gridPos: { x: 8, y: 8, w: 8, h: 8 },
                targets: [
                    new Target({
                        expr: `avg(container_memory_working_set_bytes{pod=~"${serviceName}-.*"}) by (pod)`,
                        legendFormat: '{{pod}}'
                    })
                ],
                yaxes: [{ format: 'bytes' }]
            }),
            
            new Panel.Graph({
                title: 'Network I/O',
                gridPos: { x: 16, y: 8, w: 8, h: 8 },
                targets: [
                    new Target({
                        expr: `sum(rate(container_network_receive_bytes_total{pod=~"${serviceName}-.*"}[5m])) by (pod)`,
                        legendFormat: '{{pod}} RX'
                    }),
                    new Target({
                        expr: `sum(rate(container_network_transmit_bytes_total{pod=~"${serviceName}-.*"}[5m])) by (pod)`,
                        legendFormat: '{{pod}} TX'
                    })
                ],
                yaxes: [{ format: 'Bps' }]
            })
        ]
    });
};
```

### 4. Distributed Tracing Setup

Implement OpenTelemetry-based tracing:

**OpenTelemetry Configuration**
```typescript
// tracing.ts
import { NodeSDK } from '@opentelemetry/sdk-node';
import { getNodeAutoInstrumentations } from '@opentelemetry/auto-instrumentations-node';
import { Resource } from '@opentelemetry/resources';
import { SemanticResourceAttributes } from '@opentelemetry/semantic-conventions';
import { JaegerExporter } from '@opentelemetry/exporter-jaeger';
import { BatchSpanProcessor } from '@opentelemetry/sdk-trace-base';
import { PrometheusExporter } from '@opentelemetry/exporter-prometheus';

export class TracingSetup {
    private sdk: NodeSDK;
    
    constructor(serviceName: string, environment: string) {
        const jaegerExporter = new JaegerExporter({
            endpoint: process.env.JAEGER_ENDPOINT || 'http://localhost:14268/api/traces',
        });
        
        const prometheusExporter = new PrometheusExporter({
            port: 9464,
            endpoint: '/metrics',
        }, () => {
            console.log('Prometheus metrics server started on port 9464');
        });
        
        this.sdk = new NodeSDK({
            resource: new Resource({
                [SemanticResourceAttributes.SERVICE_NAME]: serviceName,
                [SemanticResourceAttributes.SERVICE_VERSION]: process.env.SERVICE_VERSION || '1.0.0',
                [SemanticResourceAttributes.DEPLOYMENT_ENVIRONMENT]: environment,
            }),
            
            traceExporter: jaegerExporter,
            spanProcessor: new BatchSpanProcessor(jaegerExporter, {
                maxQueueSize: 2048,
                maxExportBatchSize: 512,
                scheduledDelayMillis: 5000,
                exportTimeoutMillis: 30000,
            }),
            
            metricExporter: prometheusExporter,
            
            instrumentations: [
                getNodeAutoInstrumentations({
                    '@opentelemetry/instrumentation-fs': {
                        enabled: false,
                    },
                }),
            ],
        });
    }
    
    start() {
        this.sdk.start()
            .then(() => console.log('Tracing initialized'))
            .catch((error) => console.error('Error initializing tracing', error));
    }
    
    shutdown() {
        return this.sdk.shutdown()
            .then(() => console.log('Tracing terminated'))
            .catch((error) => console.error('Error terminating tracing', error));
    }
}

// Custom span creation
import { trace, context, SpanStatusCode, SpanKind } from '@opentelemetry/api';

export class CustomTracer {
    private tracer = trace.getTracer('custom-tracer', '1.0.0');
    
    async traceOperation<T>(
        operationName: string,
        operation: () => Promise<T>,
        attributes?: Record<string, any>
    ): Promise<T> {
        const span = this.tracer.startSpan(operationName, {
            kind: SpanKind.INTERNAL,
            attributes,
        });
        
        return context.with(trace.setSpan(context.active(), span), async () => {
            try {
                const result = await operation();
                span.setStatus({ code: SpanStatusCode.OK });
                return result;
            } catch (error) {
                span.recordException(error as Error);
                span.setStatus({
                    code: SpanStatusCode.ERROR,
                    message: error.message,
                });
                throw error;
            } finally {
                span.end();
            }
        });
    }
    
    // Database query tracing
    async traceQuery<T>(
        queryName: string,
        query: () => Promise<T>,
        sql?: string
    ): Promise<T> {
        return this.traceOperation(
            `db.query.${queryName}`,
            query,
            {
                'db.system': 'postgresql',
                'db.operation': queryName,
                'db.statement': sql,
            }
        );
    }
    
    // HTTP request tracing
    async traceHttpRequest<T>(
        method: string,
        url: string,
        request: () => Promise<T>
    ): Promise<T> {
        return this.traceOperation(
            `http.request`,
            request,
            {
                'http.method': method,
                'http.url': url,
                'http.target': new URL(url).pathname,
            }
        );
    }
}
```

### 5. Log Aggregation Setup

Implement centralized logging:

**Fluentd Configuration**
```yaml
# fluent.conf
<source>
  @type tail
  path /var/log/containers/*.log
  pos_file /var/log/fluentd-containers.log.pos
  tag kubernetes.*
  <parse>
    @type json
    time_format %Y-%m-%dT%H:%M:%S.%NZ
  </parse>
</source>

# Add Kubernetes metadata
<filter kubernetes.**>
  @type kubernetes_metadata
  @id filter_kube_metadata
  kubernetes_url "#{ENV['FLUENT_FILTER_KUBERNETES_URL'] || 'https://' + ENV.fetch('KUBERNETES_SERVICE_HOST') + ':' + ENV.fetch('KUBERNETES_SERVICE_PORT') + '/api'}"
  verify_ssl "#{ENV['KUBERNETES_VERIFY_SSL'] || true}"
</filter>

# Parse application logs
<filter kubernetes.**>
  @type parser
  key_name log
  reserve_data true
  remove_key_name_field true
  <parse>
    @type multi_format
    <pattern>
      format json
    </pattern>
    <pattern>
      format regexp
      expression /^(?<severity>\w+)\s+\[(?<timestamp>[^\]]+)\]\s+(?<message>.*)$/
      time_format %Y-%m-%d %H:%M:%S
    </pattern>
  </parse>
</filter>

# Add fields
<filter kubernetes.**>
  @type record_transformer
  enable_ruby true
  <record>
    cluster_name ${ENV['CLUSTER_NAME']}
    environment ${ENV['ENVIRONMENT']}
    @timestamp ${time.strftime('%Y-%m-%dT%H:%M:%S.%LZ')}
  </record>
</filter>

# Output to Elasticsearch
<match kubernetes.**>
  @type elasticsearch
  @id out_es
  @log_level info
  include_tag_key true
  host "#{ENV['FLUENT_ELASTICSEARCH_HOST']}"
  port "#{ENV['FLUENT_ELASTICSEARCH_PORT']}"
  path "#{ENV['FLUENT_ELASTICSEARCH_PATH']}"
  scheme "#{ENV['FLUENT_ELASTICSEARCH_SCHEME'] || 'http'}"
  ssl_verify "#{ENV['FLUENT_ELASTICSEARCH_SSL_VERIFY'] || 'true'}"
  ssl_version "#{ENV['FLUENT_ELASTICSEARCH_SSL_VERSION'] || 'TLSv1_2'}"
  user "#{ENV['FLUENT_ELASTICSEARCH_USER']}"
  password "#{ENV['FLUENT_ELASTICSEARCH_PASSWORD']}"
  index_name logstash
  logstash_format true
  logstash_prefix "#{ENV['FLUENT_ELASTICSEARCH_LOGSTASH_PREFIX'] || 'logstash'}"
  <buffer>
    @type file
    path /var/log/fluentd-buffers/kubernetes.system.buffer
    flush_mode interval
    retry_type exponential_backoff
    flush_interval 5s
    retry_max_interval 30
    chunk_limit_size 2M
    queue_limit_length 8
    overflow_action block
  </buffer>
</match>
```

**Structured Logging Library**
```python
# structured_logging.py
import json
import logging
import traceback
from datetime import datetime
from typing import Any, Dict, Optional

class StructuredLogger:
    def __init__(self, name: str, service: str, version: str):
        self.logger = logging.getLogger(name)
        self.service = service
        self.version = version
        self.default_context = {
            'service': service,
            'version': version,
            'environment': os.getenv('ENVIRONMENT', 'development')
        }
    
    def _format_log(self, level: str, message: str, context: Dict[str, Any]) -> str:
        log_entry = {
            '@timestamp': datetime.utcnow().isoformat() + 'Z',
            'level': level,
            'message': message,
            **self.default_context,
            **context
        }
        
        # Add trace context if available
        trace_context = self._get_trace_context()
        if trace_context:
            log_entry['trace'] = trace_context
        
        return json.dumps(log_entry)
    
    def _get_trace_context(self) -> Optional[Dict[str, str]]:
        """Extract trace context from OpenTelemetry"""
        from opentelemetry import trace
        
        span = trace.get_current_span()
        if span and span.is_recording():
            span_context = span.get_span_context()
            return {
                'trace_id': format(span_context.trace_id, '032x'),
                'span_id': format(span_context.span_id, '016x'),
            }
        return None
    
    def info(self, message: str, **context):
        log_msg = self._format_log('INFO', message, context)
        self.logger.info(log_msg)
    
    def error(self, message: str, error: Optional[Exception] = None, **context):
        if error:
            context['error'] = {
                'type': type(error).__name__,
                'message': str(error),
                'stacktrace': traceback.format_exc()
            }
        
        log_msg = self._format_log('ERROR', message, context)
        self.logger.error(log_msg)
    
    def warning(self, message: str, **context):
        log_msg = self._format_log('WARNING', message, context)
        self.logger.warning(log_msg)
    
    def debug(self, message: str, **context):
        log_msg = self._format_log('DEBUG', message, context)
        self.logger.debug(log_msg)
    
    def audit(self, action: str, user_id: str, details: Dict[str, Any]):
        """Special method for audit logging"""
        self.info(
            f"Audit: {action}",
            audit=True,
            user_id=user_id,
            action=action,
            details=details
        )

# Log correlation middleware
from flask import Flask, request, g
import uuid

def setup_request_logging(app: Flask, logger: StructuredLogger):
    @app.before_request
    def before_request():
        g.request_id = request.headers.get('X-Request-ID', str(uuid.uuid4()))
        g.request_start = datetime.utcnow()
        
        logger.info(
            "Request started",
            request_id=g.request_id,
            method=request.method,
            path=request.path,
            remote_addr=request.remote_addr,
            user_agent=request.headers.get('User-Agent')
        )
    
    @app.after_request
    def after_request(response):
        duration = (datetime.utcnow() - g.request_start).total_seconds()
        
        logger.info(
            "Request completed",
            request_id=g.request_id,
            method=request.method,
            path=request.path,
            status_code=response.status_code,
            duration=duration
        )
        
        response.headers['X-Request-ID'] = g.request_id
        return response
```

### 6. Alert Configuration

Set up intelligent alerting:

**Alert Rules**
```yaml
# alerts/application.yml
groups:
  - name: application
    interval: 30s
    rules:
      # High error rate
      - alert: HighErrorRate
        expr: |
          sum(rate(http_requests_total{status_code=~"5.."}[5m])) by (service)
          /
          sum(rate(http_requests_total[5m])) by (service)
          > 0.05
        for: 5m
        labels:
          severity: critical
          team: backend
        annotations:
          summary: "High error rate on {{ $labels.service }}"
          description: "Error rate is {{ $value | humanizePercentage }} for {{ $labels.service }}"
          runbook_url: "https://wiki.company.com/runbooks/high-error-rate"
      
      # Slow response time
      - alert: SlowResponseTime
        expr: |
          histogram_quantile(0.95,
            sum(rate(http_request_duration_seconds_bucket[5m])) by (service, le)
          ) > 1
        for: 10m
        labels:
          severity: warning
          team: backend
        annotations:
          summary: "Slow response time on {{ $labels.service }}"
          description: "95th percentile response time is {{ $value }}s"
      
      # Pod restart
      - alert: PodRestarting
        expr: |
          increase(kube_pod_container_status_restarts_total[1h]) > 5
        labels:
          severity: warning
          team: platform
        annotations:
          summary: "Pod {{ $labels.namespace }}/{{ $labels.pod }} is restarting"
          description: "Pod has restarted {{ $value }} times in the last hour"

  - name: infrastructure
    interval: 30s
    rules:
      # High CPU usage
      - alert: HighCPUUsage
        expr: |
          avg(rate(container_cpu_usage_seconds_total[5m])) by (pod, namespace)
          > 0.8
        for: 15m
        labels:
          severity: warning
          team: platform
        annotations:
          summary: "High CPU usage on {{ $labels.pod }}"
          description: "CPU usage is {{ $value | humanizePercentage }}"
      
      # Memory pressure
      - alert: HighMemoryUsage
        expr: |
          container_memory_working_set_bytes
          / container_spec_memory_limit_bytes
          > 0.9
        for: 10m
        labels:
          severity: critical
          team: platform
        annotations:
          summary: "High memory usage on {{ $labels.pod }}"
          description: "Memory usage is {{ $value | humanizePercentage }} of limit"
      
      # Disk space
      - alert: DiskSpaceLow
        expr: |
          node_filesystem_avail_bytes{mountpoint="/"}
          / node_filesystem_size_bytes{mountpoint="/"}
          < 0.1
        for: 5m
        labels:
          severity: critical
          team: platform
        annotations:
          summary: "Low disk space on {{ $labels.instance }}"
          description: "Only {{ $value | humanizePercentage }} disk space remaining"
```

**Alertmanager Configuration**
```yaml
# alertmanager.yml
global:
  resolve_timeout: 5m
  slack_api_url: '$SLACK_API_URL'
  pagerduty_url: '$PAGERDUTY_URL'

route:
  group_by: ['alertname', 'cluster', 'service']
  group_wait: 10s
  group_interval: 10s
  repeat_interval: 12h
  receiver: 'default'
  
  routes:
    # Critical alerts go to PagerDuty
    - match:
        severity: critical
      receiver: pagerduty
      continue: true
    
    # All alerts go to Slack
    - match_re:
        severity: critical|warning
      receiver: slack
    
    # Database alerts to DBA team
    - match:
        service: database
      receiver: dba-team

receivers:
  - name: 'default'
    
  - name: 'slack'
    slack_configs:
      - channel: '#alerts'
        title: '{{ .GroupLabels.alertname }}'
        text: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'
        send_resolved: true
        actions:
          - type: button
            text: 'Runbook'
            url: '{{ .Annotations.runbook_url }}'
          - type: button
            text: 'Dashboard'
            url: 'https://grafana.company.com/d/{{ .Labels.service }}'
  
  - name: 'pagerduty'
    pagerduty_configs:
      - service_key: '$PAGERDUTY_SERVICE_KEY'
        description: '{{ .GroupLabels.alertname }}: {{ .Annotations.summary }}'
        details:
          firing: '{{ .Alerts.Firing | len }}'
          resolved: '{{ .Alerts.Resolved | len }}'
          alerts: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'

inhibit_rules:
  # Inhibit warning alerts if critical alert is firing
  - source_match:
      severity: 'critical'
    target_match:
      severity: 'warning'
    equal: ['alertname', 'service']
```

### 7. SLO Implementation

Define and monitor Service Level Objectives:

**SLO Configuration**
```typescript
// slo-manager.ts
interface SLO {
    name: string;
    description: string;
    sli: {
        metric: string;
        threshold: number;
        comparison: 'lt' | 'gt' | 'eq';
    };
    target: number; // e.g., 99.9
    window: string; // e.g., '30d'
    burnRates: BurnRate[];
}

interface BurnRate {
    window: string;
    threshold: number;
    severity: 'warning' | 'critical';
}

export class SLOManager {
    private slos: SLO[] = [
        {
            name: 'API Availability',
            description: 'Percentage of successful requests',
            sli: {
                metric: 'http_requests_total{status_code!~"5.."}',
                threshold: 0,
                comparison: 'gt'
            },
            target: 99.9,
            window: '30d',
            burnRates: [
                { window: '1h', threshold: 14.4, severity: 'critical' },
                { window: '6h', threshold: 6, severity: 'critical' },
                { window: '1d', threshold: 3, severity: 'warning' },
                { window: '3d', threshold: 1, severity: 'warning' }
            ]
        },
        {
            name: 'API Latency',
            description: '95th percentile response time under 500ms',
            sli: {
                metric: 'http_request_duration_seconds',
                threshold: 0.5,
                comparison: 'lt'
            },
            target: 99,
            window: '30d',
            burnRates: [
                { window: '1h', threshold: 36, severity: 'critical' },
                { window: '6h', threshold: 12, severity: 'warning' }
            ]
        }
    ];
    
    generateSLOQueries(): string {
        return this.slos.map(slo => this.generateSLOQuery(slo)).join('\n\n');
    }
    
    private generateSLOQuery(slo: SLO): string {
        const errorBudget = 1 - (slo.target / 100);
        
        return `
# ${slo.name} SLO
- record: slo:${this.sanitizeName(slo.name)}:error_budget
  expr: ${errorBudget}

- record: slo:${this.sanitizeName(slo.name)}:consumed_error_budget
  expr: |
    1 - (
      sum(rate(${slo.sli.metric}[${slo.window}]))
      /
      sum(rate(http_requests_total[${slo.window}]))
    )

${slo.burnRates.map(burnRate => `
- alert: ${this.sanitizeName(slo.name)}BurnRate${burnRate.window}
  expr: |
    slo:${this.sanitizeName(slo.name)}:consumed_error_budget
    > ${burnRate.threshold} * slo:${this.sanitizeName(slo.name)}:error_budget
  labels:
    severity: ${burnRate.severity}
    slo: ${slo.name}
  annotations:
    summary: "${slo.name} SLO burn rate too high"
    description: "Burning through error budget ${burnRate.threshold}x faster than sustainable"
`).join('\n')}
        `;
    }
    
    private sanitizeName(name: string): string {
        return name.toLowerCase().replace(/\s+/g, '_').replace(/[^a-z0-9_]/g, '');
    }
}
```

### 8. Monitoring Infrastructure as Code

Deploy monitoring stack with Terraform:

**Terraform Configuration**
```hcl
# monitoring.tf
module "prometheus" {
  source = "./modules/prometheus"
  
  namespace = "monitoring"
  storage_size = "100Gi"
  retention_days = 30
  
  external_labels = {
    cluster = var.cluster_name
    region  = var.region
  }
  
  scrape_configs = [
    {
      job_name = "kubernetes-pods"
      kubernetes_sd_configs = [{
        role = "pod"
      }]
    }
  ]
  
  alerting_rules = file("${path.module}/alerts/*.yml")
}

module "grafana" {
  source = "./modules/grafana"
  
  namespace = "monitoring"
  
  admin_password = var.grafana_admin_password
  
  datasources = [
    {
      name = "Prometheus"
      type = "prometheus"
      url  = "http://prometheus:9090"
    },
    {
      name = "Loki"
      type = "loki"
      url  = "http://loki:3100"
    },
    {
      name = "Jaeger"
      type = "jaeger"
      url  = "http://jaeger-query:16686"
    }
  ]
  
  dashboard_configs = [
    {
      name = "default"
      folder = "General"
      type = "file"
      options = {
        path = "/var/lib/grafana/dashboards"
      }
    }
  ]
}

module "loki" {
  source = "./modules/loki"
  
  namespace = "monitoring"
  storage_size = "50Gi"
  
  ingester_config = {
    chunk_idle_period = "15m"
    chunk_retain_period = "30s"
    max_chunk_age = "1h"
  }
}

module "alertmanager" {
  source = "./modules/alertmanager"
  
  namespace = "monitoring"
  
  config = templatefile("${path.module}/alertmanager.yml", {
    slack_webhook = var.slack_webhook
    pagerduty_key = var.pagerduty_service_key
  })
}
```

## Output Format

1. **Infrastructure Assessment**: Current monitoring capabilities analysis
2. **Monitoring Architecture**: Complete monitoring stack design
3. **Implementation Plan**: Step-by-step deployment guide
4. **Metric Definitions**: Comprehensive metrics catalog
5. **Dashboard Templates**: Ready-to-use Grafana dashboards
6. **Alert Runbooks**: Detailed alert response procedures
7. **SLO Definitions**: Service level objectives and error budgets
8. **Integration Guide**: Service instrumentation instructions

Focus on creating a monitoring system that provides actionable insights, reduces MTTR, and enables proactive issue detection.
````

## File: tools/multi-agent-optimize.md

````markdown
Optimize application stack using specialized optimization agents:

[Extended thinking: This tool coordinates database, performance, and frontend optimization agents to improve application performance holistically. Each agent focuses on their domain while ensuring optimizations work together.]

## Optimization Strategy

### 1. Database Optimization
Use Task tool with subagent_type="database-optimizer" to:
- Analyze query performance and execution plans
- Optimize indexes and table structures
- Implement caching strategies
- Review connection pooling and configurations
- Suggest schema improvements

Prompt: "Optimize database layer for: $ARGUMENTS. Analyze and improve:
1. Slow query identification and optimization
2. Index analysis and recommendations
3. Schema optimization for performance
4. Connection pool tuning
5. Caching strategy implementation"

### 2. Application Performance
Use Task tool with subagent_type="performance-engineer" to:
- Profile application code
- Identify CPU and memory bottlenecks
- Optimize algorithms and data structures
- Implement caching at application level
- Improve async/concurrent operations

Prompt: "Optimize application performance for: $ARGUMENTS. Focus on:
1. Code profiling and bottleneck identification
2. Algorithm optimization
3. Memory usage optimization
4. Concurrency improvements
5. Application-level caching"

### 3. Frontend Optimization
Use Task tool with subagent_type="frontend-developer" to:
- Reduce bundle sizes
- Implement lazy loading
- Optimize rendering performance
- Improve Core Web Vitals
- Implement efficient state management

Prompt: "Optimize frontend performance for: $ARGUMENTS. Improve:
1. Bundle size reduction strategies
2. Lazy loading implementation
3. Rendering optimization
4. Core Web Vitals (LCP, FID, CLS)
5. Network request optimization"

## Consolidated Optimization Plan

### Performance Baseline
- Current performance metrics
- Identified bottlenecks
- User experience impact

### Optimization Roadmap
1. **Quick Wins** (< 1 day)
   - Simple query optimizations
   - Basic caching implementation
   - Bundle splitting

2. **Medium Improvements** (1-3 days)
   - Index optimization
   - Algorithm improvements
   - Lazy loading implementation

3. **Major Optimizations** (3+ days)
   - Schema redesign
   - Architecture changes
   - Full caching layer

### Expected Improvements
- Database query time reduction: X%
- API response time improvement: X%
- Frontend load time reduction: X%
- Overall user experience impact

### Implementation Priority
- Ordered list of optimizations by impact/effort ratio
- Dependencies between optimizations
- Risk assessment for each change

Target for optimization: $ARGUMENTS
````

## File: tools/multi-agent-review.md

````markdown
Perform comprehensive multi-agent code review with specialized reviewers:

[Extended thinking: This tool command invokes multiple review-focused agents to provide different perspectives on code quality, security, and architecture. Each agent reviews independently, then findings are consolidated.]

## Review Process

### 1. Code Quality Review
Use Task tool with subagent_type="code-reviewer" to examine:
- Code style and readability
- Adherence to SOLID principles
- Design patterns and anti-patterns
- Code duplication and complexity
- Documentation completeness
- Test coverage and quality

Prompt: "Perform detailed code review of: $ARGUMENTS. Focus on maintainability, readability, and best practices. Provide specific line-by-line feedback where appropriate."

### 2. Security Review
Use Task tool with subagent_type="security-auditor" to check:
- Authentication and authorization flaws
- Input validation and sanitization
- SQL injection and XSS vulnerabilities
- Sensitive data exposure
- Security misconfigurations
- Dependency vulnerabilities

Prompt: "Conduct security review of: $ARGUMENTS. Identify vulnerabilities, security risks, and OWASP compliance issues. Provide severity ratings and remediation steps."

### 3. Architecture Review
Use Task tool with subagent_type="architect-reviewer" to evaluate:
- Service boundaries and coupling
- Scalability considerations
- Design pattern appropriateness
- Technology choices
- API design quality
- Data flow and dependencies

Prompt: "Review architecture and design of: $ARGUMENTS. Evaluate scalability, maintainability, and architectural patterns. Identify potential bottlenecks and design improvements."

## Consolidated Review Output

After all agents complete their reviews, consolidate findings into:

1. **Critical Issues** - Must fix before merge
   - Security vulnerabilities
   - Broken functionality
   - Major architectural flaws

2. **Important Issues** - Should fix soon
   - Performance problems
   - Code quality issues
   - Missing tests

3. **Minor Issues** - Nice to fix
   - Style inconsistencies
   - Documentation gaps
   - Refactoring opportunities

4. **Positive Findings** - Good practices to highlight
   - Well-designed components
   - Good test coverage
   - Security best practices

Target for review: $ARGUMENTS
````

## File: tools/onboard.md

````markdown
# Onboard

You are given the following context:
$ARGUMENTS

## Instructions

"AI models are geniuses who start from scratch on every task." - Noam Brown

Your job is to "onboard" yourself to the current task.

Do this by:

- Using ultrathink
- Exploring the codebase
- Making use of any MCP tools at your disposal for planning and research
- Asking me questions if needed
- Using subagents for dividing work and seperation of concerns

The goal is to get you fully prepared to start working on the task.

Take as long as you need to get yourself ready. Overdoing it is better than underdoing it. 

Record everything in a .claude/tasks/[TASK_ID]/onboarding.md file. This file will be used to onboard you to the task in a new session if needed, so make sure it's comprehensive.
````

## File: tools/pr-enhance.md

````markdown
# Pull Request Enhancement

You are a PR optimization expert specializing in creating high-quality pull requests that facilitate efficient code reviews. Generate comprehensive PR descriptions, automate review processes, and ensure PRs follow best practices for clarity, size, and reviewability.

## Context
The user needs to create or improve pull requests with detailed descriptions, proper documentation, test coverage analysis, and review facilitation. Focus on making PRs that are easy to review, well-documented, and include all necessary context.

## Requirements
$ARGUMENTS

## Instructions

### 1. PR Analysis

Analyze the changes and generate insights:

**Change Summary Generator**
```python
import subprocess
import re
from collections import defaultdict

class PRAnalyzer:
    def analyze_changes(self, base_branch='main'):
        """
        Analyze changes between current branch and base
        """
        analysis = {
            'files_changed': self._get_changed_files(base_branch),
            'change_statistics': self._get_change_stats(base_branch),
            'change_categories': self._categorize_changes(base_branch),
            'potential_impacts': self._assess_impacts(base_branch),
            'dependencies_affected': self._check_dependencies(base_branch)
        }
        
        return analysis
    
    def _get_changed_files(self, base_branch):
        """Get list of changed files with statistics"""
        cmd = f"git diff --name-status {base_branch}...HEAD"
        result = subprocess.run(cmd.split(), capture_output=True, text=True)
        
        files = []
        for line in result.stdout.strip().split('\n'):
            if line:
                status, filename = line.split('\t', 1)
                files.append({
                    'filename': filename,
                    'status': self._parse_status(status),
                    'category': self._categorize_file(filename)
                })
        
        return files
    
    def _get_change_stats(self, base_branch):
        """Get detailed change statistics"""
        cmd = f"git diff --shortstat {base_branch}...HEAD"
        result = subprocess.run(cmd.split(), capture_output=True, text=True)
        
        # Parse output like: "10 files changed, 450 insertions(+), 123 deletions(-)"
        stats_pattern = r'(\d+) files? changed(?:, (\d+) insertions?\(\+\))?(?:, (\d+) deletions?\(-\))?'
        match = re.search(stats_pattern, result.stdout)
        
        if match:
            files, insertions, deletions = match.groups()
            return {
                'files_changed': int(files),
                'insertions': int(insertions or 0),
                'deletions': int(deletions or 0),
                'net_change': int(insertions or 0) - int(deletions or 0)
            }
        
        return {'files_changed': 0, 'insertions': 0, 'deletions': 0, 'net_change': 0}
    
    def _categorize_file(self, filename):
        """Categorize file by type"""
        categories = {
            'source': ['.js', '.ts', '.py', '.java', '.go', '.rs'],
            'test': ['test', 'spec', '.test.', '.spec.'],
            'config': ['config', '.json', '.yml', '.yaml', '.toml'],
            'docs': ['.md', 'README', 'CHANGELOG', '.rst'],
            'styles': ['.css', '.scss', '.less'],
            'build': ['Makefile', 'Dockerfile', '.gradle', 'pom.xml']
        }
        
        for category, patterns in categories.items():
            if any(pattern in filename for pattern in patterns):
                return category
        
        return 'other'
```

### 2. PR Description Generation

Create comprehensive PR descriptions:

**Description Template Generator**
```python
def generate_pr_description(analysis, commits):
    """
    Generate detailed PR description from analysis
    """
    description = f"""
## Summary

{generate_summary(analysis, commits)}

## What Changed

{generate_change_list(analysis)}

## Why These Changes

{extract_why_from_commits(commits)}

## Type of Change

{determine_change_types(analysis)}

## How Has This Been Tested?

{generate_test_section(analysis)}

## Visual Changes

{generate_visual_section(analysis)}

## Performance Impact

{analyze_performance_impact(analysis)}

## Breaking Changes

{identify_breaking_changes(analysis)}

## Dependencies

{list_dependency_changes(analysis)}

## Checklist

{generate_review_checklist(analysis)}

## Additional Notes

{generate_additional_notes(analysis)}
"""
    return description

def generate_summary(analysis, commits):
    """Generate executive summary"""
    stats = analysis['change_statistics']
    
    # Extract main purpose from commits
    main_purpose = extract_main_purpose(commits)
    
    summary = f"""
This PR {main_purpose}.

**Impact**: {stats['files_changed']} files changed ({stats['insertions']} additions, {stats['deletions']} deletions)
**Risk Level**: {calculate_risk_level(analysis)}
**Review Time**: ~{estimate_review_time(stats)} minutes
"""
    return summary

def generate_change_list(analysis):
    """Generate categorized change list"""
    changes_by_category = defaultdict(list)
    
    for file in analysis['files_changed']:
        changes_by_category[file['category']].append(file)
    
    change_list = ""
    icons = {
        'source': 'üîß',
        'test': '‚úÖ',
        'docs': 'üìù',
        'config': '‚öôÔ∏è',
        'styles': 'üé®',
        'build': 'üèóÔ∏è',
        'other': 'üìÅ'
    }
    
    for category, files in changes_by_category.items():
        change_list += f"\n### {icons.get(category, 'üìÅ')} {category.title()} Changes\n"
        for file in files[:10]:  # Limit to 10 files per category
            change_list += f"- {file['status']}: `{file['filename']}`\n"
        if len(files) > 10:
            change_list += f"- ...and {len(files) - 10} more\n"
    
    return change_list
```

### 3. Review Checklist Generation

Create automated review checklists:

**Smart Checklist Generator**
```python
def generate_review_checklist(analysis):
    """
    Generate context-aware review checklist
    """
    checklist = ["## Review Checklist\n"]
    
    # General items
    general_items = [
        "Code follows project style guidelines",
        "Self-review completed",
        "Comments added for complex logic",
        "No debugging code left",
        "No sensitive data exposed"
    ]
    
    # Add general items
    checklist.append("### General")
    for item in general_items:
        checklist.append(f"- [ ] {item}")
    
    # File-specific checks
    file_types = {file['category'] for file in analysis['files_changed']}
    
    if 'source' in file_types:
        checklist.append("\n### Code Quality")
        checklist.extend([
            "- [ ] No code duplication",
            "- [ ] Functions are focused and small",
            "- [ ] Variable names are descriptive",
            "- [ ] Error handling is comprehensive",
            "- [ ] No performance bottlenecks introduced"
        ])
    
    if 'test' in file_types:
        checklist.append("\n### Testing")
        checklist.extend([
            "- [ ] All new code is covered by tests",
            "- [ ] Tests are meaningful and not just for coverage",
            "- [ ] Edge cases are tested",
            "- [ ] Tests follow AAA pattern (Arrange, Act, Assert)",
            "- [ ] No flaky tests introduced"
        ])
    
    if 'config' in file_types:
        checklist.append("\n### Configuration")
        checklist.extend([
            "- [ ] No hardcoded values",
            "- [ ] Environment variables documented",
            "- [ ] Backwards compatibility maintained",
            "- [ ] Security implications reviewed",
            "- [ ] Default values are sensible"
        ])
    
    if 'docs' in file_types:
        checklist.append("\n### Documentation")
        checklist.extend([
            "- [ ] Documentation is clear and accurate",
            "- [ ] Examples are provided where helpful",
            "- [ ] API changes are documented",
            "- [ ] README updated if necessary",
            "- [ ] Changelog updated"
        ])
    
    # Security checks
    if has_security_implications(analysis):
        checklist.append("\n### Security")
        checklist.extend([
            "- [ ] No SQL injection vulnerabilities",
            "- [ ] Input validation implemented",
            "- [ ] Authentication/authorization correct",
            "- [ ] No sensitive data in logs",
            "- [ ] Dependencies are secure"
        ])
    
    return '\n'.join(checklist)
```

### 4. Code Review Automation

Automate common review tasks:

**Automated Review Bot**
```python
class ReviewBot:
    def perform_automated_checks(self, pr_diff):
        """
        Perform automated code review checks
        """
        findings = []
        
        # Check for common issues
        checks = [
            self._check_console_logs,
            self._check_commented_code,
            self._check_large_functions,
            self._check_todo_comments,
            self._check_hardcoded_values,
            self._check_missing_error_handling,
            self._check_security_issues
        ]
        
        for check in checks:
            findings.extend(check(pr_diff))
        
        return findings
    
    def _check_console_logs(self, diff):
        """Check for console.log statements"""
        findings = []
        pattern = r'\+.*console\.(log|debug|info|warn|error)'
        
        for file, content in diff.items():
            matches = re.finditer(pattern, content, re.MULTILINE)
            for match in matches:
                findings.append({
                    'type': 'warning',
                    'file': file,
                    'line': self._get_line_number(match, content),
                    'message': 'Console statement found - remove before merging',
                    'suggestion': 'Use proper logging framework instead'
                })
        
        return findings
    
    def _check_large_functions(self, diff):
        """Check for functions that are too large"""
        findings = []
        
        # Simple heuristic: count lines between function start and end
        for file, content in diff.items():
            if file.endswith(('.js', '.ts', '.py')):
                functions = self._extract_functions(content)
                for func in functions:
                    if func['lines'] > 50:
                        findings.append({
                            'type': 'suggestion',
                            'file': file,
                            'line': func['start_line'],
                            'message': f"Function '{func['name']}' is {func['lines']} lines long",
                            'suggestion': 'Consider breaking into smaller functions'
                        })
        
        return findings
```

### 5. PR Size Optimization

Help split large PRs:

**PR Splitter Suggestions**
```python
def suggest_pr_splits(analysis):
    """
    Suggest how to split large PRs
    """
    stats = analysis['change_statistics']
    
    # Check if PR is too large
    if stats['files_changed'] > 20 or stats['insertions'] + stats['deletions'] > 1000:
        suggestions = analyze_split_opportunities(analysis)
        
        return f"""
## ‚ö†Ô∏è Large PR Detected

This PR changes {stats['files_changed']} files with {stats['insertions'] + stats['deletions']} total changes.
Large PRs are harder to review and more likely to introduce bugs.

### Suggested Splits:

{format_split_suggestions(suggestions)}

### How to Split:

1. Create feature branch from current branch
2. Cherry-pick commits for first logical unit
3. Create PR for first unit
4. Repeat for remaining units

```bash
# Example split workflow
git checkout -b feature/part-1
git cherry-pick <commit-hashes-for-part-1>
git push origin feature/part-1
# Create PR for part 1

git checkout -b feature/part-2
git cherry-pick <commit-hashes-for-part-2>
git push origin feature/part-2
# Create PR for part 2
```
"""
    
    return ""

def analyze_split_opportunities(analysis):
    """Find logical units for splitting"""
    suggestions = []
    
    # Group by feature areas
    feature_groups = defaultdict(list)
    for file in analysis['files_changed']:
        feature = extract_feature_area(file['filename'])
        feature_groups[feature].append(file)
    
    # Suggest splits
    for feature, files in feature_groups.items():
        if len(files) >= 5:
            suggestions.append({
                'name': f"{feature} changes",
                'files': files,
                'reason': f"Isolated changes to {feature} feature"
            })
    
    return suggestions
```

### 6. Visual Diff Enhancement

Generate visual representations:

**Mermaid Diagram Generator**
```python
def generate_architecture_diff(analysis):
    """
    Generate diagram showing architectural changes
    """
    if has_architectural_changes(analysis):
        return f"""
## Architecture Changes

```mermaid
graph LR
    subgraph "Before"
        A1[Component A] --> B1[Component B]
        B1 --> C1[Database]
    end
    
    subgraph "After"
        A2[Component A] --> B2[Component B]
        B2 --> C2[Database]
        B2 --> D2[New Cache Layer]
        A2 --> E2[New API Gateway]
    end
    
    style D2 fill:#90EE90
    style E2 fill:#90EE90
```

### Key Changes:
1. Added caching layer for performance
2. Introduced API gateway for better routing
3. Refactored component communication
"""
    return ""
```

### 7. Test Coverage Report

Include test coverage analysis:

**Coverage Report Generator**
```python
def generate_coverage_report(base_branch='main'):
    """
    Generate test coverage comparison
    """
    # Get coverage before and after
    before_coverage = get_coverage_for_branch(base_branch)
    after_coverage = get_coverage_for_branch('HEAD')
    
    coverage_diff = after_coverage - before_coverage
    
    report = f"""
## Test Coverage

| Metric | Before | After | Change |
|--------|--------|-------|--------|
| Lines | {before_coverage['lines']:.1f}% | {after_coverage['lines']:.1f}% | {format_diff(coverage_diff['lines'])} |
| Functions | {before_coverage['functions']:.1f}% | {after_coverage['functions']:.1f}% | {format_diff(coverage_diff['functions'])} |
| Branches | {before_coverage['branches']:.1f}% | {after_coverage['branches']:.1f}% | {format_diff(coverage_diff['branches'])} |

### Uncovered Files
"""
    
    # List files with low coverage
    for file in get_low_coverage_files():
        report += f"- `{file['name']}`: {file['coverage']:.1f}% coverage\n"
    
    return report

def format_diff(value):
    """Format coverage difference"""
    if value > 0:
        return f"<span style='color: green'>+{value:.1f}%</span> ‚úÖ"
    elif value < 0:
        return f"<span style='color: red'>{value:.1f}%</span> ‚ö†Ô∏è"
    else:
        return "No change"
```

### 8. Risk Assessment

Evaluate PR risk:

**Risk Calculator**
```python
def calculate_pr_risk(analysis):
    """
    Calculate risk score for PR
    """
    risk_factors = {
        'size': calculate_size_risk(analysis),
        'complexity': calculate_complexity_risk(analysis),
        'test_coverage': calculate_test_risk(analysis),
        'dependencies': calculate_dependency_risk(analysis),
        'security': calculate_security_risk(analysis)
    }
    
    overall_risk = sum(risk_factors.values()) / len(risk_factors)
    
    risk_report = f"""
## Risk Assessment

**Overall Risk Level**: {get_risk_level(overall_risk)} ({overall_risk:.1f}/10)

### Risk Factors

| Factor | Score | Details |
|--------|-------|---------|
| Size | {risk_factors['size']:.1f}/10 | {get_size_details(analysis)} |
| Complexity | {risk_factors['complexity']:.1f}/10 | {get_complexity_details(analysis)} |
| Test Coverage | {risk_factors['test_coverage']:.1f}/10 | {get_test_details(analysis)} |
| Dependencies | {risk_factors['dependencies']:.1f}/10 | {get_dependency_details(analysis)} |
| Security | {risk_factors['security']:.1f}/10 | {get_security_details(analysis)} |

### Mitigation Strategies

{generate_mitigation_strategies(risk_factors)}
"""
    
    return risk_report

def get_risk_level(score):
    """Convert score to risk level"""
    if score < 3:
        return "üü¢ Low"
    elif score < 6:
        return "üü° Medium"
    elif score < 8:
        return "üü† High"
    else:
        return "üî¥ Critical"
```

### 9. PR Templates

Generate context-specific templates:

```python
def generate_pr_template(pr_type, analysis):
    """
    Generate PR template based on type
    """
    templates = {
        'feature': f"""
## Feature: {extract_feature_name(analysis)}

### Description
{generate_feature_description(analysis)}

### User Story
As a [user type]
I want [feature]
So that [benefit]

### Acceptance Criteria
- [ ] Criterion 1
- [ ] Criterion 2
- [ ] Criterion 3

### Demo
[Link to demo or screenshots]

### Technical Implementation
{generate_technical_summary(analysis)}

### Testing Strategy
{generate_test_strategy(analysis)}
""",
        'bugfix': f"""
## Bug Fix: {extract_bug_description(analysis)}

### Issue
- **Reported in**: #[issue-number]
- **Severity**: {determine_severity(analysis)}
- **Affected versions**: {get_affected_versions(analysis)}

### Root Cause
{analyze_root_cause(analysis)}

### Solution
{describe_solution(analysis)}

### Testing
- [ ] Bug is reproducible before fix
- [ ] Bug is resolved after fix
- [ ] No regressions introduced
- [ ] Edge cases tested

### Verification Steps
1. Step to reproduce original issue
2. Apply this fix
3. Verify issue is resolved
""",
        'refactor': f"""
## Refactoring: {extract_refactor_scope(analysis)}

### Motivation
{describe_refactor_motivation(analysis)}

### Changes Made
{list_refactor_changes(analysis)}

### Benefits
- Improved {list_improvements(analysis)}
- Reduced {list_reductions(analysis)}

### Compatibility
- [ ] No breaking changes
- [ ] API remains unchanged
- [ ] Performance maintained or improved

### Metrics
| Metric | Before | After |
|--------|--------|-------|
| Complexity | X | Y |
| Test Coverage | X% | Y% |
| Performance | Xms | Yms |
"""
    }
    
    return templates.get(pr_type, templates['feature'])
```

### 10. Review Response Templates

Help with review responses:

```python
review_response_templates = {
    'acknowledge_feedback': """
Thank you for the thorough review! I'll address these points.
""",
    
    'explain_decision': """
Great question! I chose this approach because:
1. [Reason 1]
2. [Reason 2]

Alternative approaches considered:
- [Alternative 1]: [Why not chosen]
- [Alternative 2]: [Why not chosen]

Happy to discuss further if you have concerns.
""",
    
    'request_clarification': """
Thanks for the feedback. Could you clarify what you mean by [specific point]?
I want to make sure I understand your concern correctly before making changes.
""",
    
    'disagree_respectfully': """
I appreciate your perspective on this. I have a slightly different view:

[Your reasoning]

However, I'm open to discussing this further. What do you think about [compromise/middle ground]?
""",
    
    'commit_to_change': """
Good catch! I'll update this to [specific change].
This should address [concern] while maintaining [other requirement].
"""
}
```

## Output Format

1. **PR Summary**: Executive summary with key metrics
2. **Detailed Description**: Comprehensive PR description
3. **Review Checklist**: Context-aware review items  
4. **Risk Assessment**: Risk analysis with mitigation strategies
5. **Test Coverage**: Before/after coverage comparison
6. **Visual Aids**: Diagrams and visual diffs where applicable
7. **Size Recommendations**: Suggestions for splitting large PRs
8. **Review Automation**: Automated checks and findings

Focus on creating PRs that are a pleasure to review, with all necessary context and documentation for efficient code review process.
````

## File: tools/prompt-optimize.md

````markdown
# AI Prompt Optimization

Optimize the following prompt for better AI model performance: $ARGUMENTS

Analyze and improve the prompt by:

1. **Prompt Engineering**:
   - Apply chain-of-thought reasoning
   - Add few-shot examples
   - Implement role-based instructions
   - Use clear delimiters and formatting
   - Add output format specifications

2. **Context Optimization**:
   - Minimize token usage
   - Structure information hierarchically
   - Remove redundant information
   - Add relevant context
   - Use compression techniques

3. **Performance Testing**:
   - Create prompt variants
   - Design evaluation criteria
   - Test edge cases
   - Measure consistency
   - Compare model outputs

4. **Model-Specific Optimization**:
   - GPT-4 best practices
   - Claude optimization techniques
   - Prompt chaining strategies
   - Temperature/parameter tuning
   - Token budget management

5. **RAG Integration**:
   - Context window management
   - Retrieval query optimization
   - Chunk size recommendations
   - Embedding strategies
   - Reranking approaches

6. **Production Considerations**:
   - Prompt versioning
   - A/B testing framework
   - Monitoring metrics
   - Fallback strategies
   - Cost optimization

Provide optimized prompts with explanations for each change. Include evaluation metrics and testing strategies. Consider both quality and cost efficiency.
````

## File: tools/refactor-clean.md

````markdown
# Refactor and Clean Code

You are a code refactoring expert specializing in clean code principles, SOLID design patterns, and modern software engineering best practices. Analyze and refactor the provided code to improve its quality, maintainability, and performance.

## Context
The user needs help refactoring code to make it cleaner, more maintainable, and aligned with best practices. Focus on practical improvements that enhance code quality without over-engineering.

## Requirements
$ARGUMENTS

## Instructions

### 1. Code Analysis
First, analyze the current code for:
- **Code Smells**
  - Long methods/functions (>20 lines)
  - Large classes (>200 lines)
  - Duplicate code blocks
  - Dead code and unused variables
  - Complex conditionals and nested loops
  - Magic numbers and hardcoded values
  - Poor naming conventions
  - Tight coupling between components
  - Missing abstractions

- **SOLID Violations**
  - Single Responsibility Principle violations
  - Open/Closed Principle issues
  - Liskov Substitution problems
  - Interface Segregation concerns
  - Dependency Inversion violations

- **Performance Issues**
  - Inefficient algorithms (O(n¬≤) or worse)
  - Unnecessary object creation
  - Memory leaks potential
  - Blocking operations
  - Missing caching opportunities

### 2. Refactoring Strategy

Create a prioritized refactoring plan:

**Immediate Fixes (High Impact, Low Effort)**
- Extract magic numbers to constants
- Improve variable and function names
- Remove dead code
- Simplify boolean expressions
- Extract duplicate code to functions

**Method Extraction**
```
# Before
def process_order(order):
    # 50 lines of validation
    # 30 lines of calculation
    # 40 lines of notification
    
# After
def process_order(order):
    validate_order(order)
    total = calculate_order_total(order)
    send_order_notifications(order, total)
```

**Class Decomposition**
- Extract responsibilities to separate classes
- Create interfaces for dependencies
- Implement dependency injection
- Use composition over inheritance

**Pattern Application**
- Factory pattern for object creation
- Strategy pattern for algorithm variants
- Observer pattern for event handling
- Repository pattern for data access
- Decorator pattern for extending behavior

### 3. Refactored Implementation

Provide the complete refactored code with:

**Clean Code Principles**
- Meaningful names (searchable, pronounceable, no abbreviations)
- Functions do one thing well
- No side effects
- Consistent abstraction levels
- DRY (Don't Repeat Yourself)
- YAGNI (You Aren't Gonna Need It)

**Error Handling**
```python
# Use specific exceptions
class OrderValidationError(Exception):
    pass

class InsufficientInventoryError(Exception):
    pass

# Fail fast with clear messages
def validate_order(order):
    if not order.items:
        raise OrderValidationError("Order must contain at least one item")
    
    for item in order.items:
        if item.quantity <= 0:
            raise OrderValidationError(f"Invalid quantity for {item.name}")
```

**Documentation**
```python
def calculate_discount(order: Order, customer: Customer) -> Decimal:
    """
    Calculate the total discount for an order based on customer tier and order value.
    
    Args:
        order: The order to calculate discount for
        customer: The customer making the order
        
    Returns:
        The discount amount as a Decimal
        
    Raises:
        ValueError: If order total is negative
    """
```

### 4. Testing Strategy

Generate comprehensive tests for the refactored code:

**Unit Tests**
```python
class TestOrderProcessor:
    def test_validate_order_empty_items(self):
        order = Order(items=[])
        with pytest.raises(OrderValidationError):
            validate_order(order)
    
    def test_calculate_discount_vip_customer(self):
        order = create_test_order(total=1000)
        customer = Customer(tier="VIP")
        discount = calculate_discount(order, customer)
        assert discount == Decimal("100.00")  # 10% VIP discount
```

**Test Coverage**
- All public methods tested
- Edge cases covered
- Error conditions verified
- Performance benchmarks included

### 5. Before/After Comparison

Provide clear comparisons showing improvements:

**Metrics**
- Cyclomatic complexity reduction
- Lines of code per method
- Test coverage increase
- Performance improvements

**Example**
```
Before:
- processData(): 150 lines, complexity: 25
- 0% test coverage
- 3 responsibilities mixed

After:
- validateInput(): 20 lines, complexity: 4
- transformData(): 25 lines, complexity: 5  
- saveResults(): 15 lines, complexity: 3
- 95% test coverage
- Clear separation of concerns
```

### 6. Migration Guide

If breaking changes are introduced:

**Step-by-Step Migration**
1. Install new dependencies
2. Update import statements
3. Replace deprecated methods
4. Run migration scripts
5. Execute test suite

**Backward Compatibility**
```python
# Temporary adapter for smooth migration
class LegacyOrderProcessor:
    def __init__(self):
        self.processor = OrderProcessor()
    
    def process(self, order_data):
        # Convert legacy format
        order = Order.from_legacy(order_data)
        return self.processor.process(order)
```

### 7. Performance Optimizations

Include specific optimizations:

**Algorithm Improvements**
```python
# Before: O(n¬≤)
for item in items:
    for other in items:
        if item.id == other.id:
            # process

# After: O(n)
item_map = {item.id: item for item in items}
for item_id, item in item_map.items():
    # process
```

**Caching Strategy**
```python
from functools import lru_cache

@lru_cache(maxsize=128)
def calculate_expensive_metric(data_id: str) -> float:
    # Expensive calculation cached
    return result
```

### 8. Code Quality Checklist

Ensure the refactored code meets these criteria:

- [ ] All methods < 20 lines
- [ ] All classes < 200 lines
- [ ] No method has > 3 parameters
- [ ] Cyclomatic complexity < 10
- [ ] No nested loops > 2 levels
- [ ] All names are descriptive
- [ ] No commented-out code
- [ ] Consistent formatting
- [ ] Type hints added (Python/TypeScript)
- [ ] Error handling comprehensive
- [ ] Logging added for debugging
- [ ] Performance metrics included
- [ ] Documentation complete
- [ ] Tests achieve > 80% coverage
- [ ] No security vulnerabilities

## Severity Levels

Rate issues found and improvements made:

**Critical**: Security vulnerabilities, data corruption risks, memory leaks
**High**: Performance bottlenecks, maintainability blockers, missing tests
**Medium**: Code smells, minor performance issues, incomplete documentation  
**Low**: Style inconsistencies, minor naming issues, nice-to-have features

## Output Format

1. **Analysis Summary**: Key issues found and their impact
2. **Refactoring Plan**: Prioritized list of changes with effort estimates
3. **Refactored Code**: Complete implementation with inline comments explaining changes
4. **Test Suite**: Comprehensive tests for all refactored components
5. **Migration Guide**: Step-by-step instructions for adopting changes
6. **Metrics Report**: Before/after comparison of code quality metrics

Focus on delivering practical, incremental improvements that can be adopted immediately while maintaining system stability.
````

## File: tools/security-scan.md

````markdown
# Security Scan and Vulnerability Assessment

You are a security expert specializing in application security, vulnerability assessment, and secure coding practices. Perform comprehensive security audits to identify vulnerabilities, provide remediation guidance, and implement security best practices.

## Context
The user needs a thorough security analysis to identify vulnerabilities, assess risks, and implement protection measures. Focus on OWASP Top 10, dependency vulnerabilities, and security misconfigurations with actionable remediation steps.

## Requirements
$ARGUMENTS

## Instructions

### 1. Security Scanning Tool Selection

Choose appropriate security scanning tools based on your technology stack and requirements:

**Tool Selection Matrix**
```python
security_tools = {
    'python': {
        'sast': {
            'bandit': {
                'strengths': ['Built for Python', 'Fast', 'Good defaults', 'AST-based'],
                'best_for': ['Python codebases', 'CI/CD pipelines', 'Quick scans'],
                'command': 'bandit -r . -f json -o bandit-report.json',
                'config_file': '.bandit'
            },
            'semgrep': {
                'strengths': ['Multi-language', 'Custom rules', 'Low false positives'],
                'best_for': ['Complex projects', 'Custom security patterns', 'Enterprise'],
                'command': 'semgrep --config=auto --json --output=semgrep-report.json',
                'config_file': '.semgrep.yml'
            }
        },
        'dependency_scan': {
            'safety': {
                'command': 'safety check --json --output safety-report.json',
                'database': 'PyUp.io vulnerability database',
                'best_for': 'Python package vulnerabilities'
            },
            'pip_audit': {
                'command': 'pip-audit --format=json --output=pip-audit-report.json',
                'database': 'OSV database',
                'best_for': 'Comprehensive Python vulnerability scanning'
            }
        }
    },
    
    'javascript': {
        'sast': {
            'eslint_security': {
                'command': 'eslint . --ext .js,.jsx,.ts,.tsx --format json > eslint-security.json',
                'plugins': ['@eslint/plugin-security', 'eslint-plugin-no-secrets'],
                'best_for': 'JavaScript/TypeScript security linting'
            },
            'sonarjs': {
                'command': 'sonar-scanner -Dsonar.projectKey=myproject',
                'best_for': 'Comprehensive code quality and security',
                'features': ['Vulnerability detection', 'Code smells', 'Technical debt']
            }
        },
        'dependency_scan': {
            'npm_audit': {
                'command': 'npm audit --json > npm-audit-report.json',
                'fix': 'npm audit fix',
                'best_for': 'NPM package vulnerabilities'
            },
            'yarn_audit': {
                'command': 'yarn audit --json > yarn-audit-report.json',
                'best_for': 'Yarn package vulnerabilities'
            },
            'snyk': {
                'command': 'snyk test --json > snyk-report.json',
                'fix': 'snyk wizard',
                'best_for': 'Comprehensive vulnerability management'
            }
        }
    },
    
    'container': {
        'trivy': {
            'image_scan': 'trivy image --format json --output trivy-image.json myimage:latest',
            'fs_scan': 'trivy fs --format json --output trivy-fs.json .',
            'repo_scan': 'trivy repo --format json --output trivy-repo.json .',
            'strengths': ['Fast', 'Accurate', 'Multiple targets', 'SBOM generation'],
            'best_for': 'Container and filesystem vulnerability scanning'
        },
        'grype': {
            'command': 'grype dir:. -o json > grype-report.json',
            'strengths': ['Fast', 'Accurate vulnerability detection'],
            'best_for': 'Container image and filesystem scanning'
        },
        'clair': {
            'api_based': True,
            'strengths': ['API-driven', 'Continuous monitoring'],
            'best_for': 'Registry integration, automated scanning'
        }
    },
    
    'infrastructure': {
        'checkov': {
            'command': 'checkov -d . --framework terraform --output json > checkov-report.json',
            'supports': ['Terraform', 'CloudFormation', 'Kubernetes', 'Helm', 'Serverless'],
            'best_for': 'Infrastructure as Code security'
        },
        'tfsec': {
            'command': 'tfsec . --format json > tfsec-report.json',
            'supports': ['Terraform'],
            'best_for': 'Terraform-specific security scanning'
        },
        'kube_score': {
            'command': 'kube-score score *.yaml --output-format json > kube-score.json',
            'supports': ['Kubernetes'],
            'best_for': 'Kubernetes manifest security and best practices'
        }
    },
    
    'secrets': {
        'truffleHog': {
            'command': 'trufflehog git file://. --json > trufflehog-report.json',
            'strengths': ['Git history scanning', 'High accuracy', 'Custom regex'],
            'best_for': 'Secret detection in git repositories'
        },
        'gitleaks': {
            'command': 'gitleaks detect --report-format json --report-path gitleaks-report.json',
            'strengths': ['Fast', 'Configurable', 'Pre-commit hooks'],
            'best_for': 'Real-time secret detection'
        },
        'detect_secrets': {
            'command': 'detect-secrets scan --all-files . > .secrets.baseline',
            'strengths': ['Baseline management', 'False positive reduction'],
            'best_for': 'Enterprise secret management'
        }
    }
}
```

**Multi-Tool Security Scanner**
```python
import json
import subprocess
import os
from pathlib import Path
from typing import Dict, List, Any
from dataclasses import dataclass
from datetime import datetime

@dataclass
class VulnerabilityFinding:
    tool: str
    severity: str
    category: str
    title: str
    description: str
    file_path: str
    line_number: int
    cve: str
    cwe: str
    remediation: str
    confidence: str

class SecurityScanner:
    def __init__(self, project_path: str):
        self.project_path = Path(project_path)
        self.findings = []
        self.scan_results = {}
        
    def detect_project_type(self) -> List[str]:
        """Detect project technologies to choose appropriate scanners"""
        technologies = []
        
        # Python
        if (self.project_path / 'requirements.txt').exists() or \
           (self.project_path / 'setup.py').exists() or \
           (self.project_path / 'pyproject.toml').exists():
            technologies.append('python')
            
        # JavaScript/Node.js
        if (self.project_path / 'package.json').exists():
            technologies.append('javascript')
            
        # Go
        if (self.project_path / 'go.mod').exists():
            technologies.append('golang')
            
        # Docker
        if (self.project_path / 'Dockerfile').exists():
            technologies.append('container')
            
        # Terraform
        if list(self.project_path.glob('*.tf')):
            technologies.append('terraform')
            
        # Kubernetes
        if list(self.project_path.glob('*.yaml')) or list(self.project_path.glob('*.yml')):
            technologies.append('kubernetes')
            
        return technologies
    
    def run_comprehensive_scan(self) -> Dict[str, Any]:
        """Run all applicable security scanners"""
        technologies = self.detect_project_type()
        
        scan_plan = {
            'timestamp': datetime.now().isoformat(),
            'technologies': technologies,
            'scanners_used': [],
            'findings': []
        }
        
        # Always run secret detection
        self.run_secret_scan()
        scan_plan['scanners_used'].append('secret_detection')
        
        # Technology-specific scans
        if 'python' in technologies:
            self.run_python_scans()
            scan_plan['scanners_used'].extend(['bandit', 'safety', 'pip_audit'])
            
        if 'javascript' in technologies:
            self.run_javascript_scans()
            scan_plan['scanners_used'].extend(['eslint_security', 'npm_audit'])
            
        if 'container' in technologies:
            self.run_container_scans()
            scan_plan['scanners_used'].append('trivy')
            
        if 'terraform' in technologies:
            self.run_terraform_scans()
            scan_plan['scanners_used'].extend(['checkov', 'tfsec'])
            
        # Generate unified report
        scan_plan['findings'] = self.findings
        scan_plan['summary'] = self.generate_summary()
        
        return scan_plan
    
    def run_secret_scan(self):
        """Run secret detection tools"""
        try:
            # TruffleHog
            result = subprocess.run([
                'trufflehog', 'filesystem', str(self.project_path),
                '--json', '--no-update'
            ], capture_output=True, text=True, timeout=300)
            
            if result.stdout:
                for line in result.stdout.strip().split('\n'):
                    if line:
                        finding = json.loads(line)
                        self.findings.append(VulnerabilityFinding(
                            tool='trufflehog',
                            severity='CRITICAL',
                            category='secrets',
                            title=f"Secret detected: {finding.get('DetectorName', 'Unknown')}",
                            description=finding.get('Raw', ''),
                            file_path=finding.get('SourceMetadata', {}).get('Data', {}).get('Filesystem', {}).get('file', ''),
                            line_number=finding.get('SourceMetadata', {}).get('Data', {}).get('Filesystem', {}).get('line', 0),
                            cve='',
                            cwe='CWE-798',
                            remediation='Remove secret and rotate credentials',
                            confidence=str(finding.get('Verified', False))
                        ))
        except (subprocess.TimeoutExpired, subprocess.CalledProcessError, FileNotFoundError):
            print("TruffleHog not available or scan failed")
            
        try:
            # GitLeaks
            result = subprocess.run([
                'gitleaks', 'detect', '--source', str(self.project_path),
                '--report-format', 'json', '--no-git'
            ], capture_output=True, text=True, timeout=300)
            
            if result.stdout:
                findings = json.loads(result.stdout)
                for finding in findings:
                    self.findings.append(VulnerabilityFinding(
                        tool='gitleaks',
                        severity='HIGH',
                        category='secrets',
                        title=f"Secret pattern: {finding.get('RuleID', 'Unknown')}",
                        description=finding.get('Description', ''),
                        file_path=finding.get('File', ''),
                        line_number=finding.get('StartLine', 0),
                        cve='',
                        cwe='CWE-798',
                        remediation='Remove secret and add to .gitignore',
                        confidence='high'
                    ))
        except (subprocess.TimeoutExpired, subprocess.CalledProcessError, FileNotFoundError):
            print("GitLeaks not available or scan failed")
    
    def run_python_scans(self):
        """Run Python-specific security scanners"""
        # Bandit
        try:
            result = subprocess.run([
                'bandit', '-r', str(self.project_path),
                '-f', 'json', '--severity-level', 'medium'
            ], capture_output=True, text=True, timeout=300)
            
            if result.stdout:
                bandit_results = json.loads(result.stdout)
                for result_item in bandit_results.get('results', []):
                    self.findings.append(VulnerabilityFinding(
                        tool='bandit',
                        severity=result_item.get('issue_severity', 'MEDIUM'),
                        category='sast',
                        title=result_item.get('test_name', ''),
                        description=result_item.get('issue_text', ''),
                        file_path=result_item.get('filename', ''),
                        line_number=result_item.get('line_number', 0),
                        cve='',
                        cwe=result_item.get('test_id', ''),
                        remediation=result_item.get('more_info', ''),
                        confidence=result_item.get('issue_confidence', 'MEDIUM')
                    ))
        except (subprocess.TimeoutExpired, subprocess.CalledProcessError, FileNotFoundError):
            print("Bandit not available or scan failed")
        
        # Safety
        try:
            result = subprocess.run([
                'safety', 'check', '--json'
            ], capture_output=True, text=True, timeout=300, cwd=self.project_path)
            
            if result.stdout:
                safety_results = json.loads(result.stdout)
                for vuln in safety_results:
                    self.findings.append(VulnerabilityFinding(
                        tool='safety',
                        severity='HIGH',
                        category='dependencies',
                        title=f"Vulnerable package: {vuln.get('package_name', '')}",
                        description=vuln.get('advisory', ''),
                        file_path='requirements.txt',
                        line_number=0,
                        cve=vuln.get('cve', ''),
                        cwe='',
                        remediation=f"Update to version {vuln.get('analyzed_version', 'latest')}",
                        confidence='high'
                    ))
        except (subprocess.TimeoutExpired, subprocess.CalledProcessError, FileNotFoundError):
            print("Safety not available or scan failed")
    
    def generate_summary(self) -> Dict[str, Any]:
        """Generate summary statistics"""
        severity_counts = {'CRITICAL': 0, 'HIGH': 0, 'MEDIUM': 0, 'LOW': 0}
        category_counts = {}
        
        for finding in self.findings:
            severity_counts[finding.severity] = severity_counts.get(finding.severity, 0) + 1
            category_counts[finding.category] = category_counts.get(finding.category, 0) + 1
        
        return {
            'total_findings': len(self.findings),
            'severity_breakdown': severity_counts,
            'category_breakdown': category_counts,
            'risk_score': self.calculate_risk_score(severity_counts)
        }
    
    def calculate_risk_score(self, severity_counts: Dict[str, int]) -> int:
        """Calculate overall risk score (0-100)"""
        weights = {'CRITICAL': 10, 'HIGH': 7, 'MEDIUM': 4, 'LOW': 1}
        total_score = sum(weights[severity] * count for severity, count in severity_counts.items())
        max_possible = 100  # Arbitrary ceiling
        return min(100, int((total_score / max_possible) * 100))
```

**SAST (Static Application Security Testing)**
```python
# Enhanced code vulnerability patterns with tool-specific implementations
security_rules = {
    "sql_injection": {
        "patterns": [
            r"query\s*\(\s*[\"'].*\+.*[\"']\s*\)",
            r"execute\s*\(\s*[\"'].*%[s|d].*[\"']\s*%",
            r"f[\"'].*SELECT.*{.*}.*FROM"
        ],
        "severity": "CRITICAL",
        "cwe": "CWE-89",
        "fix": "Use parameterized queries or prepared statements"
    
    "xss": {
        "patterns": [
            r"innerHTML\s*=\s*[^\"']*\+",
            r"document\.write\s*\([^\"']*\+",
            r"dangerouslySetInnerHTML",
            r"v-html\s*=\s*[\"'][^\"']*\{"
        ],
        "severity": "HIGH",
        "cwe": "CWE-79",
        "fix": "Sanitize user input and use safe rendering methods"
    },
    
    "hardcoded_secrets": {
        "patterns": [
            r"(?i)(api[_-]?key|apikey|secret|password)\s*[:=]\s*[\"'][^\"']{8,}[\"']",
            r"(?i)bearer\s+[a-zA-Z0-9\-\._~\+\/]{20,}",
            r"(?i)(aws[_-]?access[_-]?key[_-]?id|aws[_-]?secret)\s*[:=]",
            r"private[_-]?key\s*[:=]\s*[\"'][^\"']+[\"']"
        ],
        "severity": "CRITICAL",
        "cwe": "CWE-798",
        "fix": "Use environment variables or secure key management service"
    },
    
    "path_traversal": {
        "patterns": [
            r"\.\.\/",
            r"readFile\s*\([^\"']*\+",
            r"include\s*\([^\"']*\$",
            r"require\s*\([^\"']*\+"
        ],
        "severity": "HIGH",
        "cwe": "CWE-22",
        "fix": "Validate and sanitize file paths"
    },
    
    "insecure_random": {
        "patterns": [
            r"Math\.random\(\)",
            r"rand\(\)",
            r"mt_rand\(\)"
        ],
        "severity": "MEDIUM",
        "cwe": "CWE-330",
        "fix": "Use cryptographically secure random functions"
    }
}

def scan_code_vulnerabilities(file_path, content):
    """
    Enhanced code vulnerability scanning with framework-specific patterns
    """
    vulnerabilities = []
    
    for vuln_type, rule in security_rules.items():
        for pattern in rule['patterns']:
            matches = re.finditer(pattern, content, re.MULTILINE)
            for match in matches:
                line_num = content[:match.start()].count('\n') + 1
                vulnerabilities.append({
                    'type': vuln_type,
                    'severity': rule['severity'],
                    'file': file_path,
                    'line': line_num,
                    'code': match.group(0),
                    'cwe': rule['cwe'],
                    'fix': rule['fix'],
                    'confidence': rule.get('confidence', 'medium'),
                    'owasp_category': rule.get('owasp', 'A03:2021-Injection')
                })
    
    return vulnerabilities

# Framework-specific security patterns
framework_security_patterns = {
    'django': {
        'csrf_exempt': {
            'pattern': r'@csrf_exempt',
            'severity': 'HIGH',
            'description': 'CSRF protection disabled',
            'fix': 'Remove @csrf_exempt decorator and implement proper CSRF protection'
        },
        'raw_sql': {
            'pattern': r'\.raw\(["\'][^"\']\*["\']\)',
            'severity': 'HIGH',
            'description': 'Raw SQL query detected',
            'fix': 'Use Django ORM or parameterized queries'
        },
        'eval_usage': {
            'pattern': r'eval\(',
            'severity': 'CRITICAL',
            'description': 'Code evaluation detected',
            'fix': 'Remove eval() usage and use safe alternatives'
        }
    },
    
    'flask': {
        'debug_mode': {
            'pattern': r'debug\s*=\s*True',
            'severity': 'MEDIUM',
            'description': 'Debug mode enabled in production',
            'fix': 'Set debug=False in production'
        },
        'render_template_string': {
            'pattern': r'render_template_string\([^)]*\+',
            'severity': 'HIGH',
            'description': 'Template injection vulnerability',
            'fix': 'Use render_template with static templates'
        }
    },
    
    'react': {
        'dangerous_html': {
            'pattern': r'dangerouslySetInnerHTML',
            'severity': 'HIGH',
            'description': 'XSS vulnerability through innerHTML',
            'fix': 'Sanitize HTML content or use safe rendering'
        },
        'eval_usage': {
            'pattern': r'\beval\(',
            'severity': 'CRITICAL',
            'description': 'Code evaluation detected',
            'fix': 'Remove eval() usage'
        }
    },
    
    'express': {
        'missing_helmet': {
            'pattern': r'express\(\)',
            'negative_pattern': r'helmet\(\)',
            'severity': 'MEDIUM',
            'description': 'Security headers middleware missing',
            'fix': 'Add helmet() middleware for security headers'
        },
        'cors_wildcard': {
            'pattern': r'origin:\s*["\']\*["\']',
            'severity': 'HIGH',
            'description': 'CORS configured with wildcard origin',
            'fix': 'Specify exact allowed origins'
        }
    }
}

def scan_framework_vulnerabilities(framework, file_path, content):
    """Scan for framework-specific security issues"""
    vulnerabilities = []
    
    if framework not in framework_security_patterns:
        return vulnerabilities
    
    patterns = framework_security_patterns[framework]
    
    for vuln_type, rule in patterns.items():
        matches = re.finditer(rule['pattern'], content, re.MULTILINE)
        
        # Check for negative patterns (e.g., missing security middleware)
        if 'negative_pattern' in rule:
            if not re.search(rule['negative_pattern'], content):
                vulnerabilities.append({
                    'type': f'{framework}_{vuln_type}',
                    'severity': rule['severity'],
                    'file': file_path,
                    'description': rule['description'],
                    'fix': rule['fix'],
                    'framework': framework
                })
        else:
            for match in matches:
                line_num = content[:match.start()].count('\n') + 1
                vulnerabilities.append({
                    'type': f'{framework}_{vuln_type}',
                    'severity': rule['severity'],
                    'file': file_path,
                    'line': line_num,
                    'code': match.group(0),
                    'description': rule['description'],
                    'fix': rule['fix'],
                    'framework': framework
                })
    
    return vulnerabilities
```

**Advanced Dependency Vulnerability Scanning**
```python
import subprocess
import json
import requests
from typing import Dict, List, Any
from datetime import datetime, timedelta

class DependencyScanner:
    def __init__(self):
        self.vulnerability_databases = {
            'osv': 'https://api.osv.dev/v1/query',
            'snyk': 'https://api.snyk.io/v1/test',
            'github': 'https://api.github.com/advisories'
        }
    
    def scan_all_ecosystems(self, project_path: str) -> Dict[str, Any]:
        """Comprehensive dependency scanning across all package managers"""
        results = {
            'timestamp': datetime.now().isoformat(),
            'ecosystems': {},
            'summary': {'total_vulnerabilities': 0, 'critical': 0, 'high': 0, 'medium': 0, 'low': 0}
        }
        
        # Detect and scan each ecosystem
        ecosystems = self.detect_ecosystems(project_path)
        
        for ecosystem in ecosystems:
            results['ecosystems'][ecosystem] = self.scan_ecosystem(ecosystem, project_path)
            self.update_summary(results['summary'], results['ecosystems'][ecosystem])
        
        return results
    
    def detect_ecosystems(self, project_path: str) -> List[str]:
        """Detect package managers and dependency files"""
        ecosystems = []
        
        ecosystem_files = {
            'npm': ['package.json', 'package-lock.json', 'yarn.lock'],
            'pip': ['requirements.txt', 'setup.py', 'pyproject.toml', 'Pipfile'],
            'maven': ['pom.xml'],
            'gradle': ['build.gradle', 'build.gradle.kts'],
            'gem': ['Gemfile', 'Gemfile.lock'],
            'composer': ['composer.json', 'composer.lock'],
            'nuget': ['*.csproj', 'packages.config'],
            'go': ['go.mod', 'go.sum'],
            'rust': ['Cargo.toml', 'Cargo.lock']
        }
        
        for ecosystem, files in ecosystem_files.items():
            if any(Path(project_path).glob(f) for f in files):
                ecosystems.append(ecosystem)
        
        return ecosystems
    
    def scan_npm_dependencies(self, project_path: str) -> Dict[str, Any]:
        """Scan NPM dependencies using multiple tools"""
        results = {
            'tool_results': {},
            'vulnerabilities': [],
            'total_packages': 0,
            'outdated_packages': []
        }
        
        # NPM Audit
        try:
            npm_result = subprocess.run(
                ['npm', 'audit', '--json'],
                cwd=project_path,
                capture_output=True,
                text=True,
                timeout=120
            )
            
            if npm_result.stdout:
                audit_data = json.loads(npm_result.stdout)
                results['tool_results']['npm_audit'] = audit_data
                
                for vuln_id, vuln in audit_data.get('vulnerabilities', {}).items():
                    results['vulnerabilities'].append({
                        'id': vuln_id,
                        'severity': vuln.get('severity', 'unknown'),
                        'title': vuln.get('title', ''),
                        'package': vuln.get('name', ''),
                        'version': vuln.get('range', ''),
                        'cwe': vuln.get('cwe', []),
                        'cve': vuln.get('cves', []),
                        'fixed_in': vuln.get('fixAvailable', ''),
                        'source': 'npm_audit'
                    })
        except (subprocess.TimeoutExpired, subprocess.CalledProcessError, json.JSONDecodeError):
            results['tool_results']['npm_audit'] = {'error': 'Failed to run npm audit'}
        
        # Snyk scan (if available)
        try:
            snyk_result = subprocess.run(
                ['snyk', 'test', '--json'],
                cwd=project_path,
                capture_output=True,
                text=True,
                timeout=180
            )
            
            if snyk_result.stdout:
                snyk_data = json.loads(snyk_result.stdout)
                results['tool_results']['snyk'] = snyk_data
                
                for vuln in snyk_data.get('vulnerabilities', []):
                    results['vulnerabilities'].append({
                        'id': vuln.get('id', ''),
                        'severity': vuln.get('severity', 'unknown'),
                        'title': vuln.get('title', ''),
                        'package': vuln.get('packageName', ''),
                        'version': vuln.get('version', ''),
                        'cve': vuln.get('identifiers', {}).get('CVE', []),
                        'cwe': vuln.get('identifiers', {}).get('CWE', []),
                        'upgrade_path': vuln.get('upgradePath', []),
                        'source': 'snyk'
                    })
        except (subprocess.TimeoutExpired, subprocess.CalledProcessError, json.JSONDecodeError):
            results['tool_results']['snyk'] = {'error': 'Snyk not available or failed'}
        
        return results
    
    def scan_python_dependencies(self, project_path: str) -> Dict[str, Any]:
        """Comprehensive Python dependency scanning"""
        results = {
            'tool_results': {},
            'vulnerabilities': [],
            'license_issues': []
        }
        
        # Safety scan
        try:
            safety_result = subprocess.run(
                ['safety', 'check', '--json'],
                cwd=project_path,
                capture_output=True,
                text=True,
                timeout=120
            )
            
            if safety_result.stdout:
                safety_data = json.loads(safety_result.stdout)
                results['tool_results']['safety'] = safety_data
                
                for vuln in safety_data:
                    results['vulnerabilities'].append({
                        'package': vuln.get('package_name', ''),
                        'version': vuln.get('analyzed_version', ''),
                        'vulnerability_id': vuln.get('vulnerability_id', ''),
                        'advisory': vuln.get('advisory', ''),
                        'cve': vuln.get('cve', ''),
                        'severity': self.map_safety_severity(vuln.get('vulnerability_id', '')),
                        'source': 'safety'
                    })
        except (subprocess.TimeoutExpired, subprocess.CalledProcessError, json.JSONDecodeError):
            results['tool_results']['safety'] = {'error': 'Safety scan failed'}
        
        # pip-audit scan
        try:
            pip_audit_result = subprocess.run(
                ['pip-audit', '--format=json'],
                cwd=project_path,
                capture_output=True,
                text=True,
                timeout=120
            )
            
            if pip_audit_result.stdout:
                pip_audit_data = json.loads(pip_audit_result.stdout)
                results['tool_results']['pip_audit'] = pip_audit_data
                
                for vuln in pip_audit_data.get('vulnerabilities', []):
                    results['vulnerabilities'].append({
                        'package': vuln.get('package', ''),
                        'version': vuln.get('version', ''),
                        'vulnerability_id': vuln.get('id', ''),
                        'description': vuln.get('description', ''),
                        'aliases': vuln.get('aliases', []),
                        'fix_versions': vuln.get('fix_versions', []),
                        'source': 'pip_audit'
                    })
        except (subprocess.TimeoutExpired, subprocess.CalledProcessError, json.JSONDecodeError):
            results['tool_results']['pip_audit'] = {'error': 'pip-audit not available'}
        
        return results
    
    def generate_remediation_plan(self, vulnerabilities: List[Dict]) -> Dict[str, Any]:
        """Generate prioritized remediation plan"""
        plan = {
            'immediate_actions': [],
            'short_term': [],
            'long_term': [],
            'automation_scripts': {}
        }
        
        # Sort by severity
        critical_high = [v for v in vulnerabilities if v.get('severity', '').upper() in ['CRITICAL', 'HIGH']]
        medium = [v for v in vulnerabilities if v.get('severity', '').upper() == 'MEDIUM']
        low = [v for v in vulnerabilities if v.get('severity', '').upper() == 'LOW']
        
        # Immediate actions for critical/high
        for vuln in critical_high:
            plan['immediate_actions'].append({
                'package': vuln.get('package', ''),
                'current_version': vuln.get('version', ''),
                'fixed_version': vuln.get('fixed_in', vuln.get('fix_versions', ['latest'])[0] if vuln.get('fix_versions') else 'latest'),
                'action': f"Update {vuln.get('package', '')} to {vuln.get('fixed_in', 'latest')}",
                'priority': 1,
                'effort': 'Low'
            })
        
        # Auto-update script
        plan['automation_scripts']['npm_auto_update'] = """
#!/bin/bash
# Automated npm dependency updates
npm audit fix --force
npm update
npm audit
"""
        
        plan['automation_scripts']['pip_auto_update'] = """
#!/bin/bash
# Automated Python dependency updates
pip install --upgrade pip
pip-audit --fix
safety check
"""
        
        return plan

# Example usage with specific package managers
npm_audit_example = {
    "dependencies": {
        "lodash": {
            "version": "4.17.15",
            "vulnerabilities": [{
                "severity": "HIGH",
                "cve": "CVE-2021-23337",
                "description": "Command Injection in lodash",
                "fixed_in": "4.17.21",
                "recommendation": "npm install lodash@4.17.21",
                "automated_fix": "npm audit fix"
            }]
        },
        "@types/node": {
            "version": "14.0.0",
            "vulnerabilities": [],
            "outdated": True,
            "latest_version": "20.0.0",
            "recommendation": "npm install @types/node@latest"
        }
    },
    "summary": {
        "total_packages": 847,
        "vulnerable_packages": 12,
        "outdated_packages": 45,
        "license_issues": 3
    }
}

# Python requirements scan
# Container Image Vulnerability Scanning
def scan_container_vulnerabilities(image_name: str) -> Dict[str, Any]:
    """
    Comprehensive container vulnerability scanning using multiple tools
    """
    results = {
        'image': image_name,
        'scan_results': {},
        'vulnerabilities': [],
        'sbom': {},
        'compliance_checks': {}
    }
    
    # Trivy scan
    try:
        trivy_result = subprocess.run([
            'trivy', 'image', '--format', 'json',
            '--security-checks', 'vuln,config,secret',
            image_name
        ], capture_output=True, text=True, timeout=300)
        
        if trivy_result.stdout:
            trivy_data = json.loads(trivy_result.stdout)
            results['scan_results']['trivy'] = trivy_data
            
            for result in trivy_data.get('Results', []):
                for vuln in result.get('Vulnerabilities', []):
                    results['vulnerabilities'].append({
                        'package': vuln.get('PkgName', ''),
                        'version': vuln.get('InstalledVersion', ''),
                        'vulnerability_id': vuln.get('VulnerabilityID', ''),
                        'severity': vuln.get('Severity', 'UNKNOWN'),
                        'title': vuln.get('Title', ''),
                        'description': vuln.get('Description', ''),
                        'fixed_version': vuln.get('FixedVersion', ''),
                        'source': 'trivy'
                    })
    except (subprocess.TimeoutExpired, subprocess.CalledProcessError, json.JSONDecodeError):
        results['scan_results']['trivy'] = {'error': 'Trivy scan failed'}
    
    # Generate SBOM (Software Bill of Materials)
    try:
        sbom_result = subprocess.run([
            'trivy', 'image', '--format', 'spdx-json',
            image_name
        ], capture_output=True, text=True, timeout=180)
        
        if sbom_result.stdout:
            results['sbom'] = json.loads(sbom_result.stdout)
    except (subprocess.TimeoutExpired, subprocess.CalledProcessError, json.JSONDecodeError):
        results['sbom'] = {'error': 'SBOM generation failed'}
    
    return results

# Multi-ecosystem scanner
class UniversalDependencyScanner:
    def __init__(self):
        self.scanners = {
            'python': self.scan_python_dependencies,
            'javascript': self.scan_npm_dependencies,
            'java': self.scan_java_dependencies,
            'go': self.scan_go_dependencies,
            'rust': self.scan_rust_dependencies,
            'container': self.scan_container_image
        }
    
    def scan_python_dependencies(self, project_path: str) -> Dict[str, Any]:
        """
        Enhanced Python dependency scanning with multiple tools
        """
        results = {
            'tools_used': ['safety', 'pip-audit', 'bandit'],
            'vulnerabilities': [],
            'license_compliance': [],
            'outdated_packages': []
        }
        
        # Safety check
        try:
            safety_cmd = ['safety', 'check', '--json', '--full-report']
            result = subprocess.run(safety_cmd, capture_output=True, text=True, timeout=120)
            
            if result.stdout:
                safety_data = json.loads(result.stdout)
                for vuln in safety_data:
                    results['vulnerabilities'].append({
                        'tool': 'safety',
                        'package': vuln.get('package_name'),
                        'version': vuln.get('analyzed_version'),
                        'vulnerability_id': vuln.get('vulnerability_id'),
                        'severity': self._map_safety_severity(vuln.get('vulnerability_id')),
                        'advisory': vuln.get('advisory'),
                        'cve': vuln.get('cve'),
                        'remediation': f"Update to {vuln.get('fixed_version', 'latest version')}"
                    })
        except Exception as e:
            results['safety_error'] = str(e)
        
        # pip-audit
        try:
            pip_audit_cmd = ['pip-audit', '--format=json', '--desc']
            result = subprocess.run(pip_audit_cmd, capture_output=True, text=True, timeout=120)
            
            if result.stdout:
                pip_audit_data = json.loads(result.stdout)
                for vuln in pip_audit_data.get('vulnerabilities', []):
                    results['vulnerabilities'].append({
                        'tool': 'pip-audit',
                        'package': vuln.get('package'),
                        'version': vuln.get('version'),
                        'vulnerability_id': vuln.get('id'),
                        'severity': self._calculate_severity_from_cvss(vuln.get('fix_versions', [])),
                        'description': vuln.get('description'),
                        'aliases': vuln.get('aliases', []),
                        'fix_versions': vuln.get('fix_versions', []),
                        'remediation': f"Update to one of: {', '.join(vuln.get('fix_versions', ['latest']))}"
                    })
        except Exception as e:
            results['pip_audit_error'] = str(e)
        
        # License compliance check
        try:
            pip_licenses_result = subprocess.run(
                ['pip-licenses', '--format=json'],
                capture_output=True, text=True, timeout=60
            )
            
            if pip_licenses_result.stdout:
                licenses_data = json.loads(pip_licenses_result.stdout)
                problematic_licenses = ['GPL', 'AGPL', 'SSPL', 'BUSL']
                
                for package in licenses_data:
                    license_name = package.get('License', 'Unknown')
                    if any(prob in license_name.upper() for prob in problematic_licenses):
                        results['license_compliance'].append({
                            'package': package.get('Name'),
                            'version': package.get('Version'),
                            'license': license_name,
                            'issue': 'Potentially problematic license for commercial use',
                            'action': 'Review license compatibility'
                        })
        except Exception as e:
            results['license_error'] = str(e)
        
        return results
    
    def _map_safety_severity(self, vuln_id: str) -> str:
        """Map Safety vulnerability ID to severity level"""
        # Safety uses numeric IDs, we can implement CVSS mapping
        # This is a simplified mapping - in practice, use CVSS scores
        high_risk_patterns = ['injection', 'rce', 'deserialization']
        if any(pattern in vuln_id.lower() for pattern in high_risk_patterns):
            return 'CRITICAL'
        return 'HIGH'  # Default for Safety findings
    
    def _calculate_severity_from_cvss(self, fix_versions: list) -> str:
        """Calculate severity based on fix version availability"""
        if not fix_versions:
            return 'HIGH'  # No fix available
        return 'MEDIUM'  # Fix available
```

### 2. OWASP Top 10 Assessment

Check for OWASP Top 10 vulnerabilities:

**A01: Broken Access Control**
```python
# Check for missing authentication
def check_access_control():
    findings = []
    
    # API endpoints without auth
    unprotected_endpoints = [
        {'path': '/api/admin/*', 'method': 'GET', 'auth': False},
        {'path': '/api/users/delete', 'method': 'POST', 'auth': False}
    ]
    
    # Insecure direct object references
    idor_patterns = [
        r"user_id\s*=\s*request\.(GET|POST)\[",
        r"WHERE\s+id\s*=\s*\$_GET\[",
        r"findById\(req\.params\.id\)"
    ]
    
    # Missing authorization checks
    missing_authz = [
        {'file': 'routes/admin.js', 'line': 45, 'issue': 'No role check'},
        {'file': 'api/delete.py', 'line': 12, 'issue': 'No ownership validation'}
    ]
    
    return findings
```

**A02: Cryptographic Failures**
```python
# Check encryption and hashing
crypto_issues = {
    "weak_hashing": [
        {"algorithm": "MD5", "usage": "password hashing", "severity": "CRITICAL"},
        {"algorithm": "SHA1", "usage": "token generation", "severity": "HIGH"}
    ],
    "insecure_storage": [
        {"data": "credit cards", "storage": "plain text in database"},
        {"data": "SSN", "storage": "base64 encoded only"}
    ],
    "missing_encryption": [
        {"connection": "database", "protocol": "unencrypted TCP"},
        {"api": "payment service", "protocol": "HTTP"}
    ],
    "weak_tls": [
        {"version": "TLS 1.0", "recommendation": "Use TLS 1.2+"},
        {"cipher": "DES-CBC3-SHA", "recommendation": "Use ECDHE-RSA-AES256-GCM-SHA384"}
    ]
}
```

**A03: Injection**
```python
# SQL Injection detection
sql_injection_tests = [
    {"payload": "' OR '1'='1", "vulnerable": True},
    {"payload": "'; DROP TABLE users; --", "vulnerable": True},
    {"payload": "1' UNION SELECT * FROM users--", "vulnerable": False}
]

# NoSQL Injection
nosql_injection = {
    "mongodb": [
        {"query": "db.users.find({username: req.body.username})", "vulnerable": True},
        {"fix": "db.users.find({username: {$eq: req.body.username}})"}
    ]
}

# Command Injection
command_injection = [
    {
        "code": "exec('ping ' + user_input)",
        "vulnerability": "Direct command execution with user input",
        "fix": "Use subprocess with shell=False and validate input"
    }
]
```

### 3. Infrastructure Security

Scan infrastructure and configuration:

**Container Security**
```dockerfile
# Dockerfile security scan
FROM node:14  # ISSUE: Using non-specific tag
USER root     # ISSUE: Running as root

# ISSUE: Installing packages without version pinning
RUN apt-get update && apt-get install -y curl

# ISSUE: Copying sensitive files
COPY . /app
COPY .env /app/.env  # CRITICAL: Copying secrets

# ISSUE: Not dropping privileges
CMD ["node", "server.js"]

# Secure version:
FROM node:14.17.6-alpine AS builder
RUN apk add --no-cache python3 make g++
WORKDIR /app
COPY package*.json ./
RUN npm ci --only=production

FROM node:14.17.6-alpine
RUN addgroup -g 1001 -S nodejs && adduser -S nodejs -u 1001
USER nodejs
WORKDIR /app
COPY --from=builder --chown=nodejs:nodejs /app/node_modules ./node_modules
COPY --chown=nodejs:nodejs . .
EXPOSE 3000
CMD ["node", "server.js"]
```

**Kubernetes Security**
```yaml
# Pod Security Policy
apiVersion: policy/v1beta1
kind: PodSecurityPolicy
metadata:
  name: restricted
spec:
  privileged: false
  allowPrivilegeEscalation: false
  requiredDropCapabilities:
    - ALL
  volumes:
    - 'configMap'
    - 'emptyDir'
    - 'projected'
    - 'secret'
    - 'downwardAPI'
    - 'persistentVolumeClaim'
  runAsUser:
    rule: 'MustRunAsNonRoot'
  seLinux:
    rule: 'RunAsAny'
  fsGroup:
    rule: 'RunAsAny'
  readOnlyRootFilesystem: true
```

### 4. API Security

Comprehensive API security testing:

**Authentication & Authorization**
```python
# JWT Security Issues
jwt_vulnerabilities = {
    "weak_secret": {
        "issue": "JWT signed with weak secret 'secret123'",
        "severity": "CRITICAL",
        "fix": "Use strong 256-bit secret from environment"
    },
    "algorithm_confusion": {
        "issue": "JWT accepts 'none' algorithm",
        "severity": "CRITICAL",
        "fix": "Explicitly verify algorithm: ['HS256', 'RS256']"
    },
    "missing_expiration": {
        "issue": "JWT tokens never expire",
        "severity": "HIGH",
        "fix": "Set exp claim to reasonable duration (e.g., 1 hour)"
    }
}

# API Rate Limiting
rate_limit_config = {
    "endpoints": {
        "/api/login": {"limit": 5, "window": "5m", "status": "NOT_CONFIGURED"},
        "/api/password-reset": {"limit": 3, "window": "1h", "status": "NOT_CONFIGURED"},
        "/api/data": {"limit": 100, "window": "1m", "status": "OK"}
    }
}
```

**Input Validation**
```python
# API Input Validation Issues
validation_issues = [
    {
        "endpoint": "/api/users",
        "method": "POST",
        "field": "email",
        "issue": "No email format validation",
        "exploit": "user@<script>alert(1)</script>.com"
    },
    {
        "endpoint": "/api/upload",
        "method": "POST",
        "field": "file",
        "issue": "No file type validation",
        "exploit": "shell.php renamed to image.jpg"
    }
]
```

### 5. Secret Detection

Scan for exposed secrets and credentials:

**Secret Patterns**
```python
secret_patterns = {
    "aws_access_key": r"AKIA[0-9A-Z]{16}",
    "aws_secret_key": r"[0-9a-zA-Z/+=]{40}",
    "github_token": r"ghp_[0-9a-zA-Z]{36}",
    "stripe_key": r"sk_live_[0-9a-zA-Z]{24}",
    "private_key": r"-----BEGIN (RSA |EC )?PRIVATE KEY-----",
    "google_api": r"AIza[0-9A-Za-z\-_]{35}",
    "jwt_token": r"eyJ[A-Za-z0-9-_=]+\.eyJ[A-Za-z0-9-_=]+\.[A-Za-z0-9-_.+/=]+",
    "slack_webhook": r"https://hooks\.slack\.com/services/[A-Z0-9]{9}/[A-Z0-9]{9}/[a-zA-Z0-9]{24}"
}

# Git history scan
def scan_git_history():
    """
    Scan git history for accidentally committed secrets
    """
    import subprocess
    
    # Get all commits
    commits = subprocess.run(
        ['git', 'log', '--pretty=format:%H'],
        capture_output=True,
        text=True
    ).stdout.split('\n')
    
    secrets_found = []
    
    for commit in commits[:100]:  # Last 100 commits
        diff = subprocess.run(
            ['git', 'show', commit],
            capture_output=True,
            text=True
        ).stdout
        
        for secret_type, pattern in secret_patterns.items():
            if re.search(pattern, diff):
                secrets_found.append({
                    'commit': commit,
                    'type': secret_type,
                    'action': 'Remove from history and rotate credential'
                })
    
    return secrets_found
```

### 6. Security Headers

Check HTTP security headers:

**Header Configuration**
```python
security_headers = {
    "Strict-Transport-Security": {
        "required": True,
        "value": "max-age=31536000; includeSubDomains; preload",
        "missing_impact": "Vulnerable to protocol downgrade attacks"
    },
    "X-Content-Type-Options": {
        "required": True,
        "value": "nosniff",
        "missing_impact": "Vulnerable to MIME type confusion attacks"
    },
    "X-Frame-Options": {
        "required": True,
        "value": "DENY",
        "missing_impact": "Vulnerable to clickjacking"
    },
    "Content-Security-Policy": {
        "required": True,
        "value": "default-src 'self'; script-src 'self' 'unsafe-inline'",
        "missing_impact": "Vulnerable to XSS attacks"
    },
    "X-XSS-Protection": {
        "required": False,  # Deprecated
        "value": "0",
        "note": "Modern browsers have built-in XSS protection"
    },
    "Referrer-Policy": {
        "required": True,
        "value": "strict-origin-when-cross-origin",
        "missing_impact": "May leak sensitive URLs"
    },
    "Permissions-Policy": {
        "required": True,
        "value": "geolocation=(), microphone=(), camera=()",
        "missing_impact": "Allows access to sensitive browser features"
    }
}
```

### 7. Automated Remediation Implementation

Provide intelligent, automated fixes with safety validation:

**Smart Remediation Engine**
```python
import ast
import re
import subprocess
from typing import Dict, List, Any, Optional
from dataclasses import dataclass
from pathlib import Path

@dataclass
class RemediationAction:
    vulnerability_id: str
    action_type: str  # 'dependency_update', 'code_fix', 'config_change'
    description: str
    risk_level: str  # 'safe', 'low_risk', 'medium_risk', 'high_risk'
    automated: bool
    manual_steps: List[str]
    validation_tests: List[str]
    rollback_plan: str

class AutomatedRemediationEngine:
    def __init__(self, project_path: str):
        self.project_path = Path(project_path)
        self.backup_created = False
        self.applied_fixes = []
        
    def create_safety_backup(self) -> str:
        """Create git branch backup before applying fixes"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        backup_branch = f'security_backup_{timestamp}'
        
        try:
            subprocess.run(['git', 'checkout', '-b', backup_branch], 
                         cwd=self.project_path, check=True)
            subprocess.run(['git', 'checkout', '-'], 
                         cwd=self.project_path, check=True)
            self.backup_created = True
            return backup_branch
        except subprocess.CalledProcessError:
            raise Exception("Failed to create safety backup branch")
    
    def apply_automated_fixes(self, vulnerabilities: List[Dict]) -> List[RemediationAction]:
        """Apply safe automated fixes"""
        if not self.backup_created:
            self.create_safety_backup()
        
        actions = []
        
        for vuln in vulnerabilities:
            action = self.generate_remediation_action(vuln)
            
            if action.automated and action.risk_level in ['safe', 'low_risk']:
                try:
                    success = self.apply_fix(action)
                    if success:
                        actions.append(action)
                        self.applied_fixes.append(action)
                except Exception as e:
                    print(f"Failed to apply fix for {action.vulnerability_id}: {e}")
            else:
                actions.append(action)
        
        return actions
    
    def generate_remediation_action(self, vulnerability: Dict) -> RemediationAction:
        """Generate specific remediation action for vulnerability"""
        vuln_type = vulnerability.get('type', '')
        severity = vulnerability.get('severity', 'MEDIUM')
        
        if vuln_type == 'vulnerable_dependency':
            return self._fix_vulnerable_dependency(vulnerability)
        elif vuln_type == 'sql_injection':
            return self._fix_sql_injection(vulnerability)
        elif vuln_type == 'hardcoded_secrets':
            return self._fix_hardcoded_secrets(vulnerability)
        elif vuln_type == 'missing_security_headers':
            return self._fix_security_headers(vulnerability)
        else:
            return self._generic_fix(vulnerability)
    
    def _fix_vulnerable_dependency(self, vuln: Dict) -> RemediationAction:
        """Fix vulnerable dependencies automatically"""
        package = vuln.get('package', '')
        current_version = vuln.get('version', '')
        fixed_version = vuln.get('fixed_version', 'latest')
        
        # Determine package manager
        if (self.project_path / 'package.json').exists():
            update_command = f'npm install {package}@{fixed_version}'
            ecosystem = 'npm'
        elif (self.project_path / 'requirements.txt').exists():
            update_command = f'pip install {package}=={fixed_version}'
            ecosystem = 'pip'
        else:
            ecosystem = 'unknown'
            update_command = f'# Update {package} to {fixed_version}'
        
        return RemediationAction(
            vulnerability_id=vuln.get('id', ''),
            action_type='dependency_update',
            description=f'Update {package} from {current_version} to {fixed_version}',
            risk_level='safe',  # Dependency updates are generally safe
            automated=True,
            manual_steps=[
                f'Run: {update_command}',
                'Test application functionality',
                'Update lock file if needed'
            ],
            validation_tests=[
                f'Check {package} version is {fixed_version}',
                'Run regression tests',
                'Verify no new vulnerabilities introduced'
            ],
            rollback_plan=f'Revert to {package}@{current_version}'
        )
    
    def _fix_sql_injection(self, vuln: Dict) -> RemediationAction:
        """Fix SQL injection vulnerabilities"""
        file_path = vuln.get('file_path', '')
        line_number = vuln.get('line_number', 0)
        
        # Read the vulnerable code
        try:
            with open(self.project_path / file_path, 'r') as f:
                lines = f.readlines()
            
            vulnerable_line = lines[line_number - 1] if line_number > 0 else ''
            
            # Generate fix based on language and framework
            if file_path.endswith('.py'):
                fixed_code = self._fix_python_sql_injection(vulnerable_line)
            elif file_path.endswith('.js'):
                fixed_code = self._fix_javascript_sql_injection(vulnerable_line)
            else:
                fixed_code = '# Manual fix required'
            
            return RemediationAction(
                vulnerability_id=vuln.get('id', ''),
                action_type='code_fix',
                description=f'Fix SQL injection in {file_path}:{line_number}',
                risk_level='medium_risk',  # Code changes need testing
                automated=False,  # Require manual review
                manual_steps=[
                    f'Replace line {line_number} in {file_path}',
                    f'Original: {vulnerable_line.strip()}',
                    f'Fixed: {fixed_code}',
                    'Add input validation',
                    'Test with malicious inputs'
                ],
                validation_tests=[
                    'SQL injection penetration testing',
                    'Unit tests for the affected function',
                    'Integration tests for the endpoint'
                ],
                rollback_plan=f'Revert changes to {file_path}'
            )
        except Exception as e:
            return self._generic_fix(vuln)
    
    def _fix_python_sql_injection(self, vulnerable_line: str) -> str:
        """Generate Python SQL injection fix"""
        # Simple pattern matching for common cases
        if 'cursor.execute(' in vulnerable_line and '{}' in vulnerable_line:
            return vulnerable_line.replace('.format(', ', (').replace('{}', '?')
        elif 'query(' in vulnerable_line and '+' in vulnerable_line:
            return '# Use parameterized query: query("SELECT * FROM table WHERE id = ?", (user_id,))'
        return '# Replace with parameterized query'
    
    def _fix_hardcoded_secrets(self, vuln: Dict) -> RemediationAction:
        """Fix hardcoded secrets"""
        file_path = vuln.get('file_path', '')
        secret_type = vuln.get('secret_type', 'credential')
        
        return RemediationAction(
            vulnerability_id=vuln.get('id', ''),
            action_type='code_fix',
            description=f'Remove hardcoded {secret_type} from {file_path}',
            risk_level='high_risk',  # Secrets need immediate attention
            automated=False,  # Never automate secret removal
            manual_steps=[
                f'Remove hardcoded secret from {file_path}',
                'Add secret to environment variables or secret manager',
                'Update code to read from environment',
                'Rotate the exposed credential',
                'Add {file_path} to .gitignore if needed',
                'Scan git history for credential exposure'
            ],
            validation_tests=[
                'Verify application works with environment variable',
                'Confirm no secrets in code',
                'Test with invalid/missing environment variable'
            ],
            rollback_plan='Use temporary hardcoded value until proper secret management'
        )
    
    def apply_fix(self, action: RemediationAction) -> bool:
        """Apply an automated fix"""
        if action.action_type == 'dependency_update':
            return self._apply_dependency_update(action)
        elif action.action_type == 'config_change':
            return self._apply_config_change(action)
        return False
    
    def _apply_dependency_update(self, action: RemediationAction) -> bool:
        """Apply dependency update"""
        try:
            # Extract update command from manual steps
            update_command = None
            for step in action.manual_steps:
                if step.startswith('Run: '):
                    update_command = step[5:].split()
                    break
            
            if update_command:
                result = subprocess.run(
                    update_command,
                    cwd=self.project_path,
                    capture_output=True,
                    text=True,
                    timeout=300
                )
                
                if result.returncode == 0:
                    print(f"Successfully applied: {action.description}")
                    return True
                else:
                    print(f"Failed to apply {action.description}: {result.stderr}")
                    return False
            
            return False
        except Exception as e:
            print(f"Error applying fix: {e}")
            return False
    
    def generate_remediation_report(self, actions: List[RemediationAction]) -> str:
        """Generate comprehensive remediation report"""
        report = []
        report.append("# Security Remediation Report\n")
        report.append(f"**Generated**: {datetime.now().isoformat()}\n")
        report.append(f"**Total Actions**: {len(actions)}\n")
        
        automated_count = sum(1 for a in actions if a.automated and a.risk_level in ['safe', 'low_risk'])
        manual_count = len(actions) - automated_count
        
        report.append(f"**Automated Fixes Applied**: {automated_count}\n")
        report.append(f"**Manual Actions Required**: {manual_count}\n\n")
        
        # Group by action type
        by_type = {}
        for action in actions:
            if action.action_type not in by_type:
                by_type[action.action_type] = []
            by_type[action.action_type].append(action)
        
        for action_type, type_actions in by_type.items():
            report.append(f"## {action_type.replace('_', ' ').title()}\n")
            
            for action in type_actions:
                report.append(f"### {action.description}\n")
                report.append(f"**Risk Level**: {action.risk_level}\n")
                report.append(f"**Automated**: {'‚úÖ' if action.automated else '‚ùå'}\n")
                
                if action.manual_steps:
                    report.append("**Manual Steps**:\n")
                    for step in action.manual_steps:
                        report.append(f"- {step}\n")
                
                if action.validation_tests:
                    report.append("**Validation Tests**:\n")
                    for test in action.validation_tests:
                        report.append(f"- {test}\n")
                
                report.append(f"**Rollback**: {action.rollback_plan}\n\n")
        
        return ''.join(report)

# Security Middleware Templates
security_middleware_templates = {
    'express': """
// Enhanced Express.js security middleware
const helmet = require('helmet');
const rateLimit = require('express-rate-limit');
const mongoSanitize = require('express-mongo-sanitize');
const hpp = require('hpp');
const cors = require('cors');

// Content Security Policy
app.use(helmet({
    contentSecurityPolicy: {
        directives: {
            defaultSrc: ["'self'"],
            scriptSrc: ["'self'", "'unsafe-inline'", "https://trusted-cdn.com"],
            styleSrc: ["'self'", "'unsafe-inline'", "https://fonts.googleapis.com"],
            imgSrc: ["'self'", "data:", "https:"],
            connectSrc: ["'self'"],
            fontSrc: ["'self'", "https://fonts.gstatic.com"],
            objectSrc: ["'none'"],
            mediaSrc: ["'self'"],
            frameSrc: ["'none'"],
            baseUri: ["'self'"],
            formAction: ["'self'"]
        },
    },
    hsts: {
        maxAge: 31536000,
        includeSubDomains: true,
        preload: true
    },
    noSniff: true,
    xssFilter: true,
    referrerPolicy: { policy: 'same-origin' }
}));

// Advanced rate limiting
const createRateLimiter = (windowMs, max, message) => rateLimit({
    windowMs,
    max,
    message: { error: message },
    standardHeaders: true,
    legacyHeaders: false,
    handler: (req, res) => {
        res.status(429).json({
            error: message,
            retryAfter: Math.round(windowMs / 1000)
        });
    }
});

// Different limits for different endpoints
app.use('/api/auth/login', createRateLimiter(15 * 60 * 1000, 5, 'Too many login attempts'));
app.use('/api/auth/register', createRateLimiter(60 * 60 * 1000, 3, 'Too many registration attempts'));
app.use('/api/', createRateLimiter(15 * 60 * 1000, 100, 'Too many API requests'));

// CORS configuration
app.use(cors({
    origin: process.env.ALLOWED_ORIGINS?.split(',') || ['http://localhost:3000'],
    credentials: true,
    optionsSuccessStatus: 200
}));

// Input sanitization and validation
app.use(express.json({ 
    limit: '10mb',
    verify: (req, res, buf) => {
        if (buf.length > 10 * 1024 * 1024) {
            throw new Error('Request entity too large');
        }
    }
}));
app.use(mongoSanitize()); // Prevent NoSQL injection
app.use(hpp()); // Prevent HTTP Parameter Pollution

// Custom security middleware
app.use((req, res, next) => {
    // Remove sensitive headers
    res.removeHeader('X-Powered-By');
    
    // Add security headers
    res.setHeader('X-Content-Type-Options', 'nosniff');
    res.setHeader('X-Frame-Options', 'DENY');
    res.setHeader('X-XSS-Protection', '1; mode=block');
    
    next();
});

// Secure session configuration
app.use(session({
    secret: process.env.SESSION_SECRET || throwError('SESSION_SECRET required'),
    name: 'sessionId', // Don't use default 'connect.sid'
    resave: false,
    saveUninitialized: false,
    cookie: {
        secure: process.env.NODE_ENV === 'production',
        httpOnly: true,
        maxAge: 24 * 60 * 60 * 1000, // 24 hours
        sameSite: 'strict'
    },
    store: new RedisStore({ /* Redis configuration */ })
}));

// SQL injection prevention
const db = require('better-sqlite3')('app.db', {
    verbose: process.env.NODE_ENV === 'development' ? console.log : null
});

// Prepared statements
const statements = {
    getUserByEmail: db.prepare('SELECT * FROM users WHERE email = ?'),
    getUserById: db.prepare('SELECT * FROM users WHERE id = ?'),
    createUser: db.prepare('INSERT INTO users (email, password_hash) VALUES (?, ?)')
};

// Safe database operations
app.post('/login', async (req, res) => {
    const { email, password } = req.body;
    
    // Input validation
    if (!email || !password) {
        return res.status(400).json({ error: 'Email and password required' });
    }
    
    try {
        const user = statements.getUserByEmail.get(email);
        if (user && await bcrypt.compare(password, user.password_hash)) {
            req.session.userId = user.id;
            res.json({ success: true, user: { id: user.id, email: user.email } });
        } else {
            res.status(401).json({ error: 'Invalid credentials' });
        }
    } catch (error) {
        console.error('Login error:', error);
        res.status(500).json({ error: 'Internal server error' });
    }
});
""",
    
    'flask': """
# Enhanced Flask security configuration
from flask import Flask, request, session, jsonify
from flask_talisman import Talisman
from flask_limiter import Limiter
from flask_limiter.util import get_remote_address
from flask_seasurf import SeaSurf
from flask_cors import CORS
import bcrypt
import sqlite3
import os
import secrets

app = Flask(__name__)

# Security configuration
app.config.update(
    SECRET_KEY=os.environ.get('SECRET_KEY') or secrets.token_urlsafe(32),
    SESSION_COOKIE_SECURE=True,
    SESSION_COOKIE_HTTPONLY=True,
    SESSION_COOKIE_SAMESITE='Lax',
    PERMANENT_SESSION_LIFETIME=timedelta(hours=24)
)

# HTTPS enforcement and security headers
Talisman(app, {
    'force_https': app.config.get('ENV') == 'production',
    'strict_transport_security': True,
    'strict_transport_security_max_age': 31536000,
    'content_security_policy': {
        'default-src': "'self'",
        'script-src': "'self' 'unsafe-inline'",
        'style-src': "'self' 'unsafe-inline' https://fonts.googleapis.com",
        'font-src': "'self' https://fonts.gstatic.com",
        'img-src': "'self' data: https:",
        'connect-src': "'self'",
        'frame-src': "'none'",
        'object-src': "'none'"
    },
    'referrer_policy': 'strict-origin-when-cross-origin'
})

# CORS configuration
CORS(app, {
    'origins': os.environ.get('ALLOWED_ORIGINS', 'http://localhost:3000').split(','),
    'supports_credentials': True
})

# Rate limiting
limiter = Limiter(
    app,
    key_func=get_remote_address,
    default_limits=["1000 per hour"]
)

# CSRF protection
SeaSurf(app)

# Database connection with security
def get_db_connection():
    conn = sqlite3.connect('app.db')
    conn.row_factory = sqlite3.Row
    conn.execute('PRAGMA foreign_keys = ON')  # Enable foreign key constraints
    return conn

# Secure password hashing
class PasswordManager:
    @staticmethod
    def hash_password(password: str) -> str:
        return bcrypt.hashpw(password.encode('utf-8'), bcrypt.gensalt()).decode('utf-8')
    
    @staticmethod
    def verify_password(password: str, hashed: str) -> bool:
        return bcrypt.checkpw(password.encode('utf-8'), hashed.encode('utf-8'))

# Input validation
def validate_email(email: str) -> bool:
    import re
    pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$'
    return re.match(pattern, email) is not None

# Secure login endpoint
@app.route('/api/login', methods=['POST'])
@limiter.limit("5 per minute")
def login():
    data = request.get_json()
    
    if not data or 'email' not in data or 'password' not in data:
        return jsonify({'error': 'Email and password required'}), 400
    
    email = data['email'].strip().lower()
    password = data['password']
    
    if not validate_email(email):
        return jsonify({'error': 'Invalid email format'}), 400
    
    try:
        conn = get_db_connection()
        user = conn.execute(
            'SELECT id, email, password_hash FROM users WHERE email = ?',
            (email,)
        ).fetchone()
        conn.close()
        
        if user and PasswordManager.verify_password(password, user['password_hash']):
            session['user_id'] = user['id']
            session.permanent = True
            return jsonify({
                'success': True,
                'user': {'id': user['id'], 'email': user['email']}
            })
        else:
            return jsonify({'error': 'Invalid credentials'}), 401
            
    except Exception as e:
        app.logger.error(f'Login error: {e}')
        return jsonify({'error': 'Internal server error'}), 500

# Request logging middleware
@app.before_request
def log_request_info():
    app.logger.info('Request: %s %s from %s', 
                   request.method, request.url, request.remote_addr)

# Error handlers
@app.errorhandler(429)
def ratelimit_handler(e):
    return jsonify({'error': 'Rate limit exceeded', 'retry_after': e.retry_after}), 429

@app.errorhandler(500)
def internal_error(error):
    app.logger.error(f'Server Error: {error}')
    return jsonify({'error': 'Internal server error'}), 500

if __name__ == '__main__':
    app.run(
        host='0.0.0.0' if app.config.get('ENV') == 'production' else '127.0.0.1',
        port=int(os.environ.get('PORT', 5000)),
        debug=False  # Never enable debug in production
    )
"""
}
```

**Authentication Implementation**
```python
# Secure password handling
import bcrypt
from datetime import datetime, timedelta
import jwt
import secrets

class SecureAuth:
    def __init__(self):
        self.jwt_secret = os.environ.get('JWT_SECRET', secrets.token_urlsafe(32))
        self.password_min_length = 12
        
    def hash_password(self, password):
        """
        Securely hash password with bcrypt
        """
        # Validate password strength
        if len(password) < self.password_min_length:
            raise ValueError(f"Password must be at least {self.password_min_length} characters")
            
        # Check common passwords
        if password.lower() in self.load_common_passwords():
            raise ValueError("Password is too common")
            
        # Hash with bcrypt (cost factor 12)
        salt = bcrypt.gensalt(rounds=12)
        return bcrypt.hashpw(password.encode('utf-8'), salt)
    
    def verify_password(self, password, hashed):
        """
        Verify password against hash
        """
        return bcrypt.checkpw(password.encode('utf-8'), hashed)
    
    def generate_token(self, user_id, expires_in=3600):
        """
        Generate secure JWT token
        """
        payload = {
            'user_id': user_id,
            'exp': datetime.utcnow() + timedelta(seconds=expires_in),
            'iat': datetime.utcnow(),
            'jti': secrets.token_urlsafe(16)  # Unique token ID
        }
        
        return jwt.encode(
            payload,
            self.jwt_secret,
            algorithm='HS256'
        )
    
    def verify_token(self, token):
        """
        Verify and decode JWT token
        """
        try:
            payload = jwt.decode(
                token,
                self.jwt_secret,
                algorithms=['HS256']
            )
            return payload
        except jwt.ExpiredSignatureError:
            raise ValueError("Token has expired")
        except jwt.InvalidTokenError:
            raise ValueError("Invalid token")
```

### 8. CI/CD Security Integration

Integrate security scanning into your development pipeline:

**GitHub Actions Security Workflow**
```yaml
# .github/workflows/security.yml
name: Security Scan

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  schedule:
    - cron: '0 2 * * 1'  # Weekly scan on Mondays

jobs:
  security-scan:
    runs-on: ubuntu-latest
    permissions:
      contents: read
      security-events: write
      pull-requests: write
      
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Full history for secret scanning
          
      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '18'
          cache: 'npm'
          
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
          
      - name: Install security tools
        run: |
          # Node.js tools
          npm install -g audit-ci @cyclonedx/cli
          
          # Python tools
          pip install safety bandit semgrep pip-audit
          
          # Container tools
          curl -sfL https://raw.githubusercontent.com/aquasecurity/trivy/main/contrib/install.sh | sh -s -- -b /usr/local/bin
          
          # Secret scanning
          curl -sSfL https://raw.githubusercontent.com/trufflesecurity/trufflehog/main/scripts/install.sh | sh -s -- -b /usr/local/bin
          
      - name: Run secret detection
        run: |
          trufflehog filesystem . --json --no-update > trufflehog-results.json
          
      - name: Upload secret scan results
        if: always()
        uses: github/codeql-action/upload-sarif@v2
        with:
          sarif_file: trufflehog-results.json
          
      - name: JavaScript/TypeScript Security Scan
        if: hashFiles('package.json') != ''
        run: |
          npm ci
          
          # Dependency audit
          npm audit --audit-level moderate --json > npm-audit.json || true
          
          # SAST with ESLint Security
          npx eslint . --ext .js,.jsx,.ts,.tsx --format json --output-file eslint-security.json || true
          
          # Generate SBOM
          npx @cyclonedx/cli --type npm --output-format json --output-file sbom-npm.json
          
      - name: Python Security Scan
        if: hashFiles('requirements.txt', 'setup.py', 'pyproject.toml') != ''
        run: |
          # Install dependencies
          if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
          if [ -f setup.py ]; then pip install -e .; fi
          
          # Dependency vulnerability scan
          safety check --json --output safety-results.json || true
          pip-audit --format=json --output=pip-audit-results.json || true
          
          # SAST with Bandit
          bandit -r . -f json -o bandit-results.json || true
          
          # Advanced SAST with Semgrep
          semgrep --config=auto --json --output=semgrep-results.json . || true
          
      - name: Container Security Scan
        if: hashFiles('Dockerfile', 'docker-compose.yml') != ''
        run: |
          # Build image for scanning
          if [ -f Dockerfile ]; then
            docker build -t security-scan:latest .
            
            # Trivy image scan
            trivy image --format sarif --output trivy-image.sarif security-scan:latest
            
            # Trivy filesystem scan
            trivy fs --format sarif --output trivy-fs.sarif .
          fi
          
      - name: Infrastructure as Code Scan
        if: hashFiles('*.tf', '*.yaml', '*.yml') != ''
        run: |
          # Install Checkov
          pip install checkov
          
          # Scan Terraform
          if ls *.tf 1> /dev/null 2>&1; then
            checkov -f *.tf --framework terraform --output sarif > checkov-terraform.sarif || true
          fi
          
          # Scan Kubernetes manifests
          if ls *.yaml *.yml 1> /dev/null 2>&1; then
            checkov -f *.yaml -f *.yml --framework kubernetes --output sarif > checkov-k8s.sarif || true
          fi
          
      - name: Upload scan results to Security tab
        if: always()
        uses: github/codeql-action/upload-sarif@v2
        with:
          sarif_file: |
            trivy-image.sarif
            trivy-fs.sarif
            checkov-terraform.sarif
            checkov-k8s.sarif
            
      - name: Generate Security Report
        if: always()
        run: |
          python << 'EOF'
          import json
          import glob
          from datetime import datetime
          
          # Collect all scan results
          results = {
              'timestamp': datetime.now().isoformat(),
              'summary': {'total': 0, 'critical': 0, 'high': 0, 'medium': 0, 'low': 0},
              'tools': [],
              'vulnerabilities': []
          }
          
          # Process each result file
          result_files = glob.glob('*-results.json') + glob.glob('*.sarif')
          
          for file in result_files:
              try:
                  with open(file, 'r') as f:
                      data = json.load(f)
                      results['tools'].append(file)
                      # Process based on tool format
                      # (Implementation would parse each tool's output format)
              except:
                  continue
          
          # Generate markdown report
          with open('security-report.md', 'w') as f:
              f.write(f"# Security Scan Report\n\n")
              f.write(f"**Date**: {results['timestamp']}\n\n")
              f.write(f"## Summary\n\n")
              f.write(f"- Total Vulnerabilities: {results['summary']['total']}\n")
              f.write(f"- Critical: {results['summary']['critical']}\n")
              f.write(f"- High: {results['summary']['high']}\n")
              f.write(f"- Medium: {results['summary']['medium']}\n")
              f.write(f"- Low: {results['summary']['low']}\n\n")
              f.write(f"## Tools Used\n\n")
              for tool in results['tools']:
                  f.write(f"- {tool}\n")
          
          print("Security report generated: security-report.md")
          EOF
          
      - name: Comment PR with Security Results
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v6
        with:
          script: |
            const fs = require('fs');
            
            try {
              const report = fs.readFileSync('security-report.md', 'utf8');
              
              await github.rest.issues.createComment({
                issue_number: context.issue.number,
                owner: context.repo.owner,
                repo: context.repo.repo,
                body: '## üîí Security Scan Results\n\n' + report
              });
            } catch (error) {
              console.log('Could not post security report:', error);
            }
            
      - name: Fail on Critical Vulnerabilities
        run: |
          # Check if any critical vulnerabilities found
          CRITICAL_COUNT=$(jq -r '.summary.critical // 0' security-report.json 2>/dev/null || echo "0")
          if [ "$CRITICAL_COUNT" -gt 0 ]; then
            echo "‚ùå Found $CRITICAL_COUNT critical vulnerabilities!"
            echo "Security scan failed due to critical vulnerabilities."
            exit 1
          fi
          
          HIGH_COUNT=$(jq -r '.summary.high // 0' security-report.json 2>/dev/null || echo "0")
          if [ "$HIGH_COUNT" -gt 5 ]; then
            echo "‚ö†Ô∏è Found $HIGH_COUNT high-severity vulnerabilities!"
            echo "Consider addressing high-severity issues."
            # Don't fail for high-severity, just warn
          fi
          
          echo "‚úÖ Security scan completed successfully!"
```

**Automated Remediation Workflow**
```yaml
# .github/workflows/auto-remediation.yml
name: Automated Security Remediation

on:
  schedule:
    - cron: '0 6 * * 2'  # Weekly on Tuesdays
  workflow_dispatch:
    inputs:
      fix_type:
        description: 'Type of fixes to apply'
        required: true
        default: 'dependencies'
        type: choice
        options:
        - dependencies
        - secrets
        - config
        - all

jobs:
  auto-remediation:
    runs-on: ubuntu-latest
    permissions:
      contents: write
      pull-requests: write
      
    steps:
      - uses: actions/checkout@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          
      - name: Set up Node.js
        if: hashFiles('package.json') != ''
        uses: actions/setup-node@v4
        with:
          node-version: '18'
          cache: 'npm'
          
      - name: Auto-fix npm dependencies
        if: contains(github.event.inputs.fix_type, 'dependencies') || contains(github.event.inputs.fix_type, 'all')
        run: |
          if [ -f package.json ]; then
            npm audit fix --force
            npm update
          fi
          
      - name: Auto-fix Python dependencies
        if: contains(github.event.inputs.fix_type, 'dependencies') || contains(github.event.inputs.fix_type, 'all')
        run: |
          if [ -f requirements.txt ]; then
            pip install pip-tools
            pip-compile --upgrade requirements.in
          fi
          
      - name: Remove detected secrets
        if: contains(github.event.inputs.fix_type, 'secrets') || contains(github.event.inputs.fix_type, 'all')
        run: |
          # Install git-filter-repo
          pip install git-filter-repo
          
          # Create backup branch
          git checkout -b security-remediation-$(date +%Y%m%d)
          
          # Remove common secret patterns (be very careful with this)
          echo "Warning: This would remove secrets from git history"
          echo "Manual review required for production use"
          
      - name: Update security configurations
        if: contains(github.event.inputs.fix_type, 'config') || contains(github.event.inputs.fix_type, 'all')
        run: |
          # Add .gitignore entries for common secret files
          cat >> .gitignore << 'EOF'
          
          # Security - ignore potential secret files
          .env
          .env.local
          .env.*.local
          *.pem
          *.key
          *.p12
          *.pfx
          config/secrets.yml
          config/database.yml
          EOF
          
          # Update Docker security
          if [ -f Dockerfile ]; then
            # Add security improvements to Dockerfile
            echo "RUN addgroup -g 1001 -S appgroup && adduser -S appuser -u 1001 -G appgroup" >> Dockerfile.security
            echo "USER appuser" >> Dockerfile.security
          fi
          
      - name: Create Pull Request
        uses: peter-evans/create-pull-request@v5
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          commit-message: 'security: automated vulnerability remediation'
          title: 'üîí Automated Security Fixes'
          body: |
            ## Automated Security Remediation
            
            This PR contains automated fixes for security vulnerabilities:
            
            ### Changes Made
            - ‚úÖ Updated vulnerable dependencies
            - ‚úÖ Added security configurations
            - ‚úÖ Improved .gitignore for secrets
            
            ### Manual Review Required
            - [ ] Verify all dependency updates are compatible
            - [ ] Test application functionality
            - [ ] Review any secret removal changes
            
            **‚ö†Ô∏è Important**: Always test thoroughly before merging automated security fixes.
          branch: security/automated-fixes
          delete-branch: true
```

### 9. Security Report Generation

Generate comprehensive security reports with actionable insights:

**Advanced Reporting System**
```python
import json
import jinja2
from datetime import datetime
from typing import Dict, List, Any
from dataclasses import dataclass

@dataclass
class SecurityMetrics:
    total_vulnerabilities: int
    critical_count: int
    high_count: int
    medium_count: int
    low_count: int
    tools_used: List[str]
    scan_duration: float
    coverage_percentage: float
    false_positive_rate: float

class SecurityReportGenerator:
    def __init__(self):
        self.template_env = jinja2.Environment(
            loader=jinja2.DictLoader({
                'executive_summary': self.EXECUTIVE_TEMPLATE,
                'detailed_report': self.DETAILED_TEMPLATE,
                'dashboard': self.DASHBOARD_TEMPLATE
            })
        )
    
    EXECUTIVE_TEMPLATE = """
# Executive Security Assessment Report

**Assessment Date**: {{ timestamp }}
**Overall Risk Level**: {{ risk_level }}
**Confidence Score**: {{ confidence_score }}%

## Summary
- **Total Vulnerabilities**: {{ metrics.total_vulnerabilities }}
- **Critical**: {{ metrics.critical_count }} ({{ critical_percentage }}%)
- **High**: {{ metrics.high_count }} ({{ high_percentage }}%)
- **Medium**: {{ metrics.medium_count }} ({{ medium_percentage }}%)
- **Low**: {{ metrics.low_count }} ({{ low_percentage }}%)

## Risk Assessment
| Risk Category | Current Level | Target Level | Priority |
|---------------|---------------|--------------|----------|
{% for risk in risk_categories %}
| {{ risk.category }} | {{ risk.current }} | {{ risk.target }} | {{ risk.priority }} |
{% endfor %}

## Immediate Actions Required
{% for action in immediate_actions %}
{{ loop.index }}. **{{ action.title }}** ({{ action.effort }})
   - Impact: {{ action.impact }}
   - Timeline: {{ action.timeline }}
   - Owner: {{ action.owner }}
{% endfor %}

## Compliance Status
{% for framework in compliance_frameworks %}
- **{{ framework.name }}**: {{ framework.status }} ({{ framework.score }}/100)
{% endfor %}

## Investment Required
- **Immediate (0-30 days)**: {{ costs.immediate }}
- **Short-term (1-6 months)**: {{ costs.short_term }}
- **Long-term (6+ months)**: {{ costs.long_term }}
"""
    
    DETAILED_TEMPLATE = """
# Detailed Security Findings Report

## Vulnerability Details
{% for vuln in vulnerabilities %}
### {{ loop.index }}. {{ vuln.title }}

**Severity**: {{ vuln.severity }} | **Confidence**: {{ vuln.confidence }} | **Tool**: {{ vuln.tool }}

**Location**: `{{ vuln.file_path }}:{{ vuln.line_number }}`

**Description**: {{ vuln.description }}

**Impact**: {{ vuln.impact }}

**Remediation**:
```{{ vuln.language }}
{{ vuln.remediation_code }}
```

**References**:
{% for ref in vuln.references %}
- [{{ ref.title }}]({{ ref.url }})
{% endfor %}

---
{% endfor %}

## Tool Effectiveness Analysis
{% for tool in tool_analysis %}
### {{ tool.name }}
- **Vulnerabilities Found**: {{ tool.found_count }}
- **False Positives**: {{ tool.false_positives }}%
- **Execution Time**: {{ tool.execution_time }}s
- **Coverage**: {{ tool.coverage }}%
- **Recommendation**: {{ tool.recommendation }}
{% endfor %}
"""
    
    def generate_comprehensive_report(self, scan_results: Dict[str, Any]) -> Dict[str, str]:
        """Generate all report formats"""
        # Process scan results
        metrics = self._calculate_metrics(scan_results)
        risk_assessment = self._assess_risk(scan_results, metrics)
        compliance_status = self._check_compliance(scan_results)
        
        # Generate different report formats
        reports = {
            'executive_summary': self._generate_executive_summary(
                metrics, risk_assessment, compliance_status
            ),
            'detailed_report': self._generate_detailed_report(scan_results),
            'json_report': json.dumps({
                'metadata': {
                    'timestamp': datetime.now().isoformat(),
                    'version': '2.0',
                    'format': 'sarif-2.1.0'
                },
                'metrics': metrics.__dict__,
                'vulnerabilities': scan_results.get('vulnerabilities', []),
                'risk_assessment': risk_assessment,
                'compliance': compliance_status
            }, indent=2),
            'sarif_report': self._generate_sarif_report(scan_results)
        }
        
        return reports
    
    def _calculate_metrics(self, scan_results: Dict[str, Any]) -> SecurityMetrics:
        """Calculate security metrics from scan results"""
        vulnerabilities = scan_results.get('vulnerabilities', [])
        
        severity_counts = {'CRITICAL': 0, 'HIGH': 0, 'MEDIUM': 0, 'LOW': 0}
        for vuln in vulnerabilities:
            severity = vuln.get('severity', 'UNKNOWN').upper()
            if severity in severity_counts:
                severity_counts[severity] += 1
        
        return SecurityMetrics(
            total_vulnerabilities=len(vulnerabilities),
            critical_count=severity_counts['CRITICAL'],
            high_count=severity_counts['HIGH'],
            medium_count=severity_counts['MEDIUM'],
            low_count=severity_counts['LOW'],
            tools_used=scan_results.get('tools_used', []),
            scan_duration=scan_results.get('scan_duration', 0),
            coverage_percentage=scan_results.get('coverage', 0),
            false_positive_rate=scan_results.get('false_positive_rate', 0)
        )
    
    def _assess_risk(self, scan_results: Dict[str, Any], metrics: SecurityMetrics) -> Dict[str, Any]:
        """Perform comprehensive risk assessment"""
        # Calculate risk score (0-100)
        risk_score = min(100, (
            metrics.critical_count * 25 +
            metrics.high_count * 15 +
            metrics.medium_count * 5 +
            metrics.low_count * 1
        ))
        
        # Determine risk level
        if risk_score >= 80:
            risk_level = 'CRITICAL'
        elif risk_score >= 60:
            risk_level = 'HIGH'
        elif risk_score >= 30:
            risk_level = 'MEDIUM'
        else:
            risk_level = 'LOW'
        
        # Business impact assessment
        business_impact = {
            'data_breach_probability': min(95, risk_score + metrics.critical_count * 10),
            'service_disruption_risk': min(90, risk_score * 0.8),
            'compliance_violation_risk': min(100, risk_score + (metrics.critical_count * 5)),
            'reputation_damage_potential': min(85, risk_score * 0.9)
        }
        
        return {
            'score': risk_score,
            'level': risk_level,
            'business_impact': business_impact,
            'trending': self._calculate_risk_trend(scan_results),
            'peer_comparison': self._compare_with_industry_standards(risk_score)
        }
    
    def _generate_sarif_report(self, scan_results: Dict[str, Any]) -> str:
        """Generate SARIF 2.1.0 compliant report"""
        sarif_report = {
            "version": "2.1.0",
            "$schema": "https://raw.githubusercontent.com/oasis-tcs/sarif-spec/master/Schemata/sarif-schema-2.1.0.json",
            "runs": []
        }
        
        # Group findings by tool
        tools_data = {}
        for vuln in scan_results.get('vulnerabilities', []):
            tool = vuln.get('tool', 'unknown')
            if tool not in tools_data:
                tools_data[tool] = []
            tools_data[tool].append(vuln)
        
        # Create run for each tool
        for tool_name, vulnerabilities in tools_data.items():
            run = {
                "tool": {
                    "driver": {
                        "name": tool_name,
                        "version": "1.0.0",
                        "informationUri": f"https://docs.{tool_name}.com"
                    }
                },
                "results": []
            }
            
            for vuln in vulnerabilities:
                result = {
                    "ruleId": vuln.get('type', 'unknown'),
                    "message": {
                        "text": vuln.get('description', vuln.get('title', 'Security issue detected'))
                    },
                    "level": self._map_severity_to_sarif_level(vuln.get('severity', 'medium')),
                    "locations": [{
                        "physicalLocation": {
                            "artifactLocation": {
                                "uri": vuln.get('file_path', 'unknown')
                            },
                            "region": {
                                "startLine": vuln.get('line_number', 1)
                            }
                        }
                    }]
                }
                
                if vuln.get('cwe'):
                    result["properties"] = {
                        "cwe": vuln.get('cwe'),
                        "confidence": vuln.get('confidence', 'medium')
                    }
                
                run["results"].append(result)
            
            sarif_report["runs"].append(run)
        
        return json.dumps(sarif_report, indent=2)
    
    def _map_severity_to_sarif_level(self, severity: str) -> str:
        """Map severity to SARIF level"""
        mapping = {
            'CRITICAL': 'error',
            'HIGH': 'error',
            'MEDIUM': 'warning',
            'LOW': 'note'
        }
        return mapping.get(severity.upper(), 'warning')

# Usage example
report_generator = SecurityReportGenerator()

# Sample scan results
sample_results = {
    'vulnerabilities': [
        {
            'tool': 'bandit',
            'severity': 'HIGH',
            'title': 'SQL Injection vulnerability',
            'description': 'Parameterized query missing',
            'file_path': 'api/users.py',
            'line_number': 45,
            'cwe': 'CWE-89'
        }
    ],
    'tools_used': ['bandit', 'safety', 'trivy'],
    'scan_duration': 120.5,
    'coverage': 85.2
}

reports = report_generator.generate_comprehensive_report(sample_results)
```

**Executive Summary**
```markdown
## Security Assessment Report

**Date**: 2025-07-19
**Severity**: CRITICAL
**Confidence**: 94%

### Summary
- Total Vulnerabilities: 47
- Critical: 8 (17%)
- High: 15 (32%)
- Medium: 18 (38%)
- Low: 6 (13%)

### Critical Findings
1. **SQL Injection** in user search endpoint (api/search.py:45)
2. **Hardcoded AWS credentials** in config.js:12
3. **Outdated dependencies** with known RCE vulnerabilities
4. **Missing authentication** on admin endpoints

### Business Impact
| Risk Category | Probability | Impact | Priority |
|---------------|-------------|--------|----------|
| Data Breach | 85% | Critical | P0 |
| Service Disruption | 60% | High | P1 |
| Compliance Violation | 90% | Critical | P0 |
| Reputation Damage | 70% | High | P1 |

### Immediate Actions Required (Next 24 Hours)
1. **Patch SQL injection vulnerability** (2 hours) - [@dev-team]
2. **Remove and rotate all hardcoded credentials** (1 hour) - [@security-team]
3. **Block admin endpoints** until auth is implemented (30 minutes) - [@ops-team]

### Short-term Actions (Next 30 Days)
1. **Update critical dependencies** (4 hours)
2. **Implement authentication middleware** (6 hours)
3. **Deploy security headers** (2 hours)
4. **Security training for development team** (8 hours)

### Investment Required
- **Immediate fixes**: $5,000 (40 hours @ $125/hr)
- **Security improvements**: $15,000 (120 hours)
- **Training and processes**: $10,000
- **Total**: $30,000

### Compliance Status
- **OWASP Top 10**: 3/10 major issues
- **SOC 2**: Non-compliant (authentication controls)
- **PCI DSS**: Non-compliant (data protection)
- **GDPR**: At risk (data breach potential)
```

**Detailed Findings with Remediation Code**
```json
{
  "scan_metadata": {
    "timestamp": "2025-07-19T10:30:00Z",
    "version": "2.1",
    "tools_used": ["bandit", "safety", "trivy", "semgrep", "eslint-security"],
    "scan_duration_seconds": 127,
    "coverage_percentage": 94.2,
    "false_positive_rate": 3.1
  },
  "vulnerabilities": [
    {
      "id": "VULN-001",
      "type": "SQL Injection",
      "severity": "CRITICAL",
      "cvss_score": 9.8,
      "cwe": "CWE-89",
      "owasp_category": "A03:2021-Injection",
      "tool": "semgrep",
      "confidence": "high",
      "location": {
        "file": "api/search.js",
        "line": 45,
        "column": 12,
        "code_snippet": "db.query(`SELECT * FROM users WHERE name LIKE '%${req.query.search}%'`)",
        "function": "searchUsers"
      },
      "impact": {
        "description": "Complete database compromise, data exfiltration, potential RCE",
        "business_impact": "Critical - customer data exposure, regulatory violations",
        "affected_users": "All users with search functionality access"
      },
      "remediation": {
        "effort_hours": 2,
        "priority": "P0",
        "description": "Replace string concatenation with parameterized queries",
        "fixed_code": "db.query('SELECT * FROM users WHERE name LIKE ?', [`%${req.query.search}%`])",
        "testing_required": "Unit tests for search functionality",
        "deployment_notes": "No breaking changes, safe to deploy immediately"
      },
      "references": [
        {
          "title": "OWASP SQL Injection Prevention",
          "url": "https://owasp.org/www-community/attacks/SQL_Injection"
        },
        {
          "title": "Node.js Parameterized Queries",
          "url": "https://nodejs.org/en/docs/guides/security/"
        }
      ],
      "exploitability": {
        "ease_of_exploitation": "Very Easy",
        "attack_vector": "Remote",
        "authentication_required": false,
        "user_interaction": false
      }
    },
    {
      "id": "VULN-002",
      "type": "Hardcoded Secrets",
      "severity": "CRITICAL",
      "cvss_score": 9.1,
      "cwe": "CWE-798",
      "tool": "trufflehog",
      "confidence": "verified",
      "location": {
        "file": "config/database.js",
        "line": 12,
        "code_snippet": "const password = 'MyS3cr3tP@ssw0rd123!'"
      },
      "impact": {
        "description": "Database credentials exposure, unauthorized access",
        "business_impact": "Critical - full database access, data breach potential"
      },
      "remediation": {
        "effort_hours": 1,
        "priority": "P0",
        "immediate_actions": [
          "Rotate database password immediately",
          "Remove hardcoded credential from code",
          "Implement environment variable loading"
        ],
        "fixed_code": "const password = process.env.DATABASE_PASSWORD || throwError('Missing DATABASE_PASSWORD')",
        "additional_steps": [
          "Add .env to .gitignore",
          "Update deployment scripts to use secrets management",
          "Scan git history for credential exposure"
        ]
      }
    },
    {
      "id": "VULN-003",
      "type": "Vulnerable Dependency",
      "severity": "HIGH",
      "cvss_score": 8.5,
      "cve": "CVE-2024-1234",
      "tool": "npm-audit",
      "location": {
        "file": "package.json",
        "dependency": "express",
        "version": "4.17.1",
        "vulnerable_path": "express > body-parser > raw-body"
      },
      "impact": {
        "description": "Remote code execution via malformed request body",
        "affected_endpoints": ["/api/upload", "/api/webhook"]
      },
      "remediation": {
        "effort_hours": 0.5,
        "priority": "P1",
        "fixed_version": "4.18.2",
        "update_command": "npm install express@4.18.2",
        "breaking_changes": false,
        "testing_required": "Regression testing for API endpoints"
      }
    }
  ],
  "summary": {
    "total_vulnerabilities": 47,
    "by_severity": {
      "critical": 8,
      "high": 15,
      "medium": 18,
      "low": 6
    },
    "by_category": {
      "injection": 12,
      "broken_auth": 8,
      "sensitive_data": 6,
      "xml_entities": 2,
      "broken_access_control": 5,
      "security_misconfig": 9,
      "xss": 3,
      "insecure_deserialization": 1,
      "vulnerable_components": 15,
      "insufficient_logging": 4
    },
    "remediation_timeline": {
      "immediate_p0": 9,
      "urgent_p1": 18,
      "medium_p2": 15,
      "low_p3": 5
    },
    "total_effort_hours": 47.5,
    "estimated_cost": 5938,
    "risk_score": 89
  },
  "compliance_assessment": {
    "owasp_top_10_2021": {
      "a01_broken_access_control": "FAIL",
      "a02_cryptographic_failures": "PASS",
      "a03_injection": "FAIL",
      "a04_insecure_design": "WARNING",
      "a05_security_misconfiguration": "FAIL",
      "a06_vulnerable_components": "FAIL",
      "a07_identification_failures": "FAIL",
      "a08_software_integrity_failures": "PASS",
      "a09_logging_failures": "WARNING",
      "a10_ssrf": "PASS"
    },
    "frameworks": {
      "nist_cybersecurity": 67,
      "iso_27001": 71,
      "pci_dss": 45,
      "sox_compliance": 78
    }
  }
}
```

### 10. Cross-Command Integration

### Complete Security-First Development Workflow

**Secure API Development Pipeline**
```bash
# 1. Generate secure API scaffolding
/api-scaffold
framework: "fastapi"
security_features: ["jwt_auth", "rate_limiting", "input_validation", "cors"]
database: "postgresql"

# 2. Run comprehensive security scan
/security-scan
scan_types: ["sast", "dependency", "secrets", "container", "iac"]
autofix: true
generate_report: true

# 3. Generate security-aware tests
/test-harness
test_types: ["unit", "security", "penetration"]
security_frameworks: ["bandit", "safety", "owasp-zap"]

# 4. Optimize containers with security hardening
/docker-optimize
security_hardening: true
vulnerability_scanning: true
minimal_base_images: true
```

**Integrated Security Configuration**
```python
# security-config.py - Shared across all commands
class IntegratedSecurityConfig:
    def __init__(self):
        self.api_security = self.load_api_security_config()    # From /api-scaffold
        self.scan_config = self.load_scan_config()             # From /security-scan
        self.test_security = self.load_test_security_config()  # From /test-harness
        self.container_security = self.load_container_config() # From /docker-optimize
        
    def generate_security_middleware(self):
        """Generate security middleware based on API scaffold config"""
        middleware = []
        
        if self.api_security.get('rate_limiting'):
            middleware.append({
                'type': 'rate_limiting',
                'config': {
                    'requests_per_minute': 100,
                    'burst_size': 10,
                    'key_func': 'lambda request: request.client.host'
                }
            })
        
        if self.api_security.get('jwt_auth'):
            middleware.append({
                'type': 'jwt_auth',
                'config': {
                    'secret_key': '${JWT_SECRET_KEY}',
                    'algorithm': 'HS256',
                    'token_expiry': 3600
                }
            })
        
        return middleware
    
    def generate_security_tests(self):
        """Generate security tests based on scan findings"""
        test_cases = []
        
        # SQL Injection tests based on API endpoints
        api_endpoints = self.api_security.get('endpoints', [])
        for endpoint in api_endpoints:
            if endpoint.get('accepts_input'):
                test_cases.append({
                    'type': 'sql_injection',
                    'endpoint': endpoint['path'],
                    'payloads': self.get_sql_injection_payloads()
                })
        
        # Authentication bypass tests
        if self.api_security.get('jwt_auth'):
            test_cases.append({
                'type': 'auth_bypass',
                'scenarios': [
                    'invalid_token',
                    'expired_token',
                    'malformed_token',
                    'no_token'
                ]
            })
        
        return test_cases
    
    def generate_container_security_policies(self):
        """Generate container security policies"""
        policies = {
            'dockerfile_security': {
                'non_root_user': True,
                'minimal_layers': True,
                'security_updates': True,
                'no_secrets_in_layers': True
            },
            'runtime_security': {
                'read_only_filesystem': True,
                'no_new_privileges': True,
                'drop_capabilities': ['ALL'],
                'add_capabilities': ['NET_BIND_SERVICE'] if self.api_security.get('bind_privileged_ports') else []
            }
        }
        return policies
```

**API Security Integration**
```python
# Generated secure API endpoint with integrated security
from fastapi import FastAPI, Depends, HTTPException, Request
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
from slowapi import Limiter, _rate_limit_exceeded_handler
from slowapi.util import get_remote_address
from slowapi.errors import RateLimitExceeded
import jwt
from datetime import datetime, timedelta

# Security configuration from /security-scan
security_config = IntegratedSecurityConfig()

# Rate limiting from security scan recommendations
limiter = Limiter(key_func=get_remote_address)
app = FastAPI()
app.state.limiter = limiter
app.add_exception_handler(RateLimitExceeded, _rate_limit_exceeded_handler)

# JWT authentication from security scan requirements
security = HTTPBearer()

def verify_token(credentials: HTTPAuthorizationCredentials = Depends(security)):
    """JWT verification with security scan compliance"""
    try:
        payload = jwt.decode(
            credentials.credentials, 
            security_config.jwt_secret, 
            algorithms=["HS256"]
        )
        return payload
    except jwt.ExpiredSignatureError:
        raise HTTPException(status_code=401, detail="Token expired")
    except jwt.InvalidTokenError:
        raise HTTPException(status_code=401, detail="Invalid token")

# Secure endpoint with integrated protections
@app.post("/api/v1/users/")
@limiter.limit("10/minute")  # Rate limiting from security scan
async def create_user(
    request: Request,
    user_data: UserCreateSchema,  # Input validation from security scan
    current_user: dict = Depends(verify_token)  # Authentication
):
    """
    Secure user creation endpoint with integrated security controls
    Security features applied:
    - Rate limiting (10 requests/minute)
    - JWT authentication required
    - Input validation via Pydantic
    - SQL injection prevention via ORM
    - XSS prevention via output encoding
    """
    # Additional security validation from scan results
    if not validate_user_input(user_data):
        raise HTTPException(status_code=400, detail="Invalid input data")
    
    # Create user with security logging
    try:
        user = await user_service.create_user(user_data)
        security_logger.log_user_creation(current_user['sub'], user.id)
        return user
    except Exception as e:
        security_logger.log_error("user_creation_failed", str(e))
        raise HTTPException(status_code=500, detail="User creation failed")
```

**Database Security Integration**
```python
# Database security configuration from /db-migrate and /security-scan
class SecureDatabaseConfig:
    def __init__(self):
        self.migration_config = self.load_migration_config()  # From /db-migrate
        self.security_requirements = self.load_security_scan_results()
        
    def generate_secure_migrations(self):
        """Generate database migrations with security controls"""
        migrations = []
        
        # User table with security controls
        migrations.append({
            'operation': 'create_table',
            'table': 'users',
            'columns': [
                {'name': 'id', 'type': 'UUID', 'primary_key': True},
                {'name': 'email', 'type': 'VARCHAR(255)', 'unique': True, 'encrypted': True},
                {'name': 'password_hash', 'type': 'VARCHAR(255)', 'not_null': True},
                {'name': 'created_at', 'type': 'TIMESTAMP', 'default': 'NOW()'},
                {'name': 'last_login', 'type': 'TIMESTAMP'},
                {'name': 'failed_login_attempts', 'type': 'INTEGER', 'default': 0},
                {'name': 'locked_until', 'type': 'TIMESTAMP', 'nullable': True}
            ],
            'security_features': {
                'row_level_security': True,
                'audit_logging': True,
                'field_encryption': ['email'],
                'password_policy': {
                    'min_length': 12,
                    'require_special_chars': True,
                    'require_numbers': True,
                    'expire_days': 90
                }
            }
        })
        
        # Security audit log table
        migrations.append({
            'operation': 'create_table',
            'table': 'security_audit_log',
            'columns': [
                {'name': 'id', 'type': 'UUID', 'primary_key': True},
                {'name': 'user_id', 'type': 'UUID', 'foreign_key': 'users.id'},
                {'name': 'action', 'type': 'VARCHAR(100)', 'not_null': True},
                {'name': 'ip_address', 'type': 'INET', 'not_null': True},
                {'name': 'user_agent', 'type': 'TEXT'},
                {'name': 'timestamp', 'type': 'TIMESTAMP', 'default': 'NOW()'},
                {'name': 'success', 'type': 'BOOLEAN', 'not_null': True},
                {'name': 'details', 'type': 'JSONB'}
            ],
            'indexes': [
                {'name': 'idx_audit_user_timestamp', 'columns': ['user_id', 'timestamp']},
                {'name': 'idx_audit_action_timestamp', 'columns': ['action', 'timestamp']}
            ]
        })
        
        return migrations
```

**Container Security Integration**
```dockerfile
# Dockerfile.secure - Generated with /docker-optimize + /security-scan
# Multi-stage build with security hardening
FROM python:3.11-slim-bookworm AS base

# Security: Create non-root user
RUN groupadd -r appuser && useradd -r -g appuser appuser

# Security: Update packages and remove package manager cache
RUN apt-get update && \
    apt-get upgrade -y && \
    apt-get install -y --no-install-recommends \
        # Only essential packages
        ca-certificates \
        && rm -rf /var/lib/apt/lists/*

# Security: Set work directory with proper permissions
WORKDIR /app
RUN chown appuser:appuser /app

# Install Python dependencies with security checks
COPY requirements.txt .
RUN pip install --no-cache-dir --upgrade pip && \
    pip install --no-cache-dir -r requirements.txt && \
    # Security scan dependencies during build
    pip-audit --format=json --output=/tmp/pip-audit.json && \
    safety check --json --output=/tmp/safety.json

# Copy application code
COPY --chown=appuser:appuser . .

# Security: Remove any secrets or sensitive files
RUN find . -name "*.key" -delete && \
    find . -name "*.pem" -delete && \
    find . -name ".env*" -delete

# Security: Switch to non-root user
USER appuser

# Security: Read-only filesystem, no new privileges
# These will be enforced at runtime via Kubernetes security context

EXPOSE 8000

# Health check for container security monitoring
HEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \
    CMD python -c "import requests; requests.get('http://localhost:8000/health')"

CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
```

**Kubernetes Security Integration**
```yaml
# k8s-secure-deployment.yaml - From /k8s-manifest + /security-scan
apiVersion: v1
kind: ServiceAccount
metadata:
  name: api-service-account
  namespace: production
automountServiceAccountToken: false

---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: api-network-policy
  namespace: production
spec:
  podSelector:
    matchLabels:
      app: api
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          name: ingress-nginx
    ports:
    - protocol: TCP
      port: 8000
  egress:
  - to:
    - namespaceSelector:
        matchLabels:
          name: database
    ports:
    - protocol: TCP
      port: 5432
  - to: []  # DNS
    ports:
    - protocol: UDP
      port: 53

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: api-deployment
  namespace: production
spec:
  replicas: 3
  selector:
    matchLabels:
      app: api
  template:
    metadata:
      labels:
        app: api
      annotations:
        # Security scanning annotations
        container.apparmor.security.beta.kubernetes.io/api: runtime/default
    spec:
      serviceAccountName: api-service-account
      securityContext:
        # Pod-level security context
        runAsNonRoot: true
        runAsUser: 1000
        runAsGroup: 1000
        fsGroup: 1000
        seccompProfile:
          type: RuntimeDefault
      containers:
      - name: api
        image: api:secure-latest
        ports:
        - containerPort: 8000
        securityContext:
          # Container-level security context
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
          runAsNonRoot: true
          runAsUser: 1000
          capabilities:
            drop:
            - ALL
            add:
            - NET_BIND_SERVICE
        resources:
          requests:
            memory: "256Mi"
            cpu: "250m"
          limits:
            memory: "512Mi"
            cpu: "500m"
        env:
        - name: DATABASE_URL
          valueFrom:
            secretKeyRef:
              name: database-credentials
              key: url
        - name: JWT_SECRET_KEY
          valueFrom:
            secretKeyRef:
              name: jwt-secret
              key: secret
        volumeMounts:
        - name: tmp-volume
          mountPath: /tmp
        - name: var-log
          mountPath: /var/log
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /ready
            port: 8000
          initialDelaySeconds: 5
          periodSeconds: 5
      volumes:
      - name: tmp-volume
        emptyDir: {}
      - name: var-log
        emptyDir: {}

---
# Pod Security Policy
apiVersion: policy/v1beta1
kind: PodSecurityPolicy
metadata:
  name: api-psp
spec:
  privileged: false
  allowPrivilegeEscalation: false
  requiredDropCapabilities:
    - ALL
  allowedCapabilities:
    - NET_BIND_SERVICE
  volumes:
    - 'configMap'
    - 'emptyDir'
    - 'projected'
    - 'secret'
    - 'downwardAPI'
    - 'persistentVolumeClaim'
  runAsUser:
    rule: 'MustRunAsNonRoot'
  seLinux:
    rule: 'RunAsAny'
  fsGroup:
    rule: 'RunAsAny'
```

**CI/CD Security Integration**
```yaml
# .github/workflows/security-pipeline.yml
name: Integrated Security Pipeline

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]

jobs:
  security-scan:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4
    
    # 1. Code Security Scanning
    - name: Run Bandit Security Scan
      run: |
        pip install bandit[toml]
        bandit -r . -f sarif -o bandit-results.sarif
    
    - name: Run Semgrep Security Scan
      uses: returntocorp/semgrep-action@v1
      with:
        config: auto
        generateSarif: "1"
    
    # 2. Dependency Security Scanning
    - name: Run Safety Check
      run: |
        pip install safety
        safety check --json --output safety-results.json
    
    - name: Run npm audit
      if: hashFiles('package.json') != ''
      run: |
        npm audit --audit-level high --json > npm-audit-results.json
    
    # 3. Container Security Scanning
    - name: Build Container
      run: docker build -t app:security-test .
    
    - name: Run Trivy Container Scan
      uses: aquasecurity/trivy-action@master
      with:
        image-ref: 'app:security-test'
        format: 'sarif'
        output: 'trivy-results.sarif'
    
    # 4. Infrastructure Security Scanning
    - name: Run Checkov IaC Scan
      uses: bridgecrewio/checkov-action@master
      with:
        directory: .
        output_format: sarif
        output_file_path: checkov-results.sarif
    
    # 5. Secret Scanning
    - name: Run TruffleHog Secret Scan
      uses: trufflesecurity/trufflehog@main
      with:
        path: ./
        base: main
        head: HEAD
        extra_args: --format=sarif --output=trufflehog-results.sarif
    
    # 6. Upload Security Results
    - name: Upload SARIF results to GitHub
      uses: github/codeql-action/upload-sarif@v2
      with:
        sarif_file: |
          bandit-results.sarif
          semgrep.sarif
          trivy-results.sarif
          checkov-results.sarif
          trufflehog-results.sarif
    
    # 7. Security Test Integration
    - name: Run Security Tests
      run: |
        pytest tests/security/ -v --cov=src/security
        
    # 8. Generate Security Report
    - name: Generate Security Dashboard
      run: |
        python scripts/generate_security_report.py \
          --bandit bandit-results.sarif \
          --semgrep semgrep.sarif \
          --trivy trivy-results.sarif \
          --safety safety-results.json \
          --output security-dashboard.html
    
    - name: Upload Security Dashboard
      uses: actions/upload-artifact@v3
      with:
        name: security-dashboard
        path: security-dashboard.html

  penetration-testing:
    runs-on: ubuntu-latest
    needs: security-scan
    if: github.ref == 'refs/heads/main'
    steps:
    - uses: actions/checkout@v4
    
    # Start application for dynamic testing
    - name: Start Application
      run: |
        docker-compose -f docker-compose.test.yml up -d
        sleep 30  # Wait for startup
    
    # OWASP ZAP Dynamic Testing
    - name: Run OWASP ZAP Scan
      uses: zaproxy/action-full-scan@v0.4.0
      with:
        target: 'http://localhost:8000'
        rules_file_name: '.zap/rules.tsv'
        cmd_options: '-a -j -m 10 -T 60'
        
    # API Security Testing
    - name: Run API Security Tests
      run: |
        pip install requests pytest
        pytest tests/api_security/ -v
```

**Monitoring and Alerting Integration**
```python
# security_monitoring.py - Integrated with all commands
import logging
from datetime import datetime
from typing import Dict, Any
import json

class IntegratedSecurityMonitor:
    """Security monitoring that integrates with all command outputs"""
    
    def __init__(self):
        self.api_endpoints = self.load_api_endpoints()      # From /api-scaffold
        self.container_metrics = self.load_container_config() # From /docker-optimize
        self.k8s_security = self.load_k8s_security()        # From /k8s-manifest
        
    def monitor_api_security(self):
        """Monitor API security events"""
        security_events = []
        
        # Monitor authentication failures
        auth_failures = self.get_auth_failure_rate()
        if auth_failures > 10:  # More than 10 failures per minute
            security_events.append({
                'type': 'AUTH_FAILURE_SPIKE',
                'severity': 'HIGH',
                'details': f'Authentication failure rate: {auth_failures}/min',
                'recommended_action': 'Check for brute force attacks'
            })
        
        # Monitor rate limiting violations
        rate_limit_violations = self.get_rate_limit_violations()
        if rate_limit_violations:
            security_events.append({
                'type': 'RATE_LIMIT_VIOLATION',
                'severity': 'MEDIUM',
                'details': f'Rate limit violations: {len(rate_limit_violations)}',
                'ips': [v['ip'] for v in rate_limit_violations],
                'recommended_action': 'Consider IP blocking or CAPTCHA'
            })
        
        return security_events
    
    def monitor_container_security(self):
        """Monitor container security events"""
        container_events = []
        
        # Check for privilege escalation attempts
        privilege_events = self.check_privilege_escalation()
        if privilege_events:
            container_events.append({
                'type': 'PRIVILEGE_ESCALATION',
                'severity': 'CRITICAL',
                'containers': privilege_events,
                'recommended_action': 'Immediate investigation required'
            })
        
        # Check for filesystem violations
        readonly_violations = self.check_readonly_violations()
        if readonly_violations:
            container_events.append({
                'type': 'READONLY_VIOLATION',
                'severity': 'HIGH',
                'violations': readonly_violations,
                'recommended_action': 'Review container security policies'
            })
        
        return container_events
    
    def generate_security_dashboard(self) -> Dict[str, Any]:
        """Generate comprehensive security dashboard"""
        return {
            'timestamp': datetime.utcnow().isoformat(),
            'api_security': self.monitor_api_security(),
            'container_security': self.monitor_container_security(),
            'scan_results': self.get_latest_scan_results(),
            'test_results': self.get_security_test_results(),
            'compliance_status': self.check_compliance_status(),
            'recommendations': self.generate_recommendations()
        }
```

This integrated approach ensures that security is built into every aspect of the application lifecycle, from development through deployment and monitoring.

## Output Format

1. **Tool Selection Matrix**: Recommended tools based on technology stack
2. **Comprehensive Scan Results**: Multi-tool aggregated findings
3. **Executive Security Report**: Business-focused risk assessment
4. **Detailed Technical Findings**: Code-level vulnerabilities with fixes
5. **SARIF Compliance Report**: Industry-standard security report format
6. **Automated Remediation Scripts**: Ready-to-run fix implementations
7. **CI/CD Integration Workflows**: Complete GitHub Actions security pipeline
8. **Compliance Assessment**: OWASP, NIST, ISO 27001 compliance mapping
9. **Business Impact Analysis**: Risk quantification and cost estimates
10. **Monitoring and Alerting Setup**: Real-time security event detection

**Key Features**:
- ‚úÖ **Multi-tool integration**: Bandit, Safety, Trivy, Semgrep, ESLint Security, Snyk
- ‚úÖ **Automated remediation**: Smart dependency updates and configuration fixes
- ‚úÖ **CI/CD ready**: Complete GitHub Actions workflows with SARIF uploads
- ‚úÖ **Business context**: Risk scoring with financial impact estimates
- ‚úÖ **Framework-specific**: Tailored security patterns for Django, Flask, React, Express
- ‚úÖ **Compliance-focused**: Built-in OWASP Top 10, CWE, and regulatory mappings
- ‚úÖ **Actionable insights**: Specific remediation code and deployment guidance

Focus on actionable remediation that can be implemented immediately while maintaining application functionality.
````

## File: tools/slo-implement.md

````markdown
# SLO Implementation Guide

You are an SLO (Service Level Objective) expert specializing in implementing reliability standards and error budget-based engineering practices. Design comprehensive SLO frameworks, establish meaningful SLIs, and create monitoring systems that balance reliability with feature velocity.

## Context
The user needs to implement SLOs to establish reliability targets, measure service performance, and make data-driven decisions about reliability vs. feature development. Focus on practical SLO implementation that aligns with business objectives.

## Requirements
$ARGUMENTS

## Instructions

### 1. SLO Foundation

Establish SLO fundamentals and framework:

**SLO Framework Designer**
```python
import numpy as np
from datetime import datetime, timedelta
from typing import Dict, List, Optional

class SLOFramework:
    def __init__(self, service_name: str):
        self.service = service_name
        self.slos = []
        self.error_budget = None
        
    def design_slo_framework(self):
        """
        Design comprehensive SLO framework
        """
        framework = {
            'service_context': self._analyze_service_context(),
            'user_journeys': self._identify_user_journeys(),
            'sli_candidates': self._identify_sli_candidates(),
            'slo_targets': self._calculate_slo_targets(),
            'error_budgets': self._define_error_budgets(),
            'measurement_strategy': self._design_measurement_strategy()
        }
        
        return self._generate_slo_specification(framework)
    
    def _analyze_service_context(self):
        """Analyze service characteristics for SLO design"""
        return {
            'service_tier': self._determine_service_tier(),
            'user_expectations': self._assess_user_expectations(),
            'business_impact': self._evaluate_business_impact(),
            'technical_constraints': self._identify_constraints(),
            'dependencies': self._map_dependencies()
        }
    
    def _determine_service_tier(self):
        """Determine appropriate service tier and SLO targets"""
        tiers = {
            'critical': {
                'description': 'Revenue-critical or safety-critical services',
                'availability_target': 99.95,
                'latency_p99': 100,
                'error_rate': 0.001,
                'examples': ['payment processing', 'authentication']
            },
            'essential': {
                'description': 'Core business functionality',
                'availability_target': 99.9,
                'latency_p99': 500,
                'error_rate': 0.01,
                'examples': ['search', 'product catalog']
            },
            'standard': {
                'description': 'Standard features',
                'availability_target': 99.5,
                'latency_p99': 1000,
                'error_rate': 0.05,
                'examples': ['recommendations', 'analytics']
            },
            'best_effort': {
                'description': 'Non-critical features',
                'availability_target': 99.0,
                'latency_p99': 2000,
                'error_rate': 0.1,
                'examples': ['batch processing', 'reporting']
            }
        }
        
        # Analyze service characteristics to determine tier
        characteristics = self._analyze_service_characteristics()
        recommended_tier = self._match_tier(characteristics, tiers)
        
        return {
            'recommended': recommended_tier,
            'rationale': self._explain_tier_selection(characteristics),
            'all_tiers': tiers
        }
    
    def _identify_user_journeys(self):
        """Map critical user journeys for SLI selection"""
        journeys = []
        
        # Example user journey mapping
        journey_template = {
            'name': 'User Login',
            'description': 'User authenticates and accesses dashboard',
            'steps': [
                {
                    'step': 'Load login page',
                    'sli_type': 'availability',
                    'threshold': '< 2s load time'
                },
                {
                    'step': 'Submit credentials',
                    'sli_type': 'latency',
                    'threshold': '< 500ms response'
                },
                {
                    'step': 'Validate authentication',
                    'sli_type': 'error_rate',
                    'threshold': '< 0.1% auth failures'
                },
                {
                    'step': 'Load dashboard',
                    'sli_type': 'latency',
                    'threshold': '< 3s full render'
                }
            ],
            'critical_path': True,
            'business_impact': 'high'
        }
        
        return journeys
```

### 2. SLI Selection and Measurement

Choose and implement appropriate SLIs:

**SLI Implementation**
```python
class SLIImplementation:
    def __init__(self):
        self.sli_types = {
            'availability': AvailabilitySLI,
            'latency': LatencySLI,
            'error_rate': ErrorRateSLI,
            'throughput': ThroughputSLI,
            'quality': QualitySLI
        }
    
    def implement_slis(self, service_type):
        """Implement SLIs based on service type"""
        if service_type == 'api':
            return self._api_slis()
        elif service_type == 'web':
            return self._web_slis()
        elif service_type == 'batch':
            return self._batch_slis()
        elif service_type == 'streaming':
            return self._streaming_slis()
    
    def _api_slis(self):
        """SLIs for API services"""
        return {
            'availability': {
                'definition': 'Percentage of successful requests',
                'formula': 'successful_requests / total_requests * 100',
                'implementation': '''
# Prometheus query for API availability
api_availability = """
sum(rate(http_requests_total{status!~"5.."}[5m])) / 
sum(rate(http_requests_total[5m])) * 100
"""

# Implementation
class APIAvailabilitySLI:
    def __init__(self, prometheus_client):
        self.prom = prometheus_client
        
    def calculate(self, time_range='5m'):
        query = f"""
        sum(rate(http_requests_total{{status!~"5.."}}[{time_range}])) / 
        sum(rate(http_requests_total[{time_range}])) * 100
        """
        result = self.prom.query(query)
        return float(result[0]['value'][1])
    
    def calculate_with_exclusions(self, time_range='5m'):
        """Calculate availability excluding certain endpoints"""
        query = f"""
        sum(rate(http_requests_total{{
            status!~"5..",
            endpoint!~"/health|/metrics"
        }}[{time_range}])) / 
        sum(rate(http_requests_total{{
            endpoint!~"/health|/metrics"
        }}[{time_range}])) * 100
        """
        return self.prom.query(query)
'''
            },
            'latency': {
                'definition': 'Percentage of requests faster than threshold',
                'formula': 'fast_requests / total_requests * 100',
                'implementation': '''
# Latency SLI with multiple thresholds
class LatencySLI:
    def __init__(self, thresholds_ms):
        self.thresholds = thresholds_ms  # e.g., {'p50': 100, 'p95': 500, 'p99': 1000}
    
    def calculate_latency_sli(self, time_range='5m'):
        slis = {}
        
        for percentile, threshold in self.thresholds.items():
            query = f"""
            sum(rate(http_request_duration_seconds_bucket{{
                le="{threshold/1000}"
            }}[{time_range}])) / 
            sum(rate(http_request_duration_seconds_count[{time_range}])) * 100
            """
            
            slis[f'latency_{percentile}'] = {
                'value': self.execute_query(query),
                'threshold': threshold,
                'unit': 'ms'
            }
        
        return slis
    
    def calculate_user_centric_latency(self):
        """Calculate latency from user perspective"""
        # Include client-side metrics
        query = """
        histogram_quantile(0.95,
            sum(rate(user_request_duration_bucket[5m])) by (le)
        )
        """
        return self.execute_query(query)
'''
            },
            'error_rate': {
                'definition': 'Percentage of successful requests',
                'formula': '(1 - error_requests / total_requests) * 100',
                'implementation': '''
class ErrorRateSLI:
    def calculate_error_rate(self, time_range='5m'):
        """Calculate error rate with categorization"""
        
        # Different error categories
        error_categories = {
            'client_errors': 'status=~"4.."',
            'server_errors': 'status=~"5.."',
            'timeout_errors': 'status="504"',
            'business_errors': 'error_type="business_logic"'
        }
        
        results = {}
        for category, filter_expr in error_categories.items():
            query = f"""
            sum(rate(http_requests_total{{{filter_expr}}}[{time_range}])) / 
            sum(rate(http_requests_total[{time_range}])) * 100
            """
            results[category] = self.execute_query(query)
        
        # Overall error rate (excluding 4xx)
        overall_query = f"""
        (1 - sum(rate(http_requests_total{{status=~"5.."}}[{time_range}])) / 
        sum(rate(http_requests_total[{time_range}]))) * 100
        """
        results['overall_success_rate'] = self.execute_query(overall_query)
        
        return results
'''
            }
        }
```

### 3. Error Budget Calculation

Implement error budget tracking:

**Error Budget Manager**
```python
class ErrorBudgetManager:
    def __init__(self, slo_target: float, window_days: int):
        self.slo_target = slo_target
        self.window_days = window_days
        self.error_budget_minutes = self._calculate_total_budget()
    
    def _calculate_total_budget(self):
        """Calculate total error budget in minutes"""
        total_minutes = self.window_days * 24 * 60
        allowed_downtime_ratio = 1 - (self.slo_target / 100)
        return total_minutes * allowed_downtime_ratio
    
    def calculate_error_budget_status(self, start_date, end_date):
        """Calculate current error budget status"""
        # Get actual performance
        actual_uptime = self._get_actual_uptime(start_date, end_date)
        
        # Calculate consumed budget
        total_time = (end_date - start_date).total_seconds() / 60
        expected_uptime = total_time * (self.slo_target / 100)
        consumed_minutes = expected_uptime - actual_uptime
        
        # Calculate remaining budget
        remaining_budget = self.error_budget_minutes - consumed_minutes
        burn_rate = consumed_minutes / self.error_budget_minutes
        
        # Project exhaustion
        if burn_rate > 0:
            days_until_exhaustion = (self.window_days * (1 - burn_rate)) / burn_rate
        else:
            days_until_exhaustion = float('inf')
        
        return {
            'total_budget_minutes': self.error_budget_minutes,
            'consumed_minutes': consumed_minutes,
            'remaining_minutes': remaining_budget,
            'burn_rate': burn_rate,
            'budget_percentage_remaining': (remaining_budget / self.error_budget_minutes) * 100,
            'projected_exhaustion_days': days_until_exhaustion,
            'status': self._determine_status(remaining_budget, burn_rate)
        }
    
    def _determine_status(self, remaining_budget, burn_rate):
        """Determine error budget status"""
        if remaining_budget <= 0:
            return 'exhausted'
        elif burn_rate > 2:
            return 'critical'
        elif burn_rate > 1.5:
            return 'warning'
        elif burn_rate > 1:
            return 'attention'
        else:
            return 'healthy'
    
    def generate_burn_rate_alerts(self):
        """Generate multi-window burn rate alerts"""
        return {
            'fast_burn': {
                'description': '14.4x burn rate over 1 hour',
                'condition': 'burn_rate >= 14.4 AND window = 1h',
                'action': 'page',
                'budget_consumed': '2% in 1 hour'
            },
            'slow_burn': {
                'description': '3x burn rate over 6 hours',
                'condition': 'burn_rate >= 3 AND window = 6h',
                'action': 'ticket',
                'budget_consumed': '10% in 6 hours'
            }
        }
```

### 4. SLO Monitoring Setup

Implement comprehensive SLO monitoring:

**SLO Monitoring Implementation**
```yaml
# Prometheus recording rules for SLO
groups:
  - name: slo_rules
    interval: 30s
    rules:
      # Request rate
      - record: service:request_rate
        expr: |
          sum(rate(http_requests_total[5m])) by (service, method, route)
      
      # Success rate
      - record: service:success_rate_5m
        expr: |
          (
            sum(rate(http_requests_total{status!~"5.."}[5m])) by (service)
            /
            sum(rate(http_requests_total[5m])) by (service)
          ) * 100
      
      # Multi-window success rates
      - record: service:success_rate_30m
        expr: |
          (
            sum(rate(http_requests_total{status!~"5.."}[30m])) by (service)
            /
            sum(rate(http_requests_total[30m])) by (service)
          ) * 100
      
      - record: service:success_rate_1h
        expr: |
          (
            sum(rate(http_requests_total{status!~"5.."}[1h])) by (service)
            /
            sum(rate(http_requests_total[1h])) by (service)
          ) * 100
      
      # Latency percentiles
      - record: service:latency_p50_5m
        expr: |
          histogram_quantile(0.50,
            sum(rate(http_request_duration_seconds_bucket[5m])) by (service, le)
          )
      
      - record: service:latency_p95_5m
        expr: |
          histogram_quantile(0.95,
            sum(rate(http_request_duration_seconds_bucket[5m])) by (service, le)
          )
      
      - record: service:latency_p99_5m
        expr: |
          histogram_quantile(0.99,
            sum(rate(http_request_duration_seconds_bucket[5m])) by (service, le)
          )
      
      # Error budget burn rate
      - record: service:error_budget_burn_rate_1h
        expr: |
          (
            1 - (
              sum(increase(http_requests_total{status!~"5.."}[1h])) by (service)
              /
              sum(increase(http_requests_total[1h])) by (service)
            )
          ) / (1 - 0.999) # 99.9% SLO
```

**Alert Configuration**
```yaml
# Multi-window multi-burn-rate alerts
groups:
  - name: slo_alerts
    rules:
      # Fast burn alert (2% budget in 1 hour)
      - alert: ErrorBudgetFastBurn
        expr: |
          (
            service:error_budget_burn_rate_5m{service="api"} > 14.4
            AND
            service:error_budget_burn_rate_1h{service="api"} > 14.4
          )
        for: 2m
        labels:
          severity: critical
          team: platform
        annotations:
          summary: "Fast error budget burn for {{ $labels.service }}"
          description: |
            Service {{ $labels.service }} is burning error budget at 14.4x rate.
            Current burn rate: {{ $value }}x
            This will exhaust 2% of monthly budget in 1 hour.
          
      # Slow burn alert (10% budget in 6 hours)
      - alert: ErrorBudgetSlowBurn
        expr: |
          (
            service:error_budget_burn_rate_30m{service="api"} > 3
            AND
            service:error_budget_burn_rate_6h{service="api"} > 3
          )
        for: 15m
        labels:
          severity: warning
          team: platform
        annotations:
          summary: "Slow error budget burn for {{ $labels.service }}"
          description: |
            Service {{ $labels.service }} is burning error budget at 3x rate.
            Current burn rate: {{ $value }}x
            This will exhaust 10% of monthly budget in 6 hours.
```

### 5. SLO Dashboard

Create comprehensive SLO dashboards:

**Grafana Dashboard Configuration**
```python
def create_slo_dashboard():
    """Generate Grafana dashboard for SLO monitoring"""
    return {
        "dashboard": {
            "title": "Service SLO Dashboard",
            "panels": [
                {
                    "title": "SLO Summary",
                    "type": "stat",
                    "gridPos": {"h": 4, "w": 6, "x": 0, "y": 0},
                    "targets": [{
                        "expr": "service:success_rate_30d{service=\"$service\"}",
                        "legendFormat": "30-day SLO"
                    }],
                    "fieldConfig": {
                        "defaults": {
                            "thresholds": {
                                "mode": "absolute",
                                "steps": [
                                    {"color": "red", "value": None},
                                    {"color": "yellow", "value": 99.5},
                                    {"color": "green", "value": 99.9}
                                ]
                            },
                            "unit": "percent"
                        }
                    }
                },
                {
                    "title": "Error Budget Status",
                    "type": "gauge",
                    "gridPos": {"h": 4, "w": 6, "x": 6, "y": 0},
                    "targets": [{
                        "expr": '''
                        100 * (
                            1 - (
                                (1 - service:success_rate_30d{service="$service"}/100) /
                                (1 - $slo_target/100)
                            )
                        )
                        ''',
                        "legendFormat": "Remaining Budget"
                    }],
                    "fieldConfig": {
                        "defaults": {
                            "min": 0,
                            "max": 100,
                            "thresholds": {
                                "mode": "absolute",
                                "steps": [
                                    {"color": "red", "value": None},
                                    {"color": "yellow", "value": 20},
                                    {"color": "green", "value": 50}
                                ]
                            },
                            "unit": "percent"
                        }
                    }
                },
                {
                    "title": "Burn Rate Trend",
                    "type": "graph",
                    "gridPos": {"h": 8, "w": 12, "x": 12, "y": 0},
                    "targets": [
                        {
                            "expr": "service:error_budget_burn_rate_1h{service=\"$service\"}",
                            "legendFormat": "1h burn rate"
                        },
                        {
                            "expr": "service:error_budget_burn_rate_6h{service=\"$service\"}",
                            "legendFormat": "6h burn rate"
                        },
                        {
                            "expr": "service:error_budget_burn_rate_24h{service=\"$service\"}",
                            "legendFormat": "24h burn rate"
                        }
                    ],
                    "yaxes": [{
                        "format": "short",
                        "label": "Burn Rate (x)",
                        "min": 0
                    }],
                    "alert": {
                        "conditions": [{
                            "evaluator": {"params": [14.4], "type": "gt"},
                            "operator": {"type": "and"},
                            "query": {"params": ["A", "5m", "now"]},
                            "type": "query"
                        }],
                        "name": "High burn rate detected"
                    }
                }
            ]
        }
    }
```

### 6. SLO Reporting

Generate SLO reports and reviews:

**SLO Report Generator**
```python
class SLOReporter:
    def __init__(self, metrics_client):
        self.metrics = metrics_client
        
    def generate_monthly_report(self, service, month):
        """Generate comprehensive monthly SLO report"""
        report_data = {
            'service': service,
            'period': month,
            'slo_performance': self._calculate_slo_performance(service, month),
            'incidents': self._analyze_incidents(service, month),
            'error_budget': self._analyze_error_budget(service, month),
            'trends': self._analyze_trends(service, month),
            'recommendations': self._generate_recommendations(service, month)
        }
        
        return self._format_report(report_data)
    
    def _calculate_slo_performance(self, service, month):
        """Calculate SLO performance metrics"""
        slos = {}
        
        # Availability SLO
        availability_query = f"""
        avg_over_time(
            service:success_rate_5m{{service="{service}"}}[{month}]
        )
        """
        slos['availability'] = {
            'target': 99.9,
            'actual': self.metrics.query(availability_query),
            'met': self.metrics.query(availability_query) >= 99.9
        }
        
        # Latency SLO
        latency_query = f"""
        quantile_over_time(0.95,
            service:latency_p95_5m{{service="{service}"}}[{month}]
        )
        """
        slos['latency_p95'] = {
            'target': 500,  # ms
            'actual': self.metrics.query(latency_query) * 1000,
            'met': self.metrics.query(latency_query) * 1000 <= 500
        }
        
        return slos
    
    def _format_report(self, data):
        """Format report as HTML"""
        return f"""
<!DOCTYPE html>
<html>
<head>
    <title>SLO Report - {data['service']} - {data['period']}</title>
    <style>
        body {{ font-family: Arial, sans-serif; margin: 40px; }}
        .summary {{ background: #f0f0f0; padding: 20px; border-radius: 8px; }}
        .metric {{ margin: 20px 0; }}
        .good {{ color: green; }}
        .bad {{ color: red; }}
        table {{ border-collapse: collapse; width: 100%; }}
        th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}
        .chart {{ margin: 20px 0; }}
    </style>
</head>
<body>
    <h1>SLO Report: {data['service']}</h1>
    <h2>Period: {data['period']}</h2>
    
    <div class="summary">
        <h3>Executive Summary</h3>
        <p>Service reliability: {data['slo_performance']['availability']['actual']:.2f}%</p>
        <p>Error budget remaining: {data['error_budget']['remaining_percentage']:.1f}%</p>
        <p>Number of incidents: {len(data['incidents'])}</p>
    </div>
    
    <div class="metric">
        <h3>SLO Performance</h3>
        <table>
            <tr>
                <th>SLO</th>
                <th>Target</th>
                <th>Actual</th>
                <th>Status</th>
            </tr>
            {self._format_slo_table_rows(data['slo_performance'])}
        </table>
    </div>
    
    <div class="incidents">
        <h3>Incident Analysis</h3>
        {self._format_incident_analysis(data['incidents'])}
    </div>
    
    <div class="recommendations">
        <h3>Recommendations</h3>
        {self._format_recommendations(data['recommendations'])}
    </div>
</body>
</html>
"""
```

### 7. SLO-Based Decision Making

Implement SLO-driven engineering decisions:

**SLO Decision Framework**
```python
class SLODecisionFramework:
    def __init__(self, error_budget_policy):
        self.policy = error_budget_policy
        
    def make_release_decision(self, service, release_risk):
        """Make release decisions based on error budget"""
        budget_status = self.get_error_budget_status(service)
        
        decision_matrix = {
            'healthy': {
                'low_risk': 'approve',
                'medium_risk': 'approve',
                'high_risk': 'review'
            },
            'attention': {
                'low_risk': 'approve',
                'medium_risk': 'review',
                'high_risk': 'defer'
            },
            'warning': {
                'low_risk': 'review',
                'medium_risk': 'defer',
                'high_risk': 'block'
            },
            'critical': {
                'low_risk': 'defer',
                'medium_risk': 'block',
                'high_risk': 'block'
            },
            'exhausted': {
                'low_risk': 'block',
                'medium_risk': 'block',
                'high_risk': 'block'
            }
        }
        
        decision = decision_matrix[budget_status['status']][release_risk]
        
        return {
            'decision': decision,
            'rationale': self._explain_decision(budget_status, release_risk),
            'conditions': self._get_approval_conditions(decision, budget_status),
            'alternative_actions': self._suggest_alternatives(decision, budget_status)
        }
    
    def prioritize_reliability_work(self, service):
        """Prioritize reliability improvements based on SLO gaps"""
        slo_gaps = self.analyze_slo_gaps(service)
        
        priorities = []
        for gap in slo_gaps:
            priority_score = self.calculate_priority_score(gap)
            
            priorities.append({
                'issue': gap['issue'],
                'impact': gap['impact'],
                'effort': gap['estimated_effort'],
                'priority_score': priority_score,
                'recommended_actions': self.recommend_actions(gap)
            })
        
        return sorted(priorities, key=lambda x: x['priority_score'], reverse=True)
    
    def calculate_toil_budget(self, team_size, slo_performance):
        """Calculate how much toil is acceptable based on SLOs"""
        # If meeting SLOs, can afford more toil
        # If not meeting SLOs, need to reduce toil
        
        base_toil_percentage = 50  # Google SRE recommendation
        
        if slo_performance >= 100:
            # Exceeding SLO, can take on more toil
            toil_budget = base_toil_percentage + 10
        elif slo_performance >= 99:
            # Meeting SLO
            toil_budget = base_toil_percentage
        else:
            # Not meeting SLO, reduce toil
            toil_budget = base_toil_percentage - (100 - slo_performance) * 5
        
        return {
            'toil_percentage': max(toil_budget, 20),  # Minimum 20%
            'toil_hours_per_week': (toil_budget / 100) * 40 * team_size,
            'automation_hours_per_week': ((100 - toil_budget) / 100) * 40 * team_size
        }
```

### 8. SLO Templates

Provide SLO templates for common services:

**SLO Template Library**
```python
class SLOTemplates:
    @staticmethod
    def get_api_service_template():
        """SLO template for API services"""
        return {
            'name': 'API Service SLO Template',
            'slos': [
                {
                    'name': 'availability',
                    'description': 'The proportion of successful requests',
                    'sli': {
                        'type': 'ratio',
                        'good_events': 'requests with status != 5xx',
                        'total_events': 'all requests'
                    },
                    'objectives': [
                        {'window': '30d', 'target': 99.9}
                    ]
                },
                {
                    'name': 'latency',
                    'description': 'The proportion of fast requests',
                    'sli': {
                        'type': 'ratio',
                        'good_events': 'requests faster than 500ms',
                        'total_events': 'all requests'
                    },
                    'objectives': [
                        {'window': '30d', 'target': 95.0}
                    ]
                }
            ]
        }
    
    @staticmethod
    def get_data_pipeline_template():
        """SLO template for data pipelines"""
        return {
            'name': 'Data Pipeline SLO Template',
            'slos': [
                {
                    'name': 'freshness',
                    'description': 'Data is processed within SLA',
                    'sli': {
                        'type': 'ratio',
                        'good_events': 'batches processed within 30 minutes',
                        'total_events': 'all batches'
                    },
                    'objectives': [
                        {'window': '7d', 'target': 99.0}
                    ]
                },
                {
                    'name': 'completeness',
                    'description': 'All expected data is processed',
                    'sli': {
                        'type': 'ratio',
                        'good_events': 'records successfully processed',
                        'total_events': 'all records'
                    },
                    'objectives': [
                        {'window': '7d', 'target': 99.95}
                    ]
                }
            ]
        }
```

### 9. SLO Automation

Automate SLO management:

**SLO Automation Tools**
```python
class SLOAutomation:
    def __init__(self):
        self.config = self.load_slo_config()
        
    def auto_generate_slos(self, service_discovery):
        """Automatically generate SLOs for discovered services"""
        services = service_discovery.get_all_services()
        generated_slos = []
        
        for service in services:
            # Analyze service characteristics
            characteristics = self.analyze_service(service)
            
            # Select appropriate template
            template = self.select_template(characteristics)
            
            # Customize based on observed behavior
            customized_slo = self.customize_slo(template, service)
            
            generated_slos.append(customized_slo)
        
        return generated_slos
    
    def implement_progressive_slos(self, service):
        """Implement progressively stricter SLOs"""
        return {
            'phase1': {
                'duration': '1 month',
                'target': 99.0,
                'description': 'Baseline establishment'
            },
            'phase2': {
                'duration': '2 months',
                'target': 99.5,
                'description': 'Initial improvement'
            },
            'phase3': {
                'duration': '3 months',
                'target': 99.9,
                'description': 'Production readiness'
            },
            'phase4': {
                'duration': 'ongoing',
                'target': 99.95,
                'description': 'Excellence'
            }
        }
    
    def create_slo_as_code(self):
        """Define SLOs as code"""
        return '''
# slo_definitions.yaml
apiVersion: slo.dev/v1
kind: ServiceLevelObjective
metadata:
  name: api-availability
  namespace: production
spec:
  service: api-service
  description: API service availability SLO
  
  indicator:
    type: ratio
    counter:
      metric: http_requests_total
      filters:
        - status_code != 5xx
    total:
      metric: http_requests_total
  
  objectives:
    - displayName: 30-day rolling window
      window: 30d
      target: 0.999
      
  alerting:
    burnRates:
      - severity: critical
        shortWindow: 1h
        longWindow: 5m
        burnRate: 14.4
      - severity: warning
        shortWindow: 6h
        longWindow: 30m
        burnRate: 3
        
  annotations:
    runbook: https://runbooks.example.com/api-availability
    dashboard: https://grafana.example.com/d/api-slo
'''
```

### 10. SLO Culture and Governance

Establish SLO culture:

**SLO Governance Framework**
```python
class SLOGovernance:
    def establish_slo_culture(self):
        """Establish SLO-driven culture"""
        return {
            'principles': [
                'SLOs are a shared responsibility',
                'Error budgets drive prioritization',
                'Reliability is a feature',
                'Measure what matters to users'
            ],
            'practices': {
                'weekly_reviews': self.weekly_slo_review_template(),
                'incident_retrospectives': self.slo_incident_template(),
                'quarterly_planning': self.quarterly_slo_planning(),
                'stakeholder_communication': self.stakeholder_report_template()
            },
            'roles': {
                'slo_owner': {
                    'responsibilities': [
                        'Define and maintain SLO definitions',
                        'Monitor SLO performance',
                        'Lead SLO reviews',
                        'Communicate with stakeholders'
                    ]
                },
                'engineering_team': {
                    'responsibilities': [
                        'Implement SLI measurements',
                        'Respond to SLO breaches',
                        'Improve reliability',
                        'Participate in reviews'
                    ]
                },
                'product_owner': {
                    'responsibilities': [
                        'Balance features vs reliability',
                        'Approve error budget usage',
                        'Set business priorities',
                        'Communicate with customers'
                    ]
                }
            }
        }
    
    def create_slo_review_process(self):
        """Create structured SLO review process"""
        return '''
# Weekly SLO Review Template

## Agenda (30 minutes)

### 1. SLO Performance Review (10 min)
- Current SLO status for all services
- Error budget consumption rate
- Trend analysis

### 2. Incident Review (10 min)
- Incidents impacting SLOs
- Root cause analysis
- Action items

### 3. Decision Making (10 min)
- Release approvals/deferrals
- Resource allocation
- Priority adjustments

## Review Checklist

- [ ] All SLOs reviewed
- [ ] Burn rates analyzed
- [ ] Incidents discussed
- [ ] Action items assigned
- [ ] Decisions documented

## Output Template

### Service: [Service Name]
- **SLO Status**: [Green/Yellow/Red]
- **Error Budget**: [XX%] remaining
- **Key Issues**: [List]
- **Actions**: [List with owners]
- **Decisions**: [List]
'''
```

## Output Format

1. **SLO Framework**: Comprehensive SLO design and objectives
2. **SLI Implementation**: Code and queries for measuring SLIs
3. **Error Budget Tracking**: Calculations and burn rate monitoring
4. **Monitoring Setup**: Prometheus rules and Grafana dashboards
5. **Alert Configuration**: Multi-window multi-burn-rate alerts
6. **Reporting Templates**: Monthly reports and reviews
7. **Decision Framework**: SLO-based engineering decisions
8. **Automation Tools**: SLO-as-code and auto-generation
9. **Governance Process**: Culture and review processes

Focus on creating meaningful SLOs that balance reliability with feature velocity, providing clear signals for engineering decisions and fostering a culture of reliability.
````

## File: tools/smart-debug.md

````markdown
Debug complex issues using specialized debugging agents:

[Extended thinking: This tool command leverages the debugger agent with additional support from performance-engineer when performance issues are involved. It provides deep debugging capabilities with root cause analysis.]

## Debugging Approach

### 1. Primary Debug Analysis
Use Task tool with subagent_type="debugger" to:
- Analyze error messages and stack traces
- Identify code paths leading to the issue
- Reproduce the problem systematically
- Isolate the root cause
- Suggest multiple fix approaches

Prompt: "Debug issue: $ARGUMENTS. Provide detailed analysis including:
1. Error reproduction steps
2. Root cause identification
3. Code flow analysis leading to the error
4. Multiple solution approaches with trade-offs
5. Recommended fix with implementation details"

### 2. Performance Debugging (if performance-related)
If the issue involves performance problems, also use Task tool with subagent_type="performance-engineer" to:
- Profile code execution
- Identify bottlenecks
- Analyze resource usage
- Suggest optimization strategies

Prompt: "Profile and debug performance issue: $ARGUMENTS. Include:
1. Performance metrics and profiling data
2. Bottleneck identification
3. Resource usage analysis
4. Optimization recommendations
5. Before/after performance projections"

## Debug Output Structure

### Root Cause Analysis
- Precise identification of the bug source
- Explanation of why the issue occurs
- Impact analysis on other components

### Reproduction Guide
- Step-by-step reproduction instructions
- Required environment setup
- Test data or conditions needed

### Solution Options
1. **Quick Fix** - Minimal change to resolve issue
   - Implementation details
   - Risk assessment
   
2. **Proper Fix** - Best long-term solution
   - Refactoring requirements
   - Testing needs
   
3. **Preventive Measures** - Avoid similar issues
   - Code patterns to adopt
   - Tests to add

### Implementation Guide
- Specific code changes needed
- Order of operations for the fix
- Validation steps

Issue to debug: $ARGUMENTS
````

## File: tools/standup-notes.md

````markdown
# Standup Notes Generator

Generate daily standup notes by reviewing Obsidian vault context and Jira tickets.

## Usage
```
/standup-notes
```

## Process

1. **Gather Context from Obsidian**
   - Use `mcp__mcp-obsidian__obsidian_get_recent_changes` to find recently modified files
   - Use `mcp__mcp-obsidian__obsidian_get_recent_periodic_notes` to get recent daily notes
   - Look for project updates, completed tasks, and ongoing work

2. **Check Jira Tickets**
   - Use `mcp__atlassian__searchJiraIssuesUsingJql` to find tickets assigned to current user
   - Filter for:
     - In Progress tickets (current work)
     - Recently resolved/closed tickets (yesterday's accomplishments)
     - Upcoming/todo tickets (today's planned work)

3. **Generate Standup Notes**
   Format:
   ```
   Morning!
   Yesterday:
   
   ‚Ä¢ [Completed tasks from Jira and Obsidian notes]
   ‚Ä¢ [Key accomplishments and milestones]
   
   Today:
   
   ‚Ä¢ [In-progress Jira tickets]
   ‚Ä¢ [Planned work from tickets and notes]
   ‚Ä¢ [Meetings from calendar/notes]
   
   Note: [Any blockers, dependencies, or important context]
   ```

4. **Write to Obsidian**
   - Create file in `Standup Notes/YYYY-MM-DD.md` format
   - Use `mcp__mcp-obsidian__obsidian_append_content` to write the generated notes

## Implementation Steps

1. Get current user info from Atlassian
2. Search for recent Obsidian changes (last 2 days)
3. Query Jira for:
   - `assignee = currentUser() AND (status CHANGED FROM "In Progress" TO "Done" DURING (-1d, now()) OR resolutiondate >= -1d)`
   - `assignee = currentUser() AND status = "In Progress"`
   - `assignee = currentUser() AND status in ("To Do", "Open") AND (sprint in openSprints() OR priority in (High, Highest))`
4. Parse and categorize findings
5. Generate formatted standup notes
6. Save to Obsidian vault

## Context Extraction Patterns

- Look for keywords: "completed", "finished", "deployed", "released", "fixed", "implemented"
- Extract meeting notes and action items
- Identify blockers or dependencies mentioned
- Pull sprint goals and objectives
````

## File: tools/tech-debt.md

````markdown
# Technical Debt Analysis and Remediation

You are a technical debt expert specializing in identifying, quantifying, and prioritizing technical debt in software projects. Analyze the codebase to uncover debt, assess its impact, and create actionable remediation plans.

## Context
The user needs a comprehensive technical debt analysis to understand what's slowing down development, increasing bugs, and creating maintenance challenges. Focus on practical, measurable improvements with clear ROI.

## Requirements
$ARGUMENTS

## Instructions

### 1. Technical Debt Inventory

Conduct a thorough scan for all types of technical debt:

**Code Debt**
- **Duplicated Code**
  - Exact duplicates (copy-paste)
  - Similar logic patterns
  - Repeated business rules
  - Quantify: Lines duplicated, locations
  
- **Complex Code**
  - High cyclomatic complexity (>10)
  - Deeply nested conditionals (>3 levels)
  - Long methods (>50 lines)
  - God classes (>500 lines, >20 methods)
  - Quantify: Complexity scores, hotspots

- **Poor Structure**
  - Circular dependencies
  - Inappropriate intimacy between classes
  - Feature envy (methods using other class data)
  - Shotgun surgery patterns
  - Quantify: Coupling metrics, change frequency

**Architecture Debt**
- **Design Flaws**
  - Missing abstractions
  - Leaky abstractions
  - Violated architectural boundaries
  - Monolithic components
  - Quantify: Component size, dependency violations

- **Technology Debt**
  - Outdated frameworks/libraries
  - Deprecated API usage
  - Legacy patterns (e.g., callbacks vs promises)
  - Unsupported dependencies
  - Quantify: Version lag, security vulnerabilities

**Testing Debt**
- **Coverage Gaps**
  - Untested code paths
  - Missing edge cases
  - No integration tests
  - Lack of performance tests
  - Quantify: Coverage %, critical paths untested

- **Test Quality**
  - Brittle tests (environment-dependent)
  - Slow test suites
  - Flaky tests
  - No test documentation
  - Quantify: Test runtime, failure rate

**Documentation Debt**
- **Missing Documentation**
  - No API documentation
  - Undocumented complex logic
  - Missing architecture diagrams
  - No onboarding guides
  - Quantify: Undocumented public APIs

**Infrastructure Debt**
- **Deployment Issues**
  - Manual deployment steps
  - No rollback procedures
  - Missing monitoring
  - No performance baselines
  - Quantify: Deployment time, failure rate

### 2. Impact Assessment

Calculate the real cost of each debt item:

**Development Velocity Impact**
```
Debt Item: Duplicate user validation logic
Locations: 5 files
Time Impact: 
- 2 hours per bug fix (must fix in 5 places)
- 4 hours per feature change
- Monthly impact: ~20 hours
Annual Cost: 240 hours √ó $150/hour = $36,000
```

**Quality Impact**
```
Debt Item: No integration tests for payment flow
Bug Rate: 3 production bugs/month
Average Bug Cost:
- Investigation: 4 hours
- Fix: 2 hours  
- Testing: 2 hours
- Deployment: 1 hour
Monthly Cost: 3 bugs √ó 9 hours √ó $150 = $4,050
Annual Cost: $48,600
```

**Risk Assessment**
- **Critical**: Security vulnerabilities, data loss risk
- **High**: Performance degradation, frequent outages
- **Medium**: Developer frustration, slow feature delivery
- **Low**: Code style issues, minor inefficiencies

### 3. Debt Metrics Dashboard

Create measurable KPIs:

**Code Quality Metrics**
```yaml
Metrics:
  cyclomatic_complexity:
    current: 15.2
    target: 10.0
    files_above_threshold: 45
    
  code_duplication:
    percentage: 23%
    target: 5%
    duplication_hotspots:
      - src/validation: 850 lines
      - src/api/handlers: 620 lines
      
  test_coverage:
    unit: 45%
    integration: 12%
    e2e: 5%
    target: 80% / 60% / 30%
    
  dependency_health:
    outdated_major: 12
    outdated_minor: 34
    security_vulnerabilities: 7
    deprecated_apis: 15
```

**Trend Analysis**
```python
debt_trends = {
    "2024_Q1": {"score": 750, "items": 125},
    "2024_Q2": {"score": 820, "items": 142},
    "2024_Q3": {"score": 890, "items": 156},
    "growth_rate": "18% quarterly",
    "projection": "1200 by 2025_Q1 without intervention"
}
```

### 4. Prioritized Remediation Plan

Create an actionable roadmap based on ROI:

**Quick Wins (High Value, Low Effort)**
Week 1-2:
```
1. Extract duplicate validation logic to shared module
   Effort: 8 hours
   Savings: 20 hours/month
   ROI: 250% in first month

2. Add error monitoring to payment service
   Effort: 4 hours
   Savings: 15 hours/month debugging
   ROI: 375% in first month

3. Automate deployment script
   Effort: 12 hours
   Savings: 2 hours/deployment √ó 20 deploys/month
   ROI: 333% in first month
```

**Medium-Term Improvements (Month 1-3)**
```
1. Refactor OrderService (God class)
   - Split into 4 focused services
   - Add comprehensive tests
   - Create clear interfaces
   Effort: 60 hours
   Savings: 30 hours/month maintenance
   ROI: Positive after 2 months

2. Upgrade React 16 ‚Üí 18
   - Update component patterns
   - Migrate to hooks
   - Fix breaking changes
   Effort: 80 hours  
   Benefits: Performance +30%, Better DX
   ROI: Positive after 3 months
```

**Long-Term Initiatives (Quarter 2-4)**
```
1. Implement Domain-Driven Design
   - Define bounded contexts
   - Create domain models
   - Establish clear boundaries
   Effort: 200 hours
   Benefits: 50% reduction in coupling
   ROI: Positive after 6 months

2. Comprehensive Test Suite
   - Unit: 80% coverage
   - Integration: 60% coverage
   - E2E: Critical paths
   Effort: 300 hours
   Benefits: 70% reduction in bugs
   ROI: Positive after 4 months
```

### 5. Implementation Strategy

**Incremental Refactoring**
```python
# Phase 1: Add facade over legacy code
class PaymentFacade:
    def __init__(self):
        self.legacy_processor = LegacyPaymentProcessor()
    
    def process_payment(self, order):
        # New clean interface
        return self.legacy_processor.doPayment(order.to_legacy())

# Phase 2: Implement new service alongside
class PaymentService:
    def process_payment(self, order):
        # Clean implementation
        pass

# Phase 3: Gradual migration
class PaymentFacade:
    def __init__(self):
        self.new_service = PaymentService()
        self.legacy = LegacyPaymentProcessor()
        
    def process_payment(self, order):
        if feature_flag("use_new_payment"):
            return self.new_service.process_payment(order)
        return self.legacy.doPayment(order.to_legacy())
```

**Team Allocation**
```yaml
Debt_Reduction_Team:
  dedicated_time: "20% sprint capacity"
  
  roles:
    - tech_lead: "Architecture decisions"
    - senior_dev: "Complex refactoring"  
    - dev: "Testing and documentation"
    
  sprint_goals:
    - sprint_1: "Quick wins completed"
    - sprint_2: "God class refactoring started"
    - sprint_3: "Test coverage >60%"
```

### 6. Prevention Strategy

Implement gates to prevent new debt:

**Automated Quality Gates**
```yaml
pre_commit_hooks:
  - complexity_check: "max 10"
  - duplication_check: "max 5%"
  - test_coverage: "min 80% for new code"
  
ci_pipeline:
  - dependency_audit: "no high vulnerabilities"
  - performance_test: "no regression >10%"
  - architecture_check: "no new violations"
  
code_review:
  - requires_two_approvals: true
  - must_include_tests: true
  - documentation_required: true
```

**Debt Budget**
```python
debt_budget = {
    "allowed_monthly_increase": "2%",
    "mandatory_reduction": "5% per quarter",
    "tracking": {
        "complexity": "sonarqube",
        "dependencies": "dependabot",
        "coverage": "codecov"
    }
}
```

### 7. Communication Plan

**Stakeholder Reports**
```markdown
## Executive Summary
- Current debt score: 890 (High)
- Monthly velocity loss: 35%
- Bug rate increase: 45%
- Recommended investment: 500 hours
- Expected ROI: 280% over 12 months

## Key Risks
1. Payment system: 3 critical vulnerabilities
2. Data layer: No backup strategy
3. API: Rate limiting not implemented

## Proposed Actions
1. Immediate: Security patches (this week)
2. Short-term: Core refactoring (1 month)
3. Long-term: Architecture modernization (6 months)
```

**Developer Documentation**
```markdown
## Refactoring Guide
1. Always maintain backward compatibility
2. Write tests before refactoring
3. Use feature flags for gradual rollout
4. Document architectural decisions
5. Measure impact with metrics

## Code Standards
- Complexity limit: 10
- Method length: 20 lines
- Class length: 200 lines
- Test coverage: 80%
- Documentation: All public APIs
```

### 8. Success Metrics

Track progress with clear KPIs:

**Monthly Metrics**
- Debt score reduction: Target -5%
- New bug rate: Target -20%
- Deployment frequency: Target +50%
- Lead time: Target -30%
- Test coverage: Target +10%

**Quarterly Reviews**
- Architecture health score
- Developer satisfaction survey
- Performance benchmarks
- Security audit results
- Cost savings achieved

## Output Format

1. **Debt Inventory**: Comprehensive list categorized by type with metrics
2. **Impact Analysis**: Cost calculations and risk assessments
3. **Prioritized Roadmap**: Quarter-by-quarter plan with clear deliverables
4. **Quick Wins**: Immediate actions for this sprint
5. **Implementation Guide**: Step-by-step refactoring strategies
6. **Prevention Plan**: Processes to avoid accumulating new debt
7. **ROI Projections**: Expected returns on debt reduction investment

Focus on delivering measurable improvements that directly impact development velocity, system reliability, and team morale.
````

## File: tools/test-harness.md

````markdown
# Comprehensive Test Harness Generator

You are a testing expert specializing in creating comprehensive, maintainable, and efficient test suites for modern applications. Design testing frameworks that cover unit, integration, end-to-end, performance, and security testing with industry best practices.

## Context
The user needs a complete testing strategy and implementation for their application. Focus on creating a robust testing pyramid with appropriate tools, patterns, and automation that ensures code quality and reliability.

## Requirements
$ARGUMENTS

## Instructions

### 1. Testing Framework Selection

Choose appropriate testing frameworks based on technology stack:

**Framework Selection Matrix**
```python
def select_testing_framework(tech_stack, project_type):
    """Select optimal testing frameworks based on technology"""
    
    frameworks = {
        'python': {
            'unit_testing': 'pytest',
            'mocking': 'pytest-mock, unittest.mock',
            'property_testing': 'hypothesis',
            'load_testing': 'locust',
            'contract_testing': 'pact-python',
            'security_testing': 'bandit, safety',
            'api_testing': 'requests, httpx',
            'async_testing': 'pytest-asyncio'
        },
        'javascript': {
            'unit_testing': 'jest, vitest',
            'mocking': 'jest, sinon',
            'property_testing': 'fast-check',
            'load_testing': 'artillery, k6',
            'contract_testing': 'pact-js',
            'security_testing': 'npm audit, snyk',
            'api_testing': 'supertest, axios',
            'e2e_testing': 'playwright, cypress'
        },
        'java': {
            'unit_testing': 'junit5, testng',
            'mocking': 'mockito, powermock',
            'property_testing': 'jqwik',
            'load_testing': 'gatling, jmeter',
            'contract_testing': 'pact-jvm',
            'security_testing': 'spotbugs, dependency-check',
            'api_testing': 'rest-assured',
            'integration_testing': 'testcontainers'
        },
        'go': {
            'unit_testing': 'testing, testify',
            'mocking': 'testify/mock, gomock',
            'property_testing': 'gopter',
            'load_testing': 'vegeta, hey',
            'contract_testing': 'pact-go',
            'security_testing': 'gosec, nancy',
            'api_testing': 'ginkgo, httptest',
            'fuzzing': 'go-fuzz'
        }
    }
    
    return frameworks.get(tech_stack, frameworks['python'])
```

### 2. Python Testing Implementation

Complete Python testing framework with pytest:

**Project Structure**
```
project/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ api/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ routes.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ models.py
‚îÇ   ‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ user_service.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ payment_service.py
‚îÇ   ‚îî‚îÄ‚îÄ utils/
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îî‚îÄ‚îÄ validators.py
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îú‚îÄ‚îÄ conftest.py
‚îÇ   ‚îú‚îÄ‚îÄ unit/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_services.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_utils.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ test_models.py
‚îÇ   ‚îú‚îÄ‚îÄ integration/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_api_endpoints.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_database.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ test_external_services.py
‚îÇ   ‚îú‚îÄ‚îÄ e2e/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_user_flows.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ test_payment_flows.py
‚îÇ   ‚îú‚îÄ‚îÄ performance/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_load.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ test_stress.py
‚îÇ   ‚îú‚îÄ‚îÄ security/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_auth.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ test_input_validation.py
‚îÇ   ‚îî‚îÄ‚îÄ fixtures/
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îú‚îÄ‚îÄ data_factories.py
‚îÇ       ‚îî‚îÄ‚îÄ test_data.py
‚îú‚îÄ‚îÄ pytest.ini
‚îú‚îÄ‚îÄ requirements-test.txt
‚îî‚îÄ‚îÄ .github/workflows/test.yml
```

**Test Configuration**
```ini
# pytest.ini
[tool:pytest]
minversion = 7.0
addopts = 
    -ra
    --strict-markers
    --strict-config
    --cov=src
    --cov-report=term-missing:skip-covered
    --cov-report=html:htmlcov
    --cov-report=xml
    --cov-fail-under=90
    --junitxml=pytest-results.xml
    --tb=short
    -p no:warnings
testpaths = tests
python_files = test_*.py
python_classes = Test*
python_functions = test_*
markers =
    unit: Unit tests
    integration: Integration tests
    e2e: End-to-end tests
    slow: Slow tests
    security: Security tests
    performance: Performance tests
    smoke: Smoke tests
    regression: Regression tests
    api: API tests
    database: Database tests
    external: Tests requiring external services
filterwarnings =
    error
    ignore::UserWarning
    ignore::DeprecationWarning
```

**Advanced Test Configuration**
```python
# conftest.py
import pytest
import asyncio
import tempfile
import shutil
from pathlib import Path
from unittest.mock import Mock, AsyncMock
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker
from fastapi.testclient import TestClient
import redis
from datetime import datetime, timedelta
import factory
from faker import Faker

from src.database import Base
from src.main import app
from src.config import Settings

fake = Faker()

# Pytest configuration
def pytest_configure(config):
    """Configure pytest with custom settings"""
    config.addinivalue_line(
        "markers", "vcr: mark test to use VCR cassettes"
    )
    config.addinivalue_line(
        "markers", "freeze_time: mark test to freeze time"
    )

@pytest.fixture(scope="session")
def event_loop():
    """Create an instance of the default event loop for the test session"""
    loop = asyncio.get_event_loop_policy().new_event_loop()
    yield loop
    loop.close()

# Database fixtures
@pytest.fixture(scope="session")
def test_db_engine():
    """Create test database engine"""
    engine = create_engine(
        "sqlite:///:memory:",
        connect_args={"check_same_thread": False}
    )
    Base.metadata.create_all(engine)
    yield engine
    engine.dispose()

@pytest.fixture
def db_session(test_db_engine):
    """Create database session for test"""
    connection = test_db_engine.connect()
    transaction = connection.begin()
    session = sessionmaker(bind=connection)()
    
    yield session
    
    session.close()
    transaction.rollback()
    connection.close()

# API client fixtures
@pytest.fixture
def api_client():
    """Create test API client"""
    with TestClient(app) as client:
        yield client

@pytest.fixture
def authenticated_client(api_client, test_user):
    """Create authenticated API client"""
    # Login and get token
    login_data = {"email": test_user.email, "password": "testpass123"}
    response = api_client.post("/auth/login", json=login_data)
    token = response.json()["access_token"]
    
    # Set authorization header
    api_client.headers.update({"Authorization": f"Bearer {token}"})
    yield api_client

# Data factories
class UserFactory(factory.Factory):
    class Meta:
        model = dict
    
    id = factory.Sequence(lambda n: n)
    email = factory.LazyAttribute(lambda obj: fake.email())
    username = factory.LazyAttribute(lambda obj: fake.user_name())
    full_name = factory.LazyAttribute(lambda obj: fake.name())
    is_active = True
    created_at = factory.LazyFunction(datetime.utcnow)

@pytest.fixture
def test_user():
    """Create test user"""
    return UserFactory()

@pytest.fixture
def test_users():
    """Create multiple test users"""
    return UserFactory.build_batch(5)

# Mock fixtures
@pytest.fixture
def mock_redis():
    """Mock Redis client"""
    redis_mock = Mock(spec=redis.Redis)
    redis_mock.get.return_value = None
    redis_mock.set.return_value = True
    redis_mock.delete.return_value = 1
    redis_mock.exists.return_value = 0
    return redis_mock

@pytest.fixture
def mock_email_service():
    """Mock email service"""
    mock = AsyncMock()
    mock.send_email.return_value = {"status": "sent", "message_id": "test-123"}
    return mock

@pytest.fixture
def mock_payment_gateway():
    """Mock payment gateway"""
    mock = Mock()
    mock.process_payment.return_value = {
        "transaction_id": "txn_123",
        "status": "success",
        "amount": 100.00
    }
    return mock

# Time fixtures
@pytest.fixture
def freeze_time():
    """Freeze time for testing"""
    from freezegun import freeze_time as _freeze_time
    with _freeze_time("2024-01-01 12:00:00") as frozen_time:
        yield frozen_time

# File system fixtures
@pytest.fixture
def temp_dir():
    """Create temporary directory"""
    temp_path = Path(tempfile.mkdtemp())
    yield temp_path
    shutil.rmtree(temp_path)

# Environment fixtures
@pytest.fixture
def test_settings():
    """Test application settings"""
    return Settings(
        TESTING=True,
        DATABASE_URL="sqlite:///:memory:",
        REDIS_URL="redis://localhost:6379/1",
        SECRET_KEY="test-secret-key"
    )

# Network fixtures
@pytest.fixture(scope="session")
def vcr_config():
    """VCR configuration for recording HTTP interactions"""
    return {
        "filter_headers": ["authorization", "x-api-key"],
        "record_mode": "once",
        "match_on": ["uri", "method"],
        "cassette_library_dir": "tests/fixtures/cassettes"
    }
```

**Unit Testing Implementation**
```python
# tests/unit/test_user_service.py
import pytest
from unittest.mock import Mock, patch, AsyncMock
from datetime import datetime, timedelta
from hypothesis import given, strategies as st

from src.services.user_service import UserService
from src.models.user import User
from src.exceptions import UserNotFoundError, DuplicateEmailError

class TestUserService:
    """Test suite for UserService"""
    
    @pytest.fixture
    def user_service(self, db_session):
        """Create UserService instance"""
        return UserService(db_session)
    
    @pytest.fixture
    def sample_user_data(self):
        """Sample user data for testing"""
        return {
            "email": "test@example.com",
            "username": "testuser",
            "full_name": "Test User",
            "password": "securepass123"
        }
    
    def test_create_user_success(self, user_service, sample_user_data):
        """Test successful user creation"""
        # Act
        user = user_service.create_user(sample_user_data)
        
        # Assert
        assert user.email == sample_user_data["email"]
        assert user.username == sample_user_data["username"]
        assert user.full_name == sample_user_data["full_name"]
        assert user.is_active is True
        assert user.created_at is not None
        assert hasattr(user, 'hashed_password') is False  # Password not exposed
    
    def test_create_user_duplicate_email(self, user_service, sample_user_data):
        """Test user creation with duplicate email"""
        # Arrange
        user_service.create_user(sample_user_data)
        
        # Act & Assert
        with pytest.raises(DuplicateEmailError) as exc_info:
            user_service.create_user(sample_user_data)
        
        assert "already exists" in str(exc_info.value)
    
    @pytest.mark.parametrize("invalid_email", [
        "invalid-email",
        "@example.com",
        "test@",
        "",
        None
    ])
    def test_create_user_invalid_email(self, user_service, sample_user_data, invalid_email):
        """Test user creation with invalid email formats"""
        # Arrange
        sample_user_data["email"] = invalid_email
        
        # Act & Assert
        with pytest.raises(ValueError):
            user_service.create_user(sample_user_data)
    
    @given(
        email=st.emails(),
        username=st.text(min_size=3, max_size=30, alphabet=st.characters(whitelist_categories=['L', 'N'])),
        full_name=st.text(min_size=1, max_size=100)
    )
    def test_create_user_property_based(self, user_service, email, username, full_name):
        """Property-based test for user creation"""
        # Arrange
        user_data = {
            "email": email,
            "username": username,
            "full_name": full_name,
            "password": "validpassword123"
        }
        
        # Act
        user = user_service.create_user(user_data)
        
        # Assert
        assert user.email == email
        assert user.username == username
        assert user.full_name == full_name
    
    def test_get_user_by_id_success(self, user_service, test_user):
        """Test retrieving user by ID"""
        # Arrange
        created_user = user_service.create_user(test_user)
        
        # Act
        retrieved_user = user_service.get_user_by_id(created_user.id)
        
        # Assert
        assert retrieved_user.id == created_user.id
        assert retrieved_user.email == created_user.email
    
    def test_get_user_by_id_not_found(self, user_service):
        """Test retrieving non-existent user"""
        # Act & Assert
        with pytest.raises(UserNotFoundError):
            user_service.get_user_by_id(99999)
    
    @patch('src.services.user_service.send_welcome_email')
    def test_create_user_sends_welcome_email(self, mock_send_email, user_service, sample_user_data):
        """Test that welcome email is sent on user creation"""
        # Act
        user = user_service.create_user(sample_user_data)
        
        # Assert
        mock_send_email.assert_called_once_with(user.email, user.full_name)
    
    @pytest.mark.asyncio
    async def test_create_user_async(self, user_service, sample_user_data):
        """Test async user creation"""
        # Act
        user = await user_service.create_user_async(sample_user_data)
        
        # Assert
        assert user.email == sample_user_data["email"]
    
    def test_update_user_success(self, user_service, test_user):
        """Test successful user update"""
        # Arrange
        created_user = user_service.create_user(test_user)
        update_data = {"full_name": "Updated Name"}
        
        # Act
        updated_user = user_service.update_user(created_user.id, update_data)
        
        # Assert
        assert updated_user.full_name == "Updated Name"
        assert updated_user.email == created_user.email  # Unchanged
    
    def test_authenticate_user_success(self, user_service, sample_user_data):
        """Test successful user authentication"""
        # Arrange
        user = user_service.create_user(sample_user_data)
        
        # Act
        authenticated_user = user_service.authenticate_user(
            sample_user_data["email"], 
            sample_user_data["password"]
        )
        
        # Assert
        assert authenticated_user.id == user.id
    
    def test_authenticate_user_wrong_password(self, user_service, sample_user_data):
        """Test authentication with wrong password"""
        # Arrange
        user_service.create_user(sample_user_data)
        
        # Act & Assert
        with pytest.raises(ValueError, match="Invalid credentials"):
            user_service.authenticate_user(
                sample_user_data["email"], 
                "wrongpassword"
            )
```

**Integration Testing Implementation**
```python
# tests/integration/test_api_endpoints.py
import pytest
import json
from datetime import datetime, timedelta
from unittest.mock import patch

class TestUserAPIEndpoints:
    """Integration tests for User API endpoints"""
    
    def test_create_user_endpoint(self, api_client):
        """Test user creation endpoint"""
        # Arrange
        user_data = {
            "email": "newuser@example.com",
            "username": "newuser",
            "password": "securepass123",
            "full_name": "New User"
        }
        
        # Act
        response = api_client.post("/api/v1/users/", json=user_data)
        
        # Assert
        assert response.status_code == 201
        data = response.json()
        assert data["email"] == user_data["email"]
        assert data["username"] == user_data["username"]
        assert "id" in data
        assert "password" not in data
    
    def test_create_user_validation_error(self, api_client):
        """Test user creation with validation errors"""
        # Arrange
        invalid_data = {
            "email": "invalid-email",
            "username": "ab",  # Too short
            "password": "123"  # Too short
        }
        
        # Act
        response = api_client.post("/api/v1/users/", json=invalid_data)
        
        # Assert
        assert response.status_code == 422
        errors = response.json()["detail"]
        assert any("email" in error["loc"] for error in errors)
        assert any("username" in error["loc"] for error in errors)
        assert any("password" in error["loc"] for error in errors)
    
    def test_get_user_authenticated(self, authenticated_client, test_user):
        """Test getting user profile when authenticated"""
        # Act
        response = authenticated_client.get("/api/v1/users/me")
        
        # Assert
        assert response.status_code == 200
        data = response.json()
        assert "email" in data
        assert "username" in data
    
    def test_get_user_unauthenticated(self, api_client):
        """Test getting user profile without authentication"""
        # Act
        response = api_client.get("/api/v1/users/me")
        
        # Assert
        assert response.status_code == 401
    
    def test_rate_limiting(self, api_client):
        """Test API rate limiting"""
        # Arrange
        endpoint = "/api/v1/users/"
        user_data = {
            "email": f"test{i}@example.com",
            "username": f"user{i}",
            "password": "securepass123"
        }
        
        # Act - Make rapid requests
        responses = []
        for i in range(102):  # Exceed rate limit of 100
            user_data["email"] = f"test{i}@example.com"
            user_data["username"] = f"user{i}"
            response = api_client.post(endpoint, json=user_data)
            responses.append(response)
        
        # Assert
        status_codes = [r.status_code for r in responses]
        assert 429 in status_codes  # Too Many Requests
    
    @pytest.mark.slow
    def test_bulk_user_operations(self, api_client):
        """Test creating and managing multiple users"""
        # Arrange
        user_count = 50
        users_data = [
            {
                "email": f"bulk{i}@example.com",
                "username": f"bulkuser{i}",
                "password": "securepass123",
                "full_name": f"Bulk User {i}"
            }
            for i in range(user_count)
        ]
        
        # Act - Create users
        created_users = []
        for user_data in users_data:
            response = api_client.post("/api/v1/users/", json=user_data)
            assert response.status_code == 201
            created_users.append(response.json())
        
        # Act - List users
        response = api_client.get("/api/v1/users/?limit=100")
        
        # Assert
        assert response.status_code == 200
        users_list = response.json()
        assert len(users_list) >= user_count
```

**End-to-End Testing Implementation**
```python
# tests/e2e/test_user_flows.py
import pytest
from playwright.async_api import async_playwright, Page
import asyncio

@pytest.mark.e2e
class TestUserFlows:
    """End-to-end tests for user workflows"""
    
    @pytest.fixture
    async def browser_page(self):
        """Create browser page for testing"""
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            page = await browser.new_page()
            yield page
            await browser.close()
    
    @pytest.mark.asyncio
    async def test_user_registration_flow(self, browser_page: Page):
        """Test complete user registration flow"""
        # Navigate to registration page
        await browser_page.goto("http://localhost:3000/register")
        
        # Fill registration form
        await browser_page.fill('[data-testid="email-input"]', 'e2e@example.com')
        await browser_page.fill('[data-testid="username-input"]', 'e2euser')
        await browser_page.fill('[data-testid="password-input"]', 'securepass123')
        await browser_page.fill('[data-testid="confirm-password-input"]', 'securepass123')
        
        # Submit form
        await browser_page.click('[data-testid="register-button"]')
        
        # Verify redirect to dashboard
        await browser_page.wait_for_url("**/dashboard")
        
        # Verify welcome message
        welcome_text = await browser_page.text_content('[data-testid="welcome-message"]')
        assert "Welcome, e2euser" in welcome_text
    
    @pytest.mark.asyncio
    async def test_login_logout_flow(self, browser_page: Page, test_user):
        """Test login and logout flow"""
        # Navigate to login page
        await browser_page.goto("http://localhost:3000/login")
        
        # Fill login form
        await browser_page.fill('[data-testid="email-input"]', test_user["email"])
        await browser_page.fill('[data-testid="password-input"]', test_user["password"])
        
        # Submit login
        await browser_page.click('[data-testid="login-button"]')
        
        # Verify successful login
        await browser_page.wait_for_url("**/dashboard")
        
        # Test logout
        await browser_page.click('[data-testid="user-menu"]')
        await browser_page.click('[data-testid="logout-button"]')
        
        # Verify redirect to home page
        await browser_page.wait_for_url("**/")
    
    @pytest.mark.asyncio
    async def test_password_reset_flow(self, browser_page: Page):
        """Test password reset flow"""
        # Navigate to password reset page
        await browser_page.goto("http://localhost:3000/forgot-password")
        
        # Fill email
        await browser_page.fill('[data-testid="email-input"]', 'test@example.com')
        
        # Submit request
        await browser_page.click('[data-testid="reset-button"]')
        
        # Verify success message
        success_message = await browser_page.text_content('[data-testid="success-message"]')
        assert "reset link sent" in success_message.lower()
```

**Performance Testing Implementation**
```python
# tests/performance/test_load.py
import pytest
import asyncio
import aiohttp
import time
from statistics import mean, median
from concurrent.futures import ThreadPoolExecutor
import locust
from locust import HttpUser, task, between

class APILoadTest(HttpUser):
    """Locust load test for API endpoints"""
    
    wait_time = between(1, 3)
    host = "http://localhost:8000"
    
    def on_start(self):
        """Setup for each user"""
        # Create test user and login
        user_data = {
            "email": f"loadtest{self.host}@example.com",
            "username": f"loaduser{int(time.time())}",
            "password": "loadtest123"
        }
        
        response = self.client.post("/api/v1/users/", json=user_data)
        if response.status_code == 201:
            # Login to get token
            login_data = {
                "email": user_data["email"],
                "password": user_data["password"]
            }
            login_response = self.client.post("/api/v1/auth/login", json=login_data)
            if login_response.status_code == 200:
                token = login_response.json()["access_token"]
                self.client.headers.update({"Authorization": f"Bearer {token}"})
    
    @task(3)
    def get_user_profile(self):
        """Test getting user profile"""
        self.client.get("/api/v1/users/me")
    
    @task(2)
    def list_users(self):
        """Test listing users"""
        self.client.get("/api/v1/users/?limit=10")
    
    @task(1)
    def update_profile(self):
        """Test updating user profile"""
        update_data = {"full_name": "Updated Name"}
        self.client.put("/api/v1/users/me", json=update_data)

@pytest.mark.performance
class TestAPIPerformance:
    """Performance tests for API endpoints"""
    
    @pytest.mark.asyncio
    async def test_concurrent_user_creation(self):
        """Test concurrent user creation performance"""
        async def create_user(session, user_id):
            user_data = {
                "email": f"perf{user_id}@example.com",
                "username": f"perfuser{user_id}",
                "password": "perftest123"
            }
            
            start_time = time.time()
            async with session.post(
                "http://localhost:8000/api/v1/users/",
                json=user_data
            ) as response:
                end_time = time.time()
                return {
                    "status": response.status,
                    "duration": end_time - start_time,
                    "user_id": user_id
                }
        
        # Test with 100 concurrent requests
        async with aiohttp.ClientSession() as session:
            tasks = [create_user(session, i) for i in range(100)]
            results = await asyncio.gather(*tasks)
        
        # Analyze results
        successful_requests = [r for r in results if r["status"] == 201]
        durations = [r["duration"] for r in successful_requests]
        
        # Assertions
        assert len(successful_requests) >= 95  # 95% success rate
        assert mean(durations) < 1.0  # Average response time < 1s
        assert max(durations) < 5.0   # Max response time < 5s
        assert median(durations) < 0.5  # Median response time < 0.5s
    
    def test_database_query_performance(self, db_session):
        """Test database query performance"""
        # Create test data
        users = []
        for i in range(1000):
            user_data = {
                "email": f"dbperf{i}@example.com",
                "username": f"dbperfuser{i}",
                "password": "dbperftest123"
            }
            user = user_service.create_user(user_data)
            users.append(user)
        
        # Test query performance
        start_time = time.time()
        result = db_session.query(User).limit(100).all()
        query_time = time.time() - start_time
        
        # Assertions
        assert len(result) == 100
        assert query_time < 0.1  # Query should complete in < 100ms
    
    @pytest.mark.slow
    def test_memory_usage(self):
        """Test memory usage during operations"""
        import psutil
        import gc
        
        process = psutil.Process()
        
        # Baseline memory
        gc.collect()
        initial_memory = process.memory_info().rss / 1024 / 1024  # MB
        
        # Perform memory-intensive operations
        users = []
        for i in range(10000):
            user_data = {
                "email": f"mem{i}@example.com",
                "username": f"memuser{i}",
                "password": "memtest123"
            }
            users.append(user_data)
        
        # Check memory after operations
        peak_memory = process.memory_info().rss / 1024 / 1024  # MB
        memory_increase = peak_memory - initial_memory
        
        # Cleanup
        del users
        gc.collect()
        final_memory = process.memory_info().rss / 1024 / 1024  # MB
        
        # Assertions
        assert memory_increase < 100  # Memory increase < 100MB
        assert final_memory - initial_memory < 10  # Memory leak < 10MB
```

**Security Testing Implementation**
```python
# tests/security/test_auth.py
import pytest
import jwt
from datetime import datetime, timedelta
from unittest.mock import patch

@pytest.mark.security
class TestAuthenticationSecurity:
    """Security tests for authentication system"""
    
    def test_password_hashing(self, user_service):
        """Test password is properly hashed"""
        # Arrange
        user_data = {
            "email": "security@example.com",
            "username": "securityuser",
            "password": "plainpassword123"
        }
        
        # Act
        user = user_service.create_user(user_data)
        
        # Assert
        # Password should be hashed, not stored in plain text
        assert user.hashed_password != user_data["password"]
        assert len(user.hashed_password) > 50  # Bcrypt hashes are long
        assert user.hashed_password.startswith("$2b$")  # Bcrypt format
    
    def test_jwt_token_expiration(self, api_client, test_user):
        """Test JWT token expiration"""
        # Arrange - Create user and login
        api_client.post("/api/v1/users/", json=test_user)
        login_response = api_client.post("/api/v1/auth/login", json={
            "email": test_user["email"],
            "password": test_user["password"]
        })
        token = login_response.json()["access_token"]
        
        # Act - Decode token to check expiration
        decoded = jwt.decode(token, options={"verify_signature": False})
        exp_timestamp = decoded["exp"]
        exp_datetime = datetime.fromtimestamp(exp_timestamp)
        
        # Assert
        assert exp_datetime > datetime.utcnow()  # Token not expired
        assert exp_datetime < datetime.utcnow() + timedelta(days=8)  # Reasonable expiry
    
    def test_invalid_token_rejection(self, api_client):
        """Test API rejects invalid tokens"""
        # Arrange
        invalid_tokens = [
            "invalid.token.format",
            "Bearer invalid_token",
            "",
            "expired_token_here"
        ]
        
        for invalid_token in invalid_tokens:
            # Act
            headers = {"Authorization": f"Bearer {invalid_token}"}
            response = api_client.get("/api/v1/users/me", headers=headers)
            
            # Assert
            assert response.status_code == 401
    
    def test_sql_injection_prevention(self, api_client):
        """Test SQL injection prevention"""
        # Arrange - Malicious payloads
        sql_injection_payloads = [
            "test'; DROP TABLE users; --",
            "test' OR '1'='1",
            "test' UNION SELECT * FROM users --",
            "'; DELETE FROM users WHERE '1'='1'; --"
        ]
        
        for payload in sql_injection_payloads:
            # Act
            user_data = {
                "email": f"{payload}@example.com",
                "username": payload,
                "password": "testpass123"
            }
            response = api_client.post("/api/v1/users/", json=user_data)
            
            # Assert - Should handle gracefully, not execute SQL
            assert response.status_code in [400, 422]  # Validation error, not SQL error
    
    def test_xss_prevention(self, api_client):
        """Test XSS prevention in API responses"""
        # Arrange
        xss_payloads = [
            "<script>alert('xss')</script>",
            "javascript:alert('xss')",
            "<img src=x onerror=alert('xss')>",
            "';alert('xss');//"
        ]
        
        for payload in xss_payloads:
            # Act
            user_data = {
                "email": "xsstest@example.com",
                "username": "xssuser",
                "full_name": payload,
                "password": "testpass123"
            }
            response = api_client.post("/api/v1/users/", json=user_data)
            
            if response.status_code == 201:
                # Assert - Response should be properly escaped
                response_text = response.text
                assert "<script>" not in response_text
                assert "javascript:" not in response_text
                assert "onerror=" not in response_text
    
    def test_rate_limiting_security(self, api_client):
        """Test rate limiting prevents brute force attacks"""
        # Arrange
        login_data = {
            "email": "victim@example.com",
            "password": "wrongpassword"
        }
        
        # Act - Attempt multiple failed logins
        failed_attempts = 0
        for i in range(20):
            response = api_client.post("/api/v1/auth/login", json=login_data)
            if response.status_code == 429:  # Rate limited
                break
            failed_attempts += 1
        
        # Assert - Should be rate limited before 20 attempts
        assert failed_attempts < 15  # Rate limiting kicks in
    
    def test_csrf_protection(self, api_client):
        """Test CSRF protection for state-changing operations"""
        # This test depends on your CSRF implementation
        # Example for cookie-based sessions
        pass
    
    @pytest.mark.parametrize("sensitive_header", [
        "X-API-Key",
        "Authorization",
        "X-Access-Token"
    ])
    def test_sensitive_headers_not_logged(self, api_client, sensitive_header):
        """Test sensitive headers are not logged"""
        with patch('src.middleware.logger.info') as mock_logger:
            # Act
            headers = {sensitive_header: "sensitive_value"}
            api_client.get("/api/v1/users/", headers=headers)
            
            # Assert
            logged_calls = [str(call) for call in mock_logger.call_args_list]
            for call in logged_calls:
                assert "sensitive_value" not in call
```

**Contract Testing Implementation**
```python
# tests/contract/test_api_contract.py
import pytest
from pact import Consumer, Provider, Like, EachLike, Term
import requests

# Consumer test (API client perspective)
@pytest.fixture
def pact():
    """Create Pact consumer"""
    pact = Consumer('UserServiceClient').has_pact_with(Provider('UserServiceAPI'))
    pact.start()
    yield pact
    pact.stop()

class TestUserServiceContract:
    """Contract tests for User Service API"""
    
    def test_get_user_success(self, pact):
        """Test contract for successful user retrieval"""
        # Arrange
        expected_response = {
            'id': Like(123),
            'email': Like('user@example.com'),
            'username': Like('testuser'),
            'full_name': Like('Test User'),
            'is_active': Like(True),
            'created_at': Term(r'\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}', '2024-01-01T12:00:00')
        }
        
        (pact
         .given('user with ID 123 exists')
         .upon_receiving('a request for user 123')
         .with_request('GET', '/api/v1/users/123')
         .will_respond_with(200, body=expected_response))
        
        # Act
        with pact:
            response = requests.get('http://localhost:1234/api/v1/users/123')
        
        # Assert
        assert response.status_code == 200
        assert response.json()['id'] == 123
    
    def test_create_user_success(self, pact):
        """Test contract for successful user creation"""
        # Arrange
        user_data = {
            'email': 'newuser@example.com',
            'username': 'newuser',
            'password': 'securepass123'
        }
        
        expected_response = {
            'id': Like(456),
            'email': Like('newuser@example.com'),
            'username': Like('newuser'),
            'is_active': Like(True)
        }
        
        (pact
         .given('no user exists with email newuser@example.com')
         .upon_receiving('a request to create a user')
         .with_request('POST', '/api/v1/users/', body=user_data)
         .will_respond_with(201, body=expected_response))
        
        # Act
        with pact:
            response = requests.post(
                'http://localhost:1234/api/v1/users/',
                json=user_data
            )
        
        # Assert
        assert response.status_code == 201
        assert response.json()['email'] == 'newuser@example.com'
```

### 3. JavaScript/TypeScript Testing Implementation

Complete JavaScript testing framework with Jest and Playwright:

**Jest Configuration**
```javascript
// jest.config.js
module.exports = {
  preset: 'ts-jest',
  testEnvironment: 'node',
  roots: ['<rootDir>/src', '<rootDir>/tests'],
  testMatch: [
    '**/__tests__/**/*.+(ts|tsx|js)',
    '**/*.(test|spec).+(ts|tsx|js)'
  ],
  transform: {
    '^.+\\.(ts|tsx)$': 'ts-jest'
  },
  collectCoverageFrom: [
    'src/**/*.{ts,tsx}',
    '!src/**/*.d.ts',
    '!src/types/**/*'
  ],
  coverageDirectory: 'coverage',
  coverageReporters: ['text', 'lcov', 'html'],
  coverageThreshold: {
    global: {
      branches: 80,
      functions: 80,
      lines: 80,
      statements: 80
    }
  },
  setupFilesAfterEnv: ['<rootDir>/tests/setup.ts'],
  testTimeout: 10000,
  maxWorkers: '50%'
};

// tests/setup.ts
import { jest } from '@jest/globals';

// Global test setup
beforeAll(async () => {
  // Setup test database
  // Initialize test services
});

afterAll(async () => {
  // Cleanup
});

beforeEach(() => {
  // Reset mocks
  jest.clearAllMocks();
});

// Global mocks
jest.mock('nodemailer', () => ({
  createTransport: jest.fn(() => ({
    sendMail: jest.fn().mockResolvedValue({ messageId: 'test-id' })
  }))
}));
```

**Unit Testing with Jest**
```typescript
// tests/unit/userService.test.ts
import { UserService } from '../../src/services/userService';
import { PrismaClient } from '@prisma/client';
import bcrypt from 'bcryptjs';
import { jest } from '@jest/globals';

// Mock Prisma
jest.mock('@prisma/client');
const mockPrisma = {
  user: {
    create: jest.fn(),
    findUnique: jest.fn(),
    findFirst: jest.fn(),
    findMany: jest.fn(),
    update: jest.fn(),
    delete: jest.fn()
  }
};

describe('UserService', () => {
  let userService: UserService;
  
  beforeEach(() => {
    (PrismaClient as jest.MockedClass<typeof PrismaClient>).mockImplementation(() => mockPrisma as any);
    userService = new UserService();
  });
  
  describe('createUser', () => {
    it('should create user successfully', async () => {
      // Arrange
      const userData = {
        email: 'test@example.com',
        username: 'testuser',
        password: 'password123',
        fullName: 'Test User'
      };
      
      const mockUser = {
        id: '1',
        email: userData.email,
        username: userData.username,
        fullName: userData.fullName,
        isActive: true,
        createdAt: new Date(),
        updatedAt: new Date()
      };
      
      mockPrisma.user.findFirst.mockResolvedValue(null);
      mockPrisma.user.create.mockResolvedValue(mockUser);
      
      // Act
      const result = await userService.createUser(userData);
      
      // Assert
      expect(result).toEqual(mockUser);
      expect(mockPrisma.user.create).toHaveBeenCalledWith({
        data: expect.objectContaining({
          email: userData.email,
          username: userData.username,
          fullName: userData.fullName,
          hashedPassword: expect.any(String),
          isActive: true
        })
      });
    });
    
    it('should throw error for duplicate email', async () => {
      // Arrange
      const userData = {
        email: 'existing@example.com',
        username: 'testuser',
        password: 'password123'
      };
      
      mockPrisma.user.findFirst.mockResolvedValue({ id: '1' });
      
      // Act & Assert
      await expect(userService.createUser(userData))
        .rejects
        .toThrow('User with this email or username already exists');
    });
    
    it('should hash password before storing', async () => {
      // Arrange
      const userData = {
        email: 'test@example.com',
        username: 'testuser',
        password: 'plainpassword'
      };
      
      mockPrisma.user.findFirst.mockResolvedValue(null);
      mockPrisma.user.create.mockResolvedValue({} as any);
      
      // Spy on bcrypt
      const hashSpy = jest.spyOn(bcrypt, 'hash').mockResolvedValue('hashedpassword' as never);
      
      // Act
      await userService.createUser(userData);
      
      // Assert
      expect(hashSpy).toHaveBeenCalledWith(userData.password, 12);
      expect(mockPrisma.user.create).toHaveBeenCalledWith({
        data: expect.objectContaining({
          hashedPassword: 'hashedpassword'
        })
      });
    });
  });
  
  describe('authenticateUser', () => {
    it('should return token for valid credentials', async () => {
      // Arrange
      const email = 'test@example.com';
      const password = 'password123';
      const mockUser = {
        id: '1',
        email,
        hashedPassword: 'hashedpassword'
      };
      
      mockPrisma.user.findUnique.mockResolvedValue(mockUser);
      jest.spyOn(bcrypt, 'compare').mockResolvedValue(true as never);
      
      // Act
      const result = await userService.authenticateUser(email, password);
      
      // Assert
      expect(result).toBeTruthy();
      expect(typeof result).toBe('string');
    });
    
    it('should return null for invalid credentials', async () => {
      // Arrange
      const email = 'test@example.com';
      const password = 'wrongpassword';
      
      mockPrisma.user.findUnique.mockResolvedValue(null);
      
      // Act
      const result = await userService.authenticateUser(email, password);
      
      // Assert
      expect(result).toBeNull();
    });
  });
});
```

**Property-Based Testing with fast-check**
```typescript
// tests/unit/userValidation.property.test.ts
import fc from 'fast-check';
import { validateEmail, validateUsername } from '../../src/utils/validation';

describe('User Validation - Property Tests', () => {
  describe('validateEmail', () => {
    it('should always return boolean', () => {
      fc.assert(fc.property(
        fc.emailAddress(),
        (email) => {
          const result = validateEmail(email);
          expect(typeof result).toBe('boolean');
        }
      ));
    });
    
    it('should accept valid email formats', () => {
      fc.assert(fc.property(
        fc.emailAddress(),
        (email) => {
          expect(validateEmail(email)).toBe(true);
        }
      ));
    });
    
    it('should reject strings without @ symbol', () => {
      fc.assert(fc.property(
        fc.string().filter(s => !s.includes('@')),
        (invalidEmail) => {
          expect(validateEmail(invalidEmail)).toBe(false);
        }
      ));
    });
  });
  
  describe('validateUsername', () => {
    it('should accept alphanumeric strings of valid length', () => {
      fc.assert(fc.property(
        fc.string({ minLength: 3, maxLength: 30 })
          .filter(s => /^[a-zA-Z0-9]+$/.test(s)),
        (username) => {
          expect(validateUsername(username)).toBe(true);
        }
      ));
    });
    
    it('should reject strings that are too short or too long', () => {
      fc.assert(fc.property(
        fc.oneof(
          fc.string({ maxLength: 2 }),
          fc.string({ minLength: 31 })
        ),
        (username) => {
          expect(validateUsername(username)).toBe(false);
        }
      ));
    });
  });
});
```

### 4. CI/CD Integration

**GitHub Actions Workflow**
```yaml
# .github/workflows/test.yml
name: Test Suite

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]

env:
  NODE_ENV: test
  DATABASE_URL: postgresql://postgres:postgres@localhost:5432/test_db

jobs:
  test:
    runs-on: ubuntu-latest
    
    strategy:
      matrix:
        python-version: [3.9, 3.10, 3.11]
        node-version: [16, 18, 20]
    
    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: test_db
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
      
      redis:
        image: redis:7
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: Set up Node.js ${{ matrix.node-version }}
      uses: actions/setup-node@v4
      with:
        node-version: ${{ matrix.node-version }}
    
    - name: Cache Python dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements*.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-
    
    - name: Cache Node dependencies
      uses: actions/cache@v3
      with:
        path: ~/.npm
        key: ${{ runner.os }}-node-${{ hashFiles('**/package-lock.json') }}
        restore-keys: |
          ${{ runner.os }}-node-
    
    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-test.txt
    
    - name: Install Node dependencies
      run: npm ci
    
    - name: Lint Python code
      run: |
        flake8 src tests
        black --check src tests
        isort --check-only src tests
        mypy src
    
    - name: Lint JavaScript/TypeScript code
      run: |
        npm run lint
        npm run type-check
    
    - name: Run security scans
      run: |
        bandit -r src
        npm audit --audit-level high
        safety check
    
    - name: Run Python unit tests
      run: |
        pytest tests/unit/ -v --cov=src --cov-report=xml --junitxml=pytest-results.xml
    
    - name: Run JavaScript unit tests
      run: |
        npm run test:unit -- --coverage --ci --watchAll=false
    
    - name: Run integration tests
      run: |
        pytest tests/integration/ -v --junitxml=integration-results.xml
        npm run test:integration
    
    - name: Run end-to-end tests
      run: |
        pytest tests/e2e/ -v --junitxml=e2e-results.xml
        npm run test:e2e
    
    - name: Run performance tests
      if: github.event_name == 'push' && github.ref == 'refs/heads/main'
      run: |
        pytest tests/performance/ -v --junitxml=performance-results.xml
        npm run test:performance
    
    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        files: ./coverage.xml,./coverage/lcov.info
        fail_ci_if_error: true
    
    - name: Upload test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: test-results-${{ matrix.python-version }}-${{ matrix.node-version }}
        path: |
          *-results.xml
          coverage/
          htmlcov/
    
    - name: Comment PR with test results
      if: github.event_name == 'pull_request'
      uses: dorny/test-reporter@v1
      with:
        name: Test Results
        path: '*-results.xml'
        reporter: java-junit

  mutation-testing:
    runs-on: ubuntu-latest
    needs: test
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: 3.11
    
    - name: Install dependencies
      run: |
        pip install -r requirements.txt
        pip install mutmut
    
    - name: Run mutation testing
      run: |
        mutmut run --paths-to-mutate src/
        mutmut results
        mutmut html
    
    - name: Upload mutation test results
      uses: actions/upload-artifact@v3
      with:
        name: mutation-test-results
        path: html/

  contract-testing:
    runs-on: ubuntu-latest
    needs: test
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: 3.11
    
    - name: Install dependencies
      run: |
        pip install pact-python
    
    - name: Run Pact consumer tests
      run: |
        pytest tests/contract/ -v
    
    - name: Publish Pacts
      if: github.ref == 'refs/heads/main'
      run: |
        pact-broker publish pacts/ --broker-base-url ${{ secrets.PACT_BROKER_URL }} --broker-token ${{ secrets.PACT_BROKER_TOKEN }}
```

### 5. Test Data Management

**Test Data Factories**
```python
# tests/fixtures/data_factories.py
import factory
from datetime import datetime, timedelta
from faker import Faker
import random

fake = Faker()

class UserFactory(factory.Factory):
    class Meta:
        model = dict
    
    id = factory.Sequence(lambda n: n)
    email = factory.LazyAttribute(lambda obj: fake.email())
    username = factory.LazyAttribute(lambda obj: fake.user_name())
    full_name = factory.LazyAttribute(lambda obj: fake.name())
    is_active = True
    created_at = factory.LazyFunction(datetime.utcnow)
    updated_at = factory.LazyAttribute(lambda obj: obj.created_at)
    
    @factory.post_generation
    def password(obj, create, extracted, **kwargs):
        if extracted:
            obj['password'] = extracted
        else:
            obj['password'] = fake.password(length=12)

class AdminUserFactory(UserFactory):
    is_admin = True
    email = factory.LazyAttribute(lambda obj: f"admin_{fake.user_name()}@example.com")

class InactiveUserFactory(UserFactory):
    is_active = False

class RecentUserFactory(UserFactory):
    created_at = factory.LazyFunction(
        lambda: datetime.utcnow() - timedelta(days=random.randint(1, 7))
    )

# Test data generators
class TestDataGenerator:
    @staticmethod
    def generate_test_users(count=10, user_type='standard'):
        """Generate test users of different types"""
        factories = {
            'standard': UserFactory,
            'admin': AdminUserFactory,
            'inactive': InactiveUserFactory,
            'recent': RecentUserFactory
        }
        
        factory_class = factories.get(user_type, UserFactory)
        return factory_class.build_batch(count)
    
    @staticmethod
    def generate_large_dataset(users=1000, posts=5000, comments=10000):
        """Generate large test dataset for performance testing"""
        return {
            'users': UserFactory.build_batch(users),
            'posts': PostFactory.build_batch(posts),
            'comments': CommentFactory.build_batch(comments)
        }
```

## Cross-Command Integration

### Complete Development Workflow Integration

**API Development + Testing Pipeline**
```bash
# 1. Generate API scaffolding
/api-scaffold
project_type: "microservice"
framework: "fastapi"
features: ["auth", "database", "monitoring"]

# 2. Generate comprehensive test suite
/test-harness
test_types: ["unit", "integration", "e2e", "performance", "security"]
framework: "pytest"
coverage_threshold: 90

# 3. Run security scans on tests
/security-scan
include_test_code: true
scan_types: ["static", "dependency", "secrets"]

# 4. Optimize Docker for testing
/docker-optimize
environment: "test"
include_test_data: true
optimization_level: "speed"
```

**Test-Driven Development Workflow**
```python
# Generated test configuration that integrates with all tools
class IntegratedTestConfig:
    def __init__(self):
        self.api_config = self.load_api_config()  # From /api-scaffold
        self.security_config = self.load_security_config()  # From /security-scan
        self.db_config = self.load_db_config()  # From /db-migrate
        
    def create_test_suite(self):
        """Create integrated test suite for all generated components"""
        return {
            'api_tests': self.generate_api_tests(),
            'security_tests': self.generate_security_tests(),
            'db_tests': self.generate_db_tests(),
            'integration_tests': self.generate_integration_tests()
        }
    
    def generate_api_tests(self):
        """Generate tests for API scaffold output"""
        endpoints = self.api_config.get('endpoints', [])
        return [
            self.create_endpoint_test(endpoint) 
            for endpoint in endpoints
        ]
    
    def generate_security_tests(self):
        """Generate tests based on security scan configuration"""
        return {
            'auth_tests': self.create_auth_tests(),
            'input_validation_tests': self.create_validation_tests(),
            'rate_limiting_tests': self.create_rate_limit_tests()
        }
```

**Database + Testing Integration**
```python
# conftest.py - Database test configuration
import pytest
from src.database import get_db_connection

@pytest.fixture(scope="session")
def db_migration_config():
    """Load database configuration from /db-migrate"""
    return {
        'source_db': 'postgresql://test:test@localhost:5432/source_test',
        'target_db': 'postgresql://test:test@localhost:5432/target_test',
        'migration_scripts': './migrations/test/',
        'test_data': './fixtures/test_data.sql'
    }

@pytest.fixture
def test_database(db_migration_config):
    """Setup test database with migrations"""
    # Apply migrations from /db-migrate output
    apply_test_migrations(db_migration_config['migration_scripts'])
    
    # Load test data
    load_test_fixtures(db_migration_config['test_data'])
    
    yield get_db_connection(db_migration_config['target_db'])
    
    # Cleanup
    teardown_test_database()
```

**Frontend + Backend Integration Testing**
```javascript
// Integration test configuration
// tests/integration/fullstack.test.js
import { setupTestEnvironment } from './utils/testSetup';

describe('Full Stack Integration', () => {
  beforeAll(async () => {
    // Start backend from /api-scaffold
    await startTestBackend({
      config: require('../../backend/test.config.json')
    });
    
    // Start frontend from /frontend-optimize
    await startTestFrontend({
      mode: 'test',
      apiUrl: 'http://localhost:8000/api/v1'
    });
  });
  
  test('complete user journey', async () => {
    // Test generated by combining frontend and backend tests
    const userFlow = await setupUserFlow();
    
    // API tests from backend
    const apiResponse = await userFlow.createUser(testUserData);
    expect(apiResponse.status).toBe(201);
    
    // Frontend tests
    await userFlow.navigateToLogin();
    await userFlow.login(testUserData.email, testUserData.password);
    await userFlow.verifyDashboard();
  });
});
```

**Kubernetes + Testing Integration**
```yaml
# Generated K8s test configuration
# k8s-test-environment.yaml (integrates with /k8s-manifest output)
apiVersion: v1
kind: ConfigMap
metadata:
  name: test-config
data:
  test-database-url: "postgresql://test:test@postgres-test:5432/testdb"
  test-redis-url: "redis://redis-test:6379/0"
  test-api-url: "http://api-test:8000"

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: test-runner
spec:
  template:
    spec:
      containers:
      - name: test-runner
        image: test-runner:latest
        env:
        - name: TEST_ENVIRONMENT
          value: "k8s"
        - name: API_URL
          valueFrom:
            configMapKeyRef:
              name: test-config
              key: test-api-url
        command: ["pytest", "tests/", "-v", "--k8s-integration"]
```

**CI/CD Integration Example**
```yaml
# .github/workflows/integrated-testing.yml
name: Integrated Testing Pipeline

on:
  pull_request:
    branches: [main]

jobs:
  setup-and-test:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4
    
    # 1. Build application using /docker-optimize
    - name: Build optimized containers
      run: |
        docker build -f Dockerfile.optimized -t app:test .
        docker build -f Dockerfile.test -t test-runner:latest .
    
    # 2. Setup infrastructure using /k8s-manifest
    - name: Setup test cluster
      run: |
        kind create cluster --config k8s-test-cluster.yaml
        kubectl apply -f k8s-test-manifests/
    
    # 3. Run database migrations using /db-migrate
    - name: Run test migrations
      run: |
        kubectl run migration-job --image=migrator:latest \
          --env="DATABASE_URL=${{ secrets.TEST_DB_URL }}"
    
    # 4. Execute comprehensive test suite
    - name: Run test harness
      run: |
        kubectl run test-job --image=test-runner:latest \
          --env="TEST_SUITE=full" \
          --env="COVERAGE_THRESHOLD=90"
    
    # 5. Security scanning of test results
    - name: Security scan test artifacts
      run: |
        /security-scan test-results/ --format junit
    
    # 6. Performance baseline validation
    - name: Performance regression testing
      run: |
        kubectl run performance-test --image=test-runner:latest \
          --env="TEST_SUITE=performance" \
          --env="BASELINE_FILE=performance-baseline.json"
```

**Configuration Sharing Between Commands**
```json
// shared-config.json - Used across all commands
{
  "project": {
    "name": "microservice-api",
    "type": "backend",
    "framework": "fastapi",
    "database": "postgresql",
    "cache": "redis"
  },
  "testing": {
    "framework": "pytest",
    "coverage_threshold": 90,
    "test_types": ["unit", "integration", "e2e", "performance", "security"],
    "parallel_execution": true,
    "test_data_strategy": "factories"
  },
  "security": {
    "auth_method": "jwt",
    "password_hashing": "bcrypt",
    "rate_limiting": true,
    "input_validation": "pydantic"
  },
  "deployment": {
    "platform": "kubernetes",
    "environment": "production",
    "scaling": "horizontal",
    "monitoring": "prometheus"
  }
}
```

**Shared Test Utilities**
```python
# tests/utils/integration_helpers.py
class CrossCommandTestHelper:
    """Helper for running tests across command outputs"""
    
    def __init__(self, config_path="shared-config.json"):
        self.config = self.load_shared_config(config_path)
        self.api_client = self.setup_api_client()
        self.db_client = self.setup_db_client()
        
    def test_api_database_integration(self):
        """Test API + Database integration"""
        # Create test data via API (from /api-scaffold)
        user_data = self.create_test_user_via_api()
        
        # Verify in database (using /db-migrate schema)
        db_user = self.get_user_from_db(user_data['id'])
        assert db_user is not None
        
        # Test API retrieval
        api_user = self.get_user_via_api(user_data['id'])
        assert api_user['email'] == db_user.email
    
    def test_security_compliance(self):
        """Test security compliance across all components"""
        # Run security tests from /security-scan config
        security_results = self.run_security_tests()
        
        # Validate API security
        auth_tests = self.run_auth_tests()
        
        # Validate database security
        db_security = self.check_database_security()
        
        return {
            'security_scan': security_results,
            'auth_tests': auth_tests,
            'database_security': db_security
        }
    
    def test_performance_benchmarks(self):
        """Test performance across all components"""
        return {
            'api_performance': self.benchmark_api_endpoints(),
            'database_performance': self.benchmark_db_queries(),
            'integration_performance': self.benchmark_full_workflow()
        }
```

**End-to-End Workflow Example**
```python
# Complete workflow test
@pytest.mark.integration
@pytest.mark.slow
def test_complete_development_workflow():
    """Test the complete output from all slash commands working together"""
    
    # 1. API + Database Integration
    api_client = get_api_client()  # From /api-scaffold
    db_client = get_db_client()    # From /db-migrate
    
    # Create user via API
    user_response = api_client.post('/users/', json=test_user_data)
    assert user_response.status_code == 201
    
    # Verify in database
    user_id = user_response.json()['id']
    db_user = db_client.get_user(user_id)
    assert db_user is not None
    
    # 2. Security Validation
    # Test JWT token from API
    token = authenticate_user(test_user_data['email'], test_user_data['password'])
    assert validate_jwt_token(token)
    
    # Test rate limiting (from /security-scan)
    assert test_rate_limiting(api_client)
    
    # 3. Performance Validation
    # Test API performance
    api_metrics = benchmark_api_endpoint('/users/', method='POST')
    assert api_metrics['avg_response_time'] < 200  # ms
    
    # 4. Container Integration
    # Test Docker container from /docker-optimize
    container_health = check_container_health('app:latest')
    assert container_health['status'] == 'healthy'
    
    # 5. Kubernetes Integration
    # Test K8s deployment from /k8s-manifest
    k8s_status = check_k8s_deployment_status('api-deployment')
    assert k8s_status['ready_replicas'] > 0
```

This integration approach ensures all generated code works together seamlessly and provides comprehensive validation across the entire application stack.

## Validation Checklist

- [ ] Testing framework selected based on technology stack
- [ ] Unit tests cover core business logic
- [ ] Integration tests validate component interactions
- [ ] End-to-end tests verify user workflows
- [ ] Performance tests establish baselines
- [ ] Security tests validate security controls
- [ ] Property-based tests explore edge cases
- [ ] Contract tests ensure API compatibility
- [ ] Mutation tests validate test quality
- [ ] CI/CD pipeline includes all test types
- [ ] Test coverage meets minimum thresholds
- [ ] Test data management strategy implemented
- [ ] Test environment properly configured

Focus on creating a comprehensive testing strategy that ensures code quality, performance, and security while maintaining fast feedback loops and reliable test execution.
````

## File: workflows/data-driven-feature.md

````markdown
Build data-driven features with integrated pipelines and ML capabilities using specialized agents:

[Extended thinking: This workflow orchestrates data scientists, data engineers, backend architects, and AI engineers to build features that leverage data pipelines, analytics, and machine learning. Each agent contributes their expertise to create a complete data-driven solution.]

## Phase 1: Data Analysis and Design

### 1. Data Requirements Analysis
- Use Task tool with subagent_type="data-scientist"
- Prompt: "Analyze data requirements for: $ARGUMENTS. Identify data sources, required transformations, analytics needs, and potential ML opportunities."
- Output: Data analysis report, feature engineering requirements, ML feasibility

### 2. Data Pipeline Architecture
- Use Task tool with subagent_type="data-engineer"
- Prompt: "Design data pipeline architecture for: $ARGUMENTS. Include ETL/ELT processes, data storage, streaming requirements, and integration with existing systems based on data scientist's analysis."
- Output: Pipeline architecture, technology stack, data flow diagrams

## Phase 2: Backend Integration

### 3. API and Service Design
- Use Task tool with subagent_type="backend-architect"
- Prompt: "Design backend services to support data-driven feature: $ARGUMENTS. Include APIs for data ingestion, analytics endpoints, and ML model serving based on pipeline architecture."
- Output: Service architecture, API contracts, integration patterns

### 4. Database and Storage Design
- Use Task tool with subagent_type="database-optimizer"
- Prompt: "Design optimal database schema and storage strategy for: $ARGUMENTS. Consider both transactional and analytical workloads, time-series data, and ML feature stores."
- Output: Database schemas, indexing strategies, storage recommendations

## Phase 3: ML and AI Implementation

### 5. ML Pipeline Development
- Use Task tool with subagent_type="ml-engineer"
- Prompt: "Implement ML pipeline for: $ARGUMENTS. Include feature engineering, model training, validation, and deployment based on data scientist's requirements."
- Output: ML pipeline code, model artifacts, deployment strategy

### 6. AI Integration
- Use Task tool with subagent_type="ai-engineer"
- Prompt: "Build AI-powered features for: $ARGUMENTS. Integrate LLMs, implement RAG if needed, and create intelligent automation based on ML engineer's models."
- Output: AI integration code, prompt engineering, RAG implementation

## Phase 4: Implementation and Optimization

### 7. Data Pipeline Implementation
- Use Task tool with subagent_type="data-engineer"
- Prompt: "Implement production data pipelines for: $ARGUMENTS. Include real-time streaming, batch processing, and data quality monitoring based on all previous designs."
- Output: Pipeline implementation, monitoring setup, data quality checks

### 8. Performance Optimization
- Use Task tool with subagent_type="performance-engineer"
- Prompt: "Optimize data processing and model serving performance for: $ARGUMENTS. Focus on query optimization, caching strategies, and model inference speed."
- Output: Performance improvements, caching layers, optimization report

## Phase 5: Testing and Deployment

### 9. Comprehensive Testing
- Use Task tool with subagent_type="test-automator"
- Prompt: "Create test suites for data pipelines and ML components: $ARGUMENTS. Include data validation tests, model performance tests, and integration tests."
- Output: Test suites, data quality tests, ML monitoring tests

### 10. Production Deployment
- Use Task tool with subagent_type="deployment-engineer"
- Prompt: "Deploy data-driven feature to production: $ARGUMENTS. Include pipeline orchestration, model deployment, monitoring, and rollback strategies."
- Output: Deployment configurations, monitoring dashboards, operational runbooks

## Coordination Notes
- Data flow and requirements cascade from data scientists to engineers
- ML models must integrate seamlessly with backend services
- Performance considerations apply to both data processing and model serving
- Maintain data lineage and versioning throughout the pipeline

Data-driven feature to build: $ARGUMENTS
````

## File: workflows/feature-development.md

````markdown
Implement a new feature using specialized agents with explicit Task tool invocations:

[Extended thinking: This workflow orchestrates multiple specialized agents to implement a complete feature from design to deployment. Each agent receives context from previous agents to ensure coherent implementation.]

Use the Task tool to delegate to specialized agents in sequence:

1. **Backend Architecture Design**
   - Use Task tool with subagent_type="backend-architect" 
   - Prompt: "Design RESTful API and data model for: $ARGUMENTS. Include endpoint definitions, database schema, and service boundaries."
   - Save the API design and schema for next agents

2. **Frontend Implementation**
   - Use Task tool with subagent_type="frontend-developer"
   - Prompt: "Create UI components for: $ARGUMENTS. Use the API design from backend-architect: [include API endpoints and data models from step 1]"
   - Ensure UI matches the backend API contract

3. **Test Coverage**
   - Use Task tool with subagent_type="test-automator"
   - Prompt: "Write comprehensive tests for: $ARGUMENTS. Cover both backend API endpoints: [from step 1] and frontend components: [from step 2]"
   - Include unit, integration, and e2e tests

4. **Production Deployment**
   - Use Task tool with subagent_type="deployment-engineer"
   - Prompt: "Prepare production deployment for: $ARGUMENTS. Include CI/CD pipeline, containerization, and monitoring for the implemented feature."
   - Ensure all components from previous steps are deployment-ready

Aggregate results from all agents and present a unified implementation plan.

Feature description: $ARGUMENTS
````

## File: workflows/full-review.md

````markdown
Perform a comprehensive review using multiple specialized agents with explicit Task tool invocations:

[Extended thinking: This workflow performs a thorough multi-perspective review by orchestrating specialized review agents. Each agent examines different aspects and the results are consolidated into a unified action plan.]

Execute parallel reviews using Task tool with specialized agents:

## 1. Code Quality Review
- Use Task tool with subagent_type="code-reviewer"
- Prompt: "Review code quality and maintainability for: $ARGUMENTS. Check for code smells, readability, documentation, and adherence to best practices."
- Focus: Clean code principles, SOLID, DRY, naming conventions

## 2. Security Audit
- Use Task tool with subagent_type="security-auditor"
- Prompt: "Perform security audit on: $ARGUMENTS. Check for vulnerabilities, OWASP compliance, authentication issues, and data protection."
- Focus: Injection risks, authentication, authorization, data encryption

## 3. Architecture Review
- Use Task tool with subagent_type="architect-reviewer"
- Prompt: "Review architectural design and patterns in: $ARGUMENTS. Evaluate scalability, maintainability, and adherence to architectural principles."
- Focus: Service boundaries, coupling, cohesion, design patterns

## 4. Performance Analysis
- Use Task tool with subagent_type="performance-engineer"
- Prompt: "Analyze performance characteristics of: $ARGUMENTS. Identify bottlenecks, resource usage, and optimization opportunities."
- Focus: Response times, memory usage, database queries, caching

## 5. Test Coverage Assessment
- Use Task tool with subagent_type="test-automator"
- Prompt: "Evaluate test coverage and quality for: $ARGUMENTS. Assess unit tests, integration tests, and identify gaps in test coverage."
- Focus: Coverage metrics, test quality, edge cases, test maintainability

## Consolidated Report Structure
Compile all feedback into a unified report:
- **Critical Issues** (must fix): Security vulnerabilities, broken functionality, architectural flaws
- **Recommendations** (should fix): Performance bottlenecks, code quality issues, missing tests
- **Suggestions** (nice to have): Refactoring opportunities, documentation improvements
- **Positive Feedback** (what's done well): Good practices to maintain and replicate

Target: $ARGUMENTS
````

## File: workflows/full-stack-feature.md

````markdown
Implement a full-stack feature across multiple platforms with coordinated agent orchestration:

[Extended thinking: This workflow orchestrates a comprehensive feature implementation across backend, frontend, mobile, and API layers. Each agent builds upon the work of previous agents to create a cohesive multi-platform solution.]

## Phase 1: Architecture and API Design

### 1. Backend Architecture
- Use Task tool with subagent_type="backend-architect"
- Prompt: "Design backend architecture for: $ARGUMENTS. Include service boundaries, data models, and technology recommendations."
- Output: Service architecture, database schema, API structure

### 2. GraphQL API Design (if applicable)
- Use Task tool with subagent_type="graphql-architect"
- Prompt: "Design GraphQL schema and resolvers for: $ARGUMENTS. Build on the backend architecture from previous step. Include types, queries, mutations, and subscriptions."
- Output: GraphQL schema, resolver structure, federation strategy

## Phase 2: Implementation

### 3. Frontend Development
- Use Task tool with subagent_type="frontend-developer"
- Prompt: "Implement web frontend for: $ARGUMENTS. Use the API design from previous steps. Include responsive UI, state management, and API integration."
- Output: React/Vue/Angular components, state management, API client

### 4. Mobile Development
- Use Task tool with subagent_type="mobile-developer"
- Prompt: "Implement mobile app features for: $ARGUMENTS. Ensure consistency with web frontend and use the same API. Include offline support and native integrations."
- Output: React Native/Flutter implementation, offline sync, push notifications

## Phase 3: Quality Assurance

### 5. Comprehensive Testing
- Use Task tool with subagent_type="test-automator"
- Prompt: "Create test suites for: $ARGUMENTS. Cover backend APIs, frontend components, mobile app features, and integration tests across all platforms."
- Output: Unit tests, integration tests, e2e tests, test documentation

### 6. Security Review
- Use Task tool with subagent_type="security-auditor"
- Prompt: "Audit security across all implementations for: $ARGUMENTS. Check API security, frontend vulnerabilities, and mobile app security."
- Output: Security report, remediation steps

## Phase 4: Optimization and Deployment

### 7. Performance Optimization
- Use Task tool with subagent_type="performance-engineer"
- Prompt: "Optimize performance across all platforms for: $ARGUMENTS. Focus on API response times, frontend bundle size, and mobile app performance."
- Output: Performance improvements, caching strategies, optimization report

### 8. Deployment Preparation
- Use Task tool with subagent_type="deployment-engineer"
- Prompt: "Prepare deployment for all components of: $ARGUMENTS. Include CI/CD pipelines, containerization, and monitoring setup."
- Output: Deployment configurations, monitoring setup, rollout strategy

## Coordination Notes
- Each agent receives outputs from previous agents
- Maintain consistency across all platforms
- Ensure API contracts are honored by all clients
- Document integration points between components

Feature to implement: $ARGUMENTS
````

## File: workflows/git-workflow.md

````markdown
Complete Git workflow using specialized agents:

1. code-reviewer: Review uncommitted changes
2. test-automator: Ensure tests pass
3. deployment-engineer: Verify deployment readiness
4. Create commit message following conventions
5. Push and create PR with proper description

Target branch: $ARGUMENTS
````

## File: workflows/improve-agent.md

````markdown
Improve an existing agent based on recent performance:

1. Analyze recent uses of: $ARGUMENTS
2. Identify patterns in:
   - Failed tasks
   - User corrections
   - Suboptimal outputs
3. Update the agent's prompt with:
   - New examples
   - Clarified instructions
   - Additional constraints
4. Test on recent scenarios
5. Save improved version
````

## File: workflows/incident-response.md

````markdown
Respond to production incidents with coordinated agent expertise for rapid resolution:

[Extended thinking: This workflow handles production incidents with urgency and precision. Multiple specialized agents work together to identify root causes, implement fixes, and prevent recurrence.]

## Phase 1: Immediate Response

### 1. Incident Assessment
- Use Task tool with subagent_type="incident-responder"
- Prompt: "URGENT: Assess production incident: $ARGUMENTS. Determine severity, impact, and immediate mitigation steps. Time is critical."
- Output: Incident severity, impact assessment, immediate actions

### 2. Initial Troubleshooting
- Use Task tool with subagent_type="devops-troubleshooter"
- Prompt: "Investigate production issue: $ARGUMENTS. Check logs, metrics, recent deployments, and system health. Identify potential root causes."
- Output: Initial findings, suspicious patterns, potential causes

## Phase 2: Root Cause Analysis

### 3. Deep Debugging
- Use Task tool with subagent_type="debugger"
- Prompt: "Debug production issue: $ARGUMENTS using findings from initial investigation. Analyze stack traces, reproduce issue if possible, identify exact root cause."
- Output: Root cause identification, reproduction steps, debug analysis

### 4. Performance Analysis (if applicable)
- Use Task tool with subagent_type="performance-engineer"
- Prompt: "Analyze performance aspects of incident: $ARGUMENTS. Check for resource exhaustion, bottlenecks, or performance degradation."
- Output: Performance metrics, resource analysis, bottleneck identification

### 5. Database Investigation (if applicable)
- Use Task tool with subagent_type="database-optimizer"
- Prompt: "Investigate database-related aspects of incident: $ARGUMENTS. Check for locks, slow queries, connection issues, or data corruption."
- Output: Database health report, query analysis, data integrity check

## Phase 3: Resolution Implementation

### 6. Fix Development
- Use Task tool with subagent_type="backend-architect"
- Prompt: "Design and implement fix for incident: $ARGUMENTS based on root cause analysis. Ensure fix is safe for immediate production deployment."
- Output: Fix implementation, safety analysis, rollout strategy

### 7. Emergency Deployment
- Use Task tool with subagent_type="deployment-engineer"
- Prompt: "Deploy emergency fix for incident: $ARGUMENTS. Implement with minimal risk, include rollback plan, and monitor deployment closely."
- Output: Deployment execution, rollback procedures, monitoring setup

## Phase 4: Stabilization and Prevention

### 8. System Stabilization
- Use Task tool with subagent_type="devops-troubleshooter"
- Prompt: "Stabilize system after incident fix: $ARGUMENTS. Monitor system health, clear any backlogs, and ensure full recovery."
- Output: System health report, recovery metrics, stability confirmation

### 9. Security Review (if applicable)
- Use Task tool with subagent_type="security-auditor"
- Prompt: "Review security implications of incident: $ARGUMENTS. Check for any security breaches, data exposure, or vulnerabilities exploited."
- Output: Security assessment, breach analysis, hardening recommendations

## Phase 5: Post-Incident Activities

### 10. Monitoring Enhancement
- Use Task tool with subagent_type="devops-troubleshooter"
- Prompt: "Enhance monitoring to prevent recurrence of: $ARGUMENTS. Add alerts, improve observability, and set up early warning systems."
- Output: New monitoring rules, alert configurations, observability improvements

### 11. Test Coverage
- Use Task tool with subagent_type="test-automator"
- Prompt: "Create tests to prevent regression of incident: $ARGUMENTS. Include unit tests, integration tests, and chaos engineering scenarios."
- Output: Test implementations, regression prevention, chaos tests

### 12. Documentation
- Use Task tool with subagent_type="incident-responder"
- Prompt: "Document incident postmortem for: $ARGUMENTS. Include timeline, root cause, impact, resolution, and lessons learned. No blame, focus on improvement."
- Output: Postmortem document, action items, process improvements

## Coordination Notes
- Speed is critical in early phases - parallel agent execution where possible
- Communication between agents must be clear and rapid
- All changes must be safe and reversible
- Document everything for postmortem analysis

Production incident: $ARGUMENTS
````

## File: workflows/legacy-modernize.md

````markdown
Modernize legacy code using expert agents:

1. legacy-modernizer: Analyze and plan modernization
2. test-automator: Create tests for legacy code
3. code-reviewer: Review modernization plan
4. python-pro/golang-pro: Implement modernization
5. security-auditor: Verify security improvements
6. performance-engineer: Validate performance

Target: $ARGUMENTS
````

## File: workflows/ml-pipeline.md

````markdown
# Machine Learning Pipeline

Design and implement a complete ML pipeline for: $ARGUMENTS

Create a production-ready pipeline including:

1. **Data Ingestion**:
   - Multiple data source connectors
   - Schema validation with Pydantic
   - Data versioning strategy
   - Incremental loading capabilities

2. **Feature Engineering**:
   - Feature transformation pipeline
   - Feature store integration
   - Statistical validation
   - Handling missing data and outliers

3. **Model Training**:
   - Experiment tracking (MLflow/W&B)
   - Hyperparameter optimization
   - Cross-validation strategy
   - Model versioning

4. **Model Evaluation**:
   - Comprehensive metrics
   - A/B testing framework
   - Bias detection
   - Performance monitoring

5. **Deployment**:
   - Model serving API
   - Batch/stream prediction
   - Model registry
   - Rollback capabilities

6. **Monitoring**:
   - Data drift detection
   - Model performance tracking
   - Alert system
   - Retraining triggers

Include error handling, logging, and make it cloud-agnostic. Use modern tools like DVC, MLflow, or similar. Ensure reproducibility and scalability.
````

## File: workflows/multi-platform.md

````markdown
Build the same feature across multiple platforms:

Run in parallel:
- frontend-developer: Web implementation
- mobile-developer: Mobile app implementation
- api-documenter: API documentation

Ensure consistency across all platforms.

Feature specification: $ARGUMENTS
````

## File: workflows/performance-optimization.md

````markdown
Optimize application performance end-to-end using specialized performance and optimization agents:

[Extended thinking: This workflow coordinates multiple agents to identify and fix performance bottlenecks across the entire stack. From database queries to frontend rendering, each agent contributes their expertise to create a highly optimized application.]

## Phase 1: Performance Analysis

### 1. Application Profiling
- Use Task tool with subagent_type="performance-engineer"
- Prompt: "Profile application performance for: $ARGUMENTS. Identify CPU, memory, and I/O bottlenecks. Include flame graphs, memory profiles, and resource utilization metrics."
- Output: Performance profile, bottleneck analysis, optimization priorities

### 2. Database Performance Analysis
- Use Task tool with subagent_type="database-optimizer"
- Prompt: "Analyze database performance for: $ARGUMENTS. Review query execution plans, identify slow queries, check indexing, and analyze connection pooling."
- Output: Query optimization report, index recommendations, schema improvements

## Phase 2: Backend Optimization

### 3. Backend Code Optimization
- Use Task tool with subagent_type="performance-engineer"
- Prompt: "Optimize backend code for: $ARGUMENTS based on profiling results. Focus on algorithm efficiency, caching strategies, and async operations."
- Output: Optimized code, caching implementation, performance improvements

### 4. API Optimization
- Use Task tool with subagent_type="backend-architect"
- Prompt: "Optimize API design and implementation for: $ARGUMENTS. Consider pagination, response compression, field filtering, and batch operations."
- Output: Optimized API endpoints, GraphQL query optimization, response time improvements

## Phase 3: Frontend Optimization

### 5. Frontend Performance
- Use Task tool with subagent_type="frontend-developer"
- Prompt: "Optimize frontend performance for: $ARGUMENTS. Focus on bundle size, lazy loading, code splitting, and rendering performance. Implement Core Web Vitals improvements."
- Output: Optimized bundles, lazy loading implementation, performance metrics

### 6. Mobile App Optimization
- Use Task tool with subagent_type="mobile-developer"
- Prompt: "Optimize mobile app performance for: $ARGUMENTS. Focus on startup time, memory usage, battery efficiency, and offline performance."
- Output: Optimized mobile code, reduced app size, improved battery life

## Phase 4: Infrastructure Optimization

### 7. Cloud Infrastructure Optimization
- Use Task tool with subagent_type="cloud-architect"
- Prompt: "Optimize cloud infrastructure for: $ARGUMENTS. Review auto-scaling, instance types, CDN usage, and geographic distribution."
- Output: Infrastructure improvements, cost optimization, scaling strategy

### 8. Deployment Optimization
- Use Task tool with subagent_type="deployment-engineer"
- Prompt: "Optimize deployment and build processes for: $ARGUMENTS. Improve CI/CD performance, implement caching, and optimize container images."
- Output: Faster builds, optimized containers, improved deployment times

## Phase 5: Monitoring and Validation

### 9. Performance Monitoring Setup
- Use Task tool with subagent_type="devops-troubleshooter"
- Prompt: "Set up comprehensive performance monitoring for: $ARGUMENTS. Include APM, real user monitoring, and custom performance metrics."
- Output: Monitoring dashboards, alert thresholds, SLO definitions

### 10. Performance Testing
- Use Task tool with subagent_type="test-automator"
- Prompt: "Create performance test suites for: $ARGUMENTS. Include load tests, stress tests, and performance regression tests."
- Output: Performance test suite, benchmark results, regression prevention

## Coordination Notes
- Performance metrics guide optimization priorities
- Each optimization must be validated with measurements
- Consider trade-offs between different performance aspects
- Document all optimizations and their impact

Performance optimization target: $ARGUMENTS
````

## File: workflows/security-hardening.md

````markdown
Implement security-first architecture and hardening measures with coordinated agent orchestration:

[Extended thinking: This workflow prioritizes security at every layer of the application stack. Multiple agents work together to identify vulnerabilities, implement secure patterns, and ensure compliance with security best practices.]

## Phase 1: Security Assessment

### 1. Initial Security Audit
- Use Task tool with subagent_type="security-auditor"
- Prompt: "Perform comprehensive security audit on: $ARGUMENTS. Identify vulnerabilities, compliance gaps, and security risks across all components."
- Output: Vulnerability report, risk assessment, compliance gaps

### 2. Architecture Security Review
- Use Task tool with subagent_type="backend-architect"
- Prompt: "Review and redesign architecture for security: $ARGUMENTS. Focus on secure service boundaries, data isolation, and defense in depth. Use findings from security audit."
- Output: Secure architecture design, service isolation strategy, data flow diagrams

## Phase 2: Security Implementation

### 3. Backend Security Hardening
- Use Task tool with subagent_type="backend-architect"
- Prompt: "Implement backend security measures for: $ARGUMENTS. Include authentication, authorization, input validation, and secure data handling based on security audit findings."
- Output: Secure API implementations, auth middleware, validation layers

### 4. Infrastructure Security
- Use Task tool with subagent_type="devops-troubleshooter"
- Prompt: "Implement infrastructure security for: $ARGUMENTS. Configure firewalls, secure secrets management, implement least privilege access, and set up security monitoring."
- Output: Infrastructure security configs, secrets management, monitoring setup

### 5. Frontend Security
- Use Task tool with subagent_type="frontend-developer"
- Prompt: "Implement frontend security measures for: $ARGUMENTS. Include CSP headers, XSS prevention, secure authentication flows, and sensitive data handling."
- Output: Secure frontend code, CSP policies, auth integration

## Phase 3: Compliance and Testing

### 6. Compliance Verification
- Use Task tool with subagent_type="security-auditor"
- Prompt: "Verify compliance with security standards for: $ARGUMENTS. Check OWASP Top 10, GDPR, SOC2, or other relevant standards. Validate all security implementations."
- Output: Compliance report, remediation requirements

### 7. Security Testing
- Use Task tool with subagent_type="test-automator"
- Prompt: "Create security test suites for: $ARGUMENTS. Include penetration tests, security regression tests, and automated vulnerability scanning."
- Output: Security test suite, penetration test results, CI/CD integration

## Phase 4: Deployment and Monitoring

### 8. Secure Deployment
- Use Task tool with subagent_type="deployment-engineer"
- Prompt: "Implement secure deployment pipeline for: $ARGUMENTS. Include security gates, vulnerability scanning in CI/CD, and secure configuration management."
- Output: Secure CI/CD pipeline, deployment security checks, rollback procedures

### 9. Security Monitoring Setup
- Use Task tool with subagent_type="devops-troubleshooter"
- Prompt: "Set up security monitoring and incident response for: $ARGUMENTS. Include intrusion detection, log analysis, and automated alerting."
- Output: Security monitoring dashboards, alert rules, incident response procedures

## Coordination Notes
- Security findings from each phase inform subsequent implementations
- All agents must prioritize security in their recommendations
- Regular security reviews between phases ensure nothing is missed
- Document all security decisions and trade-offs

Security hardening target: $ARGUMENTS
````

## File: workflows/smart-fix.md

````markdown
Intelligently fix the issue using automatic agent selection with explicit Task tool invocations:

[Extended thinking: This workflow analyzes the issue and automatically routes to the most appropriate specialist agent(s). Complex issues may require multiple agents working together.]

First, analyze the issue to categorize it, then use Task tool with the appropriate agent:

## Analysis Phase
Examine the issue: "$ARGUMENTS" to determine the problem domain.

## Agent Selection and Execution

### For Deployment/Infrastructure Issues
If the issue involves deployment failures, infrastructure problems, or DevOps concerns:
- Use Task tool with subagent_type="devops-troubleshooter"
- Prompt: "Debug and fix this deployment/infrastructure issue: $ARGUMENTS"

### For Code Errors and Bugs
If the issue involves application errors, exceptions, or functional bugs:
- Use Task tool with subagent_type="debugger"
- Prompt: "Analyze and fix this code error: $ARGUMENTS. Provide root cause analysis and solution."

### For Database Performance
If the issue involves slow queries, database bottlenecks, or data access patterns:
- Use Task tool with subagent_type="database-optimizer"
- Prompt: "Optimize database performance for: $ARGUMENTS. Include query analysis, indexing strategies, and schema improvements."

### For Application Performance
If the issue involves slow response times, high resource usage, or performance degradation:
- Use Task tool with subagent_type="performance-engineer"
- Prompt: "Profile and optimize application performance issue: $ARGUMENTS. Identify bottlenecks and provide optimization strategies."

### For Legacy Code Issues
If the issue involves outdated code, deprecated patterns, or technical debt:
- Use Task tool with subagent_type="legacy-modernizer"
- Prompt: "Modernize and fix legacy code issue: $ARGUMENTS. Provide migration path and updated implementation."

## Multi-Domain Coordination
For complex issues spanning multiple domains:
1. Use primary agent based on main symptom
2. Use secondary agents for related aspects
3. Coordinate fixes across all affected areas
4. Verify integration between different fixes

Issue: $ARGUMENTS
````

## File: workflows/workflow-automate.md

````markdown
# Workflow Automation

You are a workflow automation expert specializing in creating efficient CI/CD pipelines, GitHub Actions workflows, and automated development processes. Design and implement automation that reduces manual work, improves consistency, and accelerates delivery while maintaining quality and security.

## Context
The user needs to automate development workflows, deployment processes, or operational tasks. Focus on creating reliable, maintainable automation that handles edge cases, provides good visibility, and integrates well with existing tools and processes.

## Requirements
$ARGUMENTS

## Instructions

### 1. Workflow Analysis

Analyze existing processes and identify automation opportunities:

**Workflow Discovery Script**
```python
import os
import yaml
import json
from pathlib import Path
from typing import List, Dict, Any

class WorkflowAnalyzer:
    def analyze_project(self, project_path: str) -> Dict[str, Any]:
        """
        Analyze project to identify automation opportunities
        """
        analysis = {
            'current_workflows': self._find_existing_workflows(project_path),
            'manual_processes': self._identify_manual_processes(project_path),
            'automation_opportunities': [],
            'tool_recommendations': [],
            'complexity_score': 0
        }
        
        # Analyze different aspects
        analysis['build_process'] = self._analyze_build_process(project_path)
        analysis['test_process'] = self._analyze_test_process(project_path)
        analysis['deployment_process'] = self._analyze_deployment_process(project_path)
        analysis['code_quality'] = self._analyze_code_quality_checks(project_path)
        
        # Generate recommendations
        self._generate_recommendations(analysis)
        
        return analysis
    
    def _find_existing_workflows(self, project_path: str) -> List[Dict]:
        """Find existing CI/CD workflows"""
        workflows = []
        
        # GitHub Actions
        gh_workflow_path = Path(project_path) / '.github' / 'workflows'
        if gh_workflow_path.exists():
            for workflow_file in gh_workflow_path.glob('*.y*ml'):
                with open(workflow_file) as f:
                    workflow = yaml.safe_load(f)
                    workflows.append({
                        'type': 'github_actions',
                        'name': workflow.get('name', workflow_file.stem),
                        'file': str(workflow_file),
                        'triggers': list(workflow.get('on', {}).keys())
                    })
        
        # GitLab CI
        gitlab_ci = Path(project_path) / '.gitlab-ci.yml'
        if gitlab_ci.exists():
            with open(gitlab_ci) as f:
                config = yaml.safe_load(f)
                workflows.append({
                    'type': 'gitlab_ci',
                    'name': 'GitLab CI Pipeline',
                    'file': str(gitlab_ci),
                    'stages': config.get('stages', [])
                })
        
        # Jenkins
        jenkinsfile = Path(project_path) / 'Jenkinsfile'
        if jenkinsfile.exists():
            workflows.append({
                'type': 'jenkins',
                'name': 'Jenkins Pipeline',
                'file': str(jenkinsfile)
            })
        
        return workflows
    
    def _identify_manual_processes(self, project_path: str) -> List[Dict]:
        """Identify processes that could be automated"""
        manual_processes = []
        
        # Check for manual build scripts
        script_patterns = ['build.sh', 'deploy.sh', 'release.sh', 'test.sh']
        for pattern in script_patterns:
            scripts = Path(project_path).glob(f'**/{pattern}')
            for script in scripts:
                manual_processes.append({
                    'type': 'script',
                    'file': str(script),
                    'purpose': pattern.replace('.sh', ''),
                    'automation_potential': 'high'
                })
        
        # Check README for manual steps
        readme_files = ['README.md', 'README.rst', 'README.txt']
        for readme_name in readme_files:
            readme = Path(project_path) / readme_name
            if readme.exists():
                content = readme.read_text()
                if any(keyword in content.lower() for keyword in ['manually', 'by hand', 'steps to']):
                    manual_processes.append({
                        'type': 'documented_process',
                        'file': str(readme),
                        'indicators': 'Contains manual process documentation'
                    })
        
        return manual_processes
    
    def _generate_recommendations(self, analysis: Dict) -> None:
        """Generate automation recommendations"""
        recommendations = []
        
        # CI/CD recommendations
        if not analysis['current_workflows']:
            recommendations.append({
                'priority': 'high',
                'category': 'ci_cd',
                'recommendation': 'Implement CI/CD pipeline',
                'tools': ['GitHub Actions', 'GitLab CI', 'Jenkins'],
                'effort': 'medium'
            })
        
        # Build automation
        if analysis['build_process']['manual_steps']:
            recommendations.append({
                'priority': 'high',
                'category': 'build',
                'recommendation': 'Automate build process',
                'tools': ['Make', 'Gradle', 'npm scripts'],
                'effort': 'low'
            })
        
        # Test automation
        if not analysis['test_process']['automated_tests']:
            recommendations.append({
                'priority': 'high',
                'category': 'testing',
                'recommendation': 'Implement automated testing',
                'tools': ['Jest', 'Pytest', 'JUnit'],
                'effort': 'medium'
            })
        
        # Deployment automation
        if analysis['deployment_process']['manual_deployment']:
            recommendations.append({
                'priority': 'critical',
                'category': 'deployment',
                'recommendation': 'Automate deployment process',
                'tools': ['ArgoCD', 'Flux', 'Terraform'],
                'effort': 'high'
            })
        
        analysis['automation_opportunities'] = recommendations
```

### 2. GitHub Actions Workflows

Create comprehensive GitHub Actions workflows:

**Multi-Environment CI/CD Pipeline**
```yaml
# .github/workflows/ci-cd.yml
name: CI/CD Pipeline

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]
  release:
    types: [created]

env:
  NODE_VERSION: '18'
  PYTHON_VERSION: '3.11'
  GO_VERSION: '1.21'

jobs:
  # Code quality checks
  quality:
    name: Code Quality
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Full history for better analysis

      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: Cache dependencies
        uses: actions/cache@v3
        with:
          path: |
            ~/.npm
            ~/.cache
            node_modules
          key: ${{ runner.os }}-node-${{ hashFiles('**/package-lock.json') }}
          restore-keys: |
            ${{ runner.os }}-node-

      - name: Install dependencies
        run: npm ci

      - name: Run linting
        run: |
          npm run lint
          npm run lint:styles

      - name: Type checking
        run: npm run typecheck

      - name: Security audit
        run: |
          npm audit --production
          npx snyk test

      - name: License check
        run: npx license-checker --production --onlyAllow 'MIT;Apache-2.0;BSD-3-Clause;BSD-2-Clause;ISC'

  # Testing
  test:
    name: Test Suite
    runs-on: ${{ matrix.os }}
    strategy:
      matrix:
        os: [ubuntu-latest, windows-latest, macos-latest]
        node: [16, 18, 20]
    steps:
      - uses: actions/checkout@v4

      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ matrix.node }}
          cache: 'npm'

      - name: Install dependencies
        run: npm ci

      - name: Run unit tests
        run: npm run test:unit -- --coverage

      - name: Run integration tests
        run: npm run test:integration
        env:
          TEST_DATABASE_URL: ${{ secrets.TEST_DATABASE_URL }}

      - name: Upload coverage
        if: matrix.os == 'ubuntu-latest' && matrix.node == 18
        uses: codecov/codecov-action@v3
        with:
          token: ${{ secrets.CODECOV_TOKEN }}
          flags: unittests
          name: codecov-umbrella

  # Build
  build:
    name: Build Application
    needs: [quality, test]
    runs-on: ubuntu-latest
    strategy:
      matrix:
        environment: [development, staging, production]
    steps:
      - uses: actions/checkout@v4

      - name: Set up build environment
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: Install dependencies
        run: npm ci

      - name: Build application
        run: npm run build
        env:
          NODE_ENV: ${{ matrix.environment }}
          BUILD_NUMBER: ${{ github.run_number }}
          COMMIT_SHA: ${{ github.sha }}

      - name: Build Docker image
        run: |
          docker build \
            --build-arg BUILD_DATE=$(date -u +'%Y-%m-%dT%H:%M:%SZ') \
            --build-arg VCS_REF=${GITHUB_SHA::8} \
            --build-arg VERSION=${GITHUB_REF#refs/tags/} \
            -t ${{ github.repository }}:${{ matrix.environment }}-${{ github.sha }} \
            -t ${{ github.repository }}:${{ matrix.environment }}-latest \
            .

      - name: Scan Docker image
        uses: aquasecurity/trivy-action@master
        with:
          image-ref: ${{ github.repository }}:${{ matrix.environment }}-${{ github.sha }}
          format: 'sarif'
          output: 'trivy-results.sarif'

      - name: Upload scan results
        uses: github/codeql-action/upload-sarif@v2
        with:
          sarif_file: 'trivy-results.sarif'

      - name: Push to registry
        if: github.event_name != 'pull_request'
        run: |
          echo ${{ secrets.DOCKER_PASSWORD }} | docker login -u ${{ secrets.DOCKER_USERNAME }} --password-stdin
          docker push ${{ github.repository }}:${{ matrix.environment }}-${{ github.sha }}
          docker push ${{ github.repository }}:${{ matrix.environment }}-latest

      - name: Upload artifacts
        uses: actions/upload-artifact@v3
        with:
          name: build-${{ matrix.environment }}
          path: |
            dist/
            build/
            .next/
          retention-days: 7

  # Deploy
  deploy:
    name: Deploy to ${{ matrix.environment }}
    needs: build
    runs-on: ubuntu-latest
    if: github.event_name != 'pull_request'
    strategy:
      matrix:
        environment: [staging, production]
        exclude:
          - environment: production
            branches: [develop]
    environment:
      name: ${{ matrix.environment }}
      url: ${{ steps.deploy.outputs.url }}
    steps:
      - uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: us-east-1

      - name: Deploy to ECS
        id: deploy
        run: |
          # Update task definition
          aws ecs register-task-definition \
            --family myapp-${{ matrix.environment }} \
            --container-definitions "[{
              \"name\": \"app\",
              \"image\": \"${{ github.repository }}:${{ matrix.environment }}-${{ github.sha }}\",
              \"environment\": [{
                \"name\": \"ENVIRONMENT\",
                \"value\": \"${{ matrix.environment }}\"
              }]
            }]"
          
          # Update service
          aws ecs update-service \
            --cluster ${{ matrix.environment }}-cluster \
            --service myapp-service \
            --task-definition myapp-${{ matrix.environment }}
          
          # Get service URL
          echo "url=https://${{ matrix.environment }}.example.com" >> $GITHUB_OUTPUT

      - name: Notify deployment
        uses: 8398a7/action-slack@v3
        with:
          status: ${{ job.status }}
          text: Deployment to ${{ matrix.environment }} ${{ job.status }}
          webhook_url: ${{ secrets.SLACK_WEBHOOK }}
        if: always()

  # Post-deployment verification
  verify:
    name: Verify Deployment
    needs: deploy
    runs-on: ubuntu-latest
    strategy:
      matrix:
        environment: [staging, production]
    steps:
      - uses: actions/checkout@v4

      - name: Run smoke tests
        run: |
          npm run test:smoke -- --url https://${{ matrix.environment }}.example.com

      - name: Run E2E tests
        uses: cypress-io/github-action@v5
        with:
          config: baseUrl=https://${{ matrix.environment }}.example.com
          record: true
        env:
          CYPRESS_RECORD_KEY: ${{ secrets.CYPRESS_RECORD_KEY }}

      - name: Performance test
        run: |
          npm install -g @sitespeed.io/sitespeed.io
          sitespeed.io https://${{ matrix.environment }}.example.com \
            --budget.configPath=.sitespeed.io/budget.json \
            --plugins.add=@sitespeed.io/plugin-lighthouse

      - name: Security scan
        run: |
          npm install -g @zaproxy/action-baseline
          zaproxy/action-baseline -t https://${{ matrix.environment }}.example.com
```

### 3. Release Automation

Automate release processes:

**Semantic Release Workflow**
```yaml
# .github/workflows/release.yml
name: Release

on:
  push:
    branches:
      - main

jobs:
  release:
    name: Create Release
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0
          persist-credentials: false

      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: 18

      - name: Install dependencies
        run: npm ci

      - name: Run semantic release
        env:
          GITHUB_TOKEN: ${{ secrets.SEMANTIC_RELEASE_TOKEN }}
          NPM_TOKEN: ${{ secrets.NPM_TOKEN }}
        run: npx semantic-release

      - name: Update documentation
        if: steps.semantic-release.outputs.new_release_published == 'true'
        run: |
          npm run docs:generate
          npm run docs:publish

      - name: Create release notes
        if: steps.semantic-release.outputs.new_release_published == 'true'
        uses: actions/github-script@v6
        with:
          script: |
            const { data: releases } = await github.rest.repos.listReleases({
              owner: context.repo.owner,
              repo: context.repo.repo,
              per_page: 1
            });
            
            const latestRelease = releases[0];
            const changelog = await generateChangelog(latestRelease);
            
            // Update release notes
            await github.rest.repos.updateRelease({
              owner: context.repo.owner,
              repo: context.repo.repo,
              release_id: latestRelease.id,
              body: changelog
            });
```

**Release Configuration**
```javascript
// .releaserc.js
module.exports = {
  branches: [
    'main',
    { name: 'beta', prerelease: true },
    { name: 'alpha', prerelease: true }
  ],
  plugins: [
    '@semantic-release/commit-analyzer',
    '@semantic-release/release-notes-generator',
    ['@semantic-release/changelog', {
      changelogFile: 'CHANGELOG.md'
    }],
    '@semantic-release/npm',
    ['@semantic-release/git', {
      assets: ['CHANGELOG.md', 'package.json'],
      message: 'chore(release): ${nextRelease.version} [skip ci]\n\n${nextRelease.notes}'
    }],
    '@semantic-release/github'
  ]
};
```

### 4. Development Workflow Automation

Automate common development tasks:

**Pre-commit Hooks**
```yaml
# .pre-commit-config.yaml
repos:
  - repo: https://github.com/pre-commit/pre-commit-hooks
    rev: v4.5.0
    hooks:
      - id: trailing-whitespace
      - id: end-of-file-fixer
      - id: check-yaml
      - id: check-added-large-files
        args: ['--maxkb=1000']
      - id: check-case-conflict
      - id: check-merge-conflict
      - id: detect-private-key

  - repo: https://github.com/psf/black
    rev: 23.10.0
    hooks:
      - id: black
        language_version: python3.11

  - repo: https://github.com/pycqa/isort
    rev: 5.12.0
    hooks:
      - id: isort
        args: ["--profile", "black"]

  - repo: https://github.com/pycqa/flake8
    rev: 6.1.0
    hooks:
      - id: flake8
        additional_dependencies: [flake8-docstrings]

  - repo: https://github.com/pre-commit/mirrors-eslint
    rev: v8.52.0
    hooks:
      - id: eslint
        files: \.[jt]sx?$
        types: [file]
        additional_dependencies:
          - eslint@8.52.0
          - eslint-config-prettier@9.0.0
          - eslint-plugin-react@7.33.2

  - repo: https://github.com/pre-commit/mirrors-prettier
    rev: v3.0.3
    hooks:
      - id: prettier
        types_or: [css, javascript, jsx, typescript, tsx, json, yaml]

  - repo: local
    hooks:
      - id: unit-tests
        name: Run unit tests
        entry: npm run test:unit -- --passWithNoTests
        language: system
        pass_filenames: false
        stages: [commit]
```

**Development Environment Setup**
```bash
#!/bin/bash
# scripts/setup-dev-environment.sh

set -euo pipefail

echo "üöÄ Setting up development environment..."

# Check prerequisites
check_prerequisites() {
    echo "Checking prerequisites..."
    
    commands=("git" "node" "npm" "docker" "docker-compose")
    for cmd in "${commands[@]}"; do
        if ! command -v "$cmd" &> /dev/null; then
            echo "‚ùå $cmd is not installed"
            exit 1
        fi
    done
    
    echo "‚úÖ All prerequisites installed"
}

# Install dependencies
install_dependencies() {
    echo "Installing dependencies..."
    npm ci
    
    # Install global tools
    npm install -g @commitlint/cli @commitlint/config-conventional
    npm install -g semantic-release
    
    # Install pre-commit
    pip install pre-commit
    pre-commit install
    pre-commit install --hook-type commit-msg
}

# Setup local services
setup_services() {
    echo "Setting up local services..."
    
    # Create docker network
    docker network create dev-network 2>/dev/null || true
    
    # Start services
    docker-compose -f docker-compose.dev.yml up -d
    
    # Wait for services
    echo "Waiting for services to be ready..."
    ./scripts/wait-for-services.sh
}

# Initialize database
initialize_database() {
    echo "Initializing database..."
    npm run db:migrate
    npm run db:seed
}

# Setup environment variables
setup_environment() {
    echo "Setting up environment variables..."
    
    if [ ! -f .env.local ]; then
        cp .env.example .env.local
        echo "‚úÖ Created .env.local from .env.example"
        echo "‚ö†Ô∏è  Please update .env.local with your values"
    fi
}

# Main execution
main() {
    check_prerequisites
    install_dependencies
    setup_services
    setup_environment
    initialize_database
    
    echo "‚úÖ Development environment setup complete!"
    echo ""
    echo "Next steps:"
    echo "1. Update .env.local with your configuration"
    echo "2. Run 'npm run dev' to start the development server"
    echo "3. Visit http://localhost:3000"
}

main
```

### 5. Infrastructure Automation

Automate infrastructure provisioning:

**Terraform Workflow**
```yaml
# .github/workflows/terraform.yml
name: Terraform

on:
  pull_request:
    paths:
      - 'terraform/**'
      - '.github/workflows/terraform.yml'
  push:
    branches:
      - main
    paths:
      - 'terraform/**'

env:
  TF_VERSION: '1.6.0'
  TF_VAR_project_name: ${{ github.event.repository.name }}

jobs:
  terraform:
    name: Terraform Plan & Apply
    runs-on: ubuntu-latest
    defaults:
      run:
        working-directory: terraform
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v2
        with:
          terraform_version: ${{ env.TF_VERSION }}
          terraform_wrapper: false
      
      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: us-east-1
      
      - name: Terraform Format Check
        run: terraform fmt -check -recursive
      
      - name: Terraform Init
        run: |
          terraform init \
            -backend-config="bucket=${{ secrets.TF_STATE_BUCKET }}" \
            -backend-config="key=${{ github.repository }}/terraform.tfstate" \
            -backend-config="region=us-east-1"
      
      - name: Terraform Validate
        run: terraform validate
      
      - name: Terraform Plan
        id: plan
        run: |
          terraform plan -out=tfplan -no-color | tee plan_output.txt
          
          # Extract plan summary
          echo "PLAN_SUMMARY<<EOF" >> $GITHUB_ENV
          grep -E '(Plan:|No changes.|# )' plan_output.txt >> $GITHUB_ENV
          echo "EOF" >> $GITHUB_ENV
      
      - name: Comment PR
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v6
        with:
          script: |
            const output = `#### Terraform Plan üìñ
            \`\`\`
            ${process.env.PLAN_SUMMARY}
            \`\`\`
            
            *Pushed by: @${{ github.actor }}, Action: \`${{ github.event_name }}\`*`;
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: output
            });
      
      - name: Terraform Apply
        if: github.ref == 'refs/heads/main' && github.event_name == 'push'
        run: terraform apply tfplan
```

### 6. Monitoring and Alerting Automation

Automate monitoring setup:

**Monitoring Stack Deployment**
```yaml
# .github/workflows/monitoring.yml
name: Deploy Monitoring

on:
  push:
    paths:
      - 'monitoring/**'
      - '.github/workflows/monitoring.yml'
    branches:
      - main

jobs:
  deploy-monitoring:
    name: Deploy Monitoring Stack
    runs-on: ubuntu-latest
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Setup Helm
        uses: azure/setup-helm@v3
        with:
          version: '3.12.0'
      
      - name: Configure Kubernetes
        run: |
          echo "${{ secrets.KUBE_CONFIG }}" | base64 -d > kubeconfig
          export KUBECONFIG=kubeconfig
      
      - name: Add Helm repositories
        run: |
          helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
          helm repo add grafana https://grafana.github.io/helm-charts
          helm repo update
      
      - name: Deploy Prometheus
        run: |
          helm upgrade --install prometheus prometheus-community/kube-prometheus-stack \
            --namespace monitoring \
            --create-namespace \
            --values monitoring/prometheus-values.yaml \
            --wait
      
      - name: Deploy Grafana Dashboards
        run: |
          kubectl apply -f monitoring/dashboards/
      
      - name: Deploy Alert Rules
        run: |
          kubectl apply -f monitoring/alerts/
      
      - name: Setup Alert Routing
        run: |
          helm upgrade --install alertmanager prometheus-community/alertmanager \
            --namespace monitoring \
            --values monitoring/alertmanager-values.yaml
```

### 7. Dependency Update Automation

Automate dependency updates:

**Renovate Configuration**
```json
{
  "extends": [
    "config:base",
    ":dependencyDashboard",
    ":semanticCommits",
    ":automergeDigest",
    ":automergeMinor"
  ],
  "schedule": ["after 10pm every weekday", "before 5am every weekday", "every weekend"],
  "timezone": "America/New_York",
  "vulnerabilityAlerts": {
    "labels": ["security"],
    "automerge": true
  },
  "packageRules": [
    {
      "matchDepTypes": ["devDependencies"],
      "automerge": true
    },
    {
      "matchPackagePatterns": ["^@types/"],
      "automerge": true
    },
    {
      "matchPackageNames": ["node"],
      "enabled": false
    },
    {
      "matchPackagePatterns": ["^eslint"],
      "groupName": "eslint packages",
      "automerge": true
    },
    {
      "matchManagers": ["docker"],
      "pinDigests": true
    }
  ],
  "postUpdateOptions": [
    "npmDedupe",
    "yarnDedupeHighest"
  ],
  "prConcurrentLimit": 3,
  "prCreation": "not-pending",
  "rebaseWhen": "behind-base-branch",
  "semanticCommitScope": "deps"
}
```

### 8. Documentation Automation

Automate documentation generation:

**Documentation Workflow**
```yaml
# .github/workflows/docs.yml
name: Documentation

on:
  push:
    branches: [main]
    paths:
      - 'src/**'
      - 'docs/**'
      - 'README.md'

jobs:
  generate-docs:
    name: Generate Documentation
    runs-on: ubuntu-latest
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: 18
      
      - name: Install dependencies
        run: npm ci
      
      - name: Generate API docs
        run: |
          npm run docs:api
          npm run docs:typescript
      
      - name: Generate architecture diagrams
        run: |
          npm install -g @mermaid-js/mermaid-cli
          mmdc -i docs/architecture.mmd -o docs/architecture.png
      
      - name: Build documentation site
        run: |
          npm run docs:build
      
      - name: Deploy to GitHub Pages
        uses: peaceiris/actions-gh-pages@v3
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          publish_dir: ./docs/dist
          cname: docs.example.com
```

**Documentation Generation Script**
```typescript
// scripts/generate-docs.ts
import { Application, TSConfigReader, TypeDocReader } from 'typedoc';
import { generateMarkdown } from './markdown-generator';
import { createApiReference } from './api-reference';

async function generateDocumentation() {
  // TypeDoc for TypeScript documentation
  const app = new Application();
  app.options.addReader(new TSConfigReader());
  app.options.addReader(new TypeDocReader());
  
  app.bootstrap({
    entryPoints: ['src/index.ts'],
    out: 'docs/api',
    theme: 'default',
    includeVersion: true,
    excludePrivate: true,
    readme: 'README.md',
    plugin: ['typedoc-plugin-markdown']
  });
  
  const project = app.convert();
  if (project) {
    await app.generateDocs(project, 'docs/api');
    
    // Generate custom markdown docs
    await generateMarkdown(project, {
      output: 'docs/guides',
      includeExamples: true,
      generateTOC: true
    });
    
    // Create API reference
    await createApiReference(project, {
      format: 'openapi',
      output: 'docs/openapi.json',
      includeSchemas: true
    });
  }
  
  // Generate architecture documentation
  await generateArchitectureDocs();
  
  // Generate deployment guides
  await generateDeploymentGuides();
}

async function generateArchitectureDocs() {
  const mermaidDiagrams = `
    graph TB
      A[Client] --> B[Load Balancer]
      B --> C[Web Server]
      C --> D[Application Server]
      D --> E[Database]
      D --> F[Cache]
      D --> G[Message Queue]
  `;
  
  // Save diagrams and generate documentation
  await fs.writeFile('docs/architecture.mmd', mermaidDiagrams);
}
```

### 9. Security Automation

Automate security scanning and compliance:

**Security Scanning Workflow**
```yaml
# .github/workflows/security.yml
name: Security Scan

on:
  push:
    branches: [main, develop]
  pull_request:
  schedule:
    - cron: '0 0 * * 0'  # Weekly on Sunday

jobs:
  security-scan:
    name: Security Scanning
    runs-on: ubuntu-latest
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Run Trivy vulnerability scanner
        uses: aquasecurity/trivy-action@master
        with:
          scan-type: 'fs'
          scan-ref: '.'
          format: 'sarif'
          output: 'trivy-results.sarif'
          severity: 'CRITICAL,HIGH'
      
      - name: Upload Trivy results
        uses: github/codeql-action/upload-sarif@v2
        with:
          sarif_file: 'trivy-results.sarif'
      
      - name: Run Snyk security scan
        uses: snyk/actions/node@master
        env:
          SNYK_TOKEN: ${{ secrets.SNYK_TOKEN }}
        with:
          args: --severity-threshold=high
      
      - name: Run OWASP Dependency Check
        uses: dependency-check/Dependency-Check_Action@main
        with:
          project: ${{ github.repository }}
          path: '.'
          format: 'ALL'
          args: >
            --enableRetired
            --enableExperimental
      
      - name: SonarCloud Scan
        uses: SonarSource/sonarcloud-github-action@master
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          SONAR_TOKEN: ${{ secrets.SONAR_TOKEN }}
      
      - name: Run Semgrep
        uses: returntocorp/semgrep-action@v1
        with:
          config: >-
            p/security-audit
            p/secrets
            p/owasp-top-ten
      
      - name: GitLeaks secret scanning
        uses: gitleaks/gitleaks-action@v2
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
```

### 10. Workflow Orchestration

Create complex workflow orchestration:

**Workflow Orchestrator**
```typescript
// workflow-orchestrator.ts
import { EventEmitter } from 'events';
import { Logger } from 'winston';

interface WorkflowStep {
  name: string;
  type: 'parallel' | 'sequential';
  steps?: WorkflowStep[];
  action?: () => Promise<any>;
  retries?: number;
  timeout?: number;
  condition?: () => boolean;
  onError?: 'fail' | 'continue' | 'retry';
}

export class WorkflowOrchestrator extends EventEmitter {
  constructor(
    private logger: Logger,
    private config: WorkflowConfig
  ) {
    super();
  }
  
  async execute(workflow: WorkflowStep): Promise<WorkflowResult> {
    const startTime = Date.now();
    const result: WorkflowResult = {
      success: true,
      steps: [],
      duration: 0
    };
    
    try {
      await this.executeStep(workflow, result);
    } catch (error) {
      result.success = false;
      result.error = error;
      this.emit('workflow:failed', result);
    }
    
    result.duration = Date.now() - startTime;
    this.emit('workflow:completed', result);
    
    return result;
  }
  
  private async executeStep(
    step: WorkflowStep,
    result: WorkflowResult,
    parentPath: string = ''
  ): Promise<void> {
    const stepPath = parentPath ? `${parentPath}.${step.name}` : step.name;
    
    this.emit('step:start', { step: stepPath });
    
    // Check condition
    if (step.condition && !step.condition()) {
      this.logger.info(`Skipping step ${stepPath} due to condition`);
      this.emit('step:skipped', { step: stepPath });
      return;
    }
    
    const stepResult: StepResult = {
      name: step.name,
      path: stepPath,
      startTime: Date.now(),
      success: true
    };
    
    try {
      if (step.action) {
        // Execute single action
        await this.executeAction(step, stepResult);
      } else if (step.steps) {
        // Execute sub-steps
        if (step.type === 'parallel') {
          await this.executeParallel(step.steps, result, stepPath);
        } else {
          await this.executeSequential(step.steps, result, stepPath);
        }
      }
      
      stepResult.endTime = Date.now();
      stepResult.duration = stepResult.endTime - stepResult.startTime;
      result.steps.push(stepResult);
      
      this.emit('step:complete', { step: stepPath, result: stepResult });
    } catch (error) {
      stepResult.success = false;
      stepResult.error = error;
      result.steps.push(stepResult);
      
      this.emit('step:failed', { step: stepPath, error });
      
      if (step.onError === 'fail') {
        throw error;
      }
    }
  }
  
  private async executeAction(
    step: WorkflowStep,
    stepResult: StepResult
  ): Promise<void> {
    const timeout = step.timeout || this.config.defaultTimeout;
    const retries = step.retries || 0;
    
    let lastError: Error;
    
    for (let attempt = 0; attempt <= retries; attempt++) {
      try {
        const result = await Promise.race([
          step.action!(),
          this.createTimeout(timeout)
        ]);
        
        stepResult.output = result;
        return;
      } catch (error) {
        lastError = error as Error;
        
        if (attempt < retries) {
          this.logger.warn(`Step ${step.name} failed, retry ${attempt + 1}/${retries}`);
          await this.delay(this.calculateBackoff(attempt));
        }
      }
    }
    
    throw lastError!;
  }
  
  private async executeParallel(
    steps: WorkflowStep[],
    result: WorkflowResult,
    parentPath: string
  ): Promise<void> {
    await Promise.all(
      steps.map(step => this.executeStep(step, result, parentPath))
    );
  }
  
  private async executeSequential(
    steps: WorkflowStep[],
    result: WorkflowResult,
    parentPath: string
  ): Promise<void> {
    for (const step of steps) {
      await this.executeStep(step, result, parentPath);
    }
  }
  
  private createTimeout(ms: number): Promise<never> {
    return new Promise((_, reject) => {
      setTimeout(() => reject(new Error(`Timeout after ${ms}ms`)), ms);
    });
  }
  
  private calculateBackoff(attempt: number): number {
    return Math.min(1000 * Math.pow(2, attempt), 30000);
  }
  
  private delay(ms: number): Promise<void> {
    return new Promise(resolve => setTimeout(resolve, ms));
  }
}

// Example workflow definition
export const deploymentWorkflow: WorkflowStep = {
  name: 'deployment',
  type: 'sequential',
  steps: [
    {
      name: 'pre-deployment',
      type: 'parallel',
      steps: [
        {
          name: 'backup-database',
          action: async () => {
            // Backup database
          },
          timeout: 300000 // 5 minutes
        },
        {
          name: 'health-check',
          action: async () => {
            // Check system health
          },
          retries: 3
        }
      ]
    },
    {
      name: 'deployment',
      type: 'sequential',
      steps: [
        {
          name: 'blue-green-switch',
          action: async () => {
            // Switch traffic to new version
          },
          onError: 'retry',
          retries: 2
        },
        {
          name: 'smoke-tests',
          action: async () => {
            // Run smoke tests
          },
          onError: 'fail'
        }
      ]
    },
    {
      name: 'post-deployment',
      type: 'parallel',
      steps: [
        {
          name: 'notify-teams',
          action: async () => {
            // Send notifications
          },
          onError: 'continue'
        },
        {
          name: 'update-monitoring',
          action: async () => {
            // Update monitoring dashboards
          }
        }
      ]
    }
  ]
};
```

## Output Format

1. **Workflow Analysis**: Current processes and automation opportunities
2. **CI/CD Pipeline**: Complete GitHub Actions/GitLab CI configuration
3. **Release Automation**: Semantic versioning and release workflows
4. **Development Automation**: Pre-commit hooks and setup scripts
5. **Infrastructure Automation**: Terraform and Kubernetes workflows
6. **Security Automation**: Scanning and compliance workflows
7. **Documentation Generation**: Automated docs and diagrams
8. **Workflow Orchestration**: Complex workflow management
9. **Monitoring Integration**: Automated alerts and dashboards
10. **Implementation Guide**: Step-by-step setup instructions

Focus on creating reliable, maintainable automation that reduces manual work while maintaining quality and security standards.
````

## File: .gitignore

````
# Folder view configuration files
.DS_Store
Desktop.ini

# Thumbnail cache files
._*
Thumbs.db

# Files that might appear on external disks
.Spotlight-V100
.Trashes

# Compiled Python files
*.pyc

# Compiled C++ files
*.out

# Application specific files
venv
node_modules
.sass-cache
````

## File: LICENSE

````
MIT License

Copyright (c) 2024

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
````

## File: README.md

````markdown
<div align="right">
  <details>
    <summary >üåê Language</summary>
    <div>
      <div align="center">
        <a href="https://openaitx.github.io/view.html?user=wshobson&project=commands&lang=en">English</a>
        | <a href="https://openaitx.github.io/view.html?user=wshobson&project=commands&lang=zh-CN">ÁÆÄ‰Ωì‰∏≠Êñá</a>
        | <a href="https://openaitx.github.io/view.html?user=wshobson&project=commands&lang=zh-TW">ÁπÅÈ´î‰∏≠Êñá</a>
        | <a href="https://openaitx.github.io/view.html?user=wshobson&project=commands&lang=ja">Êó•Êú¨Ë™û</a>
        | <a href="https://openaitx.github.io/view.html?user=wshobson&project=commands&lang=ko">ÌïúÍµ≠Ïñ¥</a>
        | <a href="https://openaitx.github.io/view.html?user=wshobson&project=commands&lang=hi">‡§π‡§ø‡§®‡•ç‡§¶‡•Ä</a>
        | <a href="https://openaitx.github.io/view.html?user=wshobson&project=commands&lang=th">‡πÑ‡∏ó‡∏¢</a>
        | <a href="https://openaitx.github.io/view.html?user=wshobson&project=commands&lang=fr">Fran√ßais</a>
        | <a href="https://openaitx.github.io/view.html?user=wshobson&project=commands&lang=de">Deutsch</a>
        | <a href="https://openaitx.github.io/view.html?user=wshobson&project=commands&lang=es">Espa√±ol</a>
        | <a href="https://openaitx.github.io/view.html?user=wshobson&project=commands&lang=it">Italiano</a>
        | <a href="https://openaitx.github.io/view.html?user=wshobson&project=commands&lang=ru">–†—É—Å—Å–∫–∏–π</a>
        | <a href="https://openaitx.github.io/view.html?user=wshobson&project=commands&lang=pt">Portugu√™s</a>
        | <a href="https://openaitx.github.io/view.html?user=wshobson&project=commands&lang=nl">Nederlands</a>
        | <a href="https://openaitx.github.io/view.html?user=wshobson&project=commands&lang=pl">Polski</a>
        | <a href="https://openaitx.github.io/view.html?user=wshobson&project=commands&lang=ar">ÿßŸÑÿπÿ±ÿ®Ÿäÿ©</a>
        | <a href="https://openaitx.github.io/view.html?user=wshobson&project=commands&lang=fa">ŸÅÿßÿ±ÿ≥€å</a>
        | <a href="https://openaitx.github.io/view.html?user=wshobson&project=commands&lang=tr">T√ºrk√ße</a>
        | <a href="https://openaitx.github.io/view.html?user=wshobson&project=commands&lang=vi">Ti·∫øng Vi·ªát</a>
        | <a href="https://openaitx.github.io/view.html?user=wshobson&project=commands&lang=id">Bahasa Indonesia</a>
        | <a href="https://openaitx.github.io/view.html?user=wshobson&project=commands&lang=as">‡¶Ö‡¶∏‡¶Æ‡ßÄ‡¶Ø‡¶º‡¶æ</
      </div>
    </div>
  </details>
</div>

# Claude Code Slash Commands

Production-ready slash commands for [Claude Code](https://docs.anthropic.com/en/docs/claude-code) that accelerate development through intelligent automation.

**52 commands** organized as:
- **ü§ñ Workflows**: Multi-subagent orchestration for complex tasks
- **üîß Tools**: Single-purpose utilities for specific operations

### ü§ù Requires Claude Code Subagents

These commands work with the [Claude Code Subagents](https://github.com/wshobson/agents) for orchestration capabilities.

## Installation

```bash
cd ~/.claude
git clone https://github.com/wshobson/commands.git
git clone https://github.com/wshobson/agents.git  # Required for subagent orchestration
```

## Available Commands

- **ü§ñ Workflows** - Orchestrate multiple subagents for complex tasks
- **üîß Tools** - Single-purpose commands for specific operations

## Usage

```bash
/api-scaffold user management with authentication
/security-scan check for vulnerabilities
/feature-development implement chat functionality
```

Claude Code automatically suggests relevant commands based on context.

## ü§ñ Workflows

Multi-subagent orchestration for complex tasks:

### Feature Development
- **[feature-development](workflows/feature-development.md)** - Backend, frontend, testing, and deployment subagents build complete features
- **[full-review](workflows/full-review.md)** - Multiple review subagents provide comprehensive code analysis
- **[smart-fix](workflows/smart-fix.md)** - Analyzes issues and delegates to appropriate specialist subagents

### Development Processes
- **[git-workflow](workflows/git-workflow.md)** - Implements effective Git workflows with branching strategies and PR templates
- **[improve-agent](workflows/improve-agent.md)** - Enhances subagent performance through prompt optimization
- **[legacy-modernize](workflows/legacy-modernize.md)** - Modernizes legacy codebases using specialized subagents
- **[ml-pipeline](workflows/ml-pipeline.md)** - Creates ML pipelines with data and ML engineering subagents
- **[multi-platform](workflows/multi-platform.md)** - Builds cross-platform apps with coordinated subagents
- **[workflow-automate](workflows/workflow-automate.md)** - Automates CI/CD, monitoring, and deployment workflows

### Subagent-Orchestrated Workflows
- **[full-stack-feature](workflows/full-stack-feature.md)** - Multi-platform features with backend, frontend, and mobile subagents
- **[security-hardening](workflows/security-hardening.md)** - Security-first implementation with specialized subagents
- **[data-driven-feature](workflows/data-driven-feature.md)** - ML-powered features with data science subagents
- **[performance-optimization](workflows/performance-optimization.md)** - End-to-end optimization with performance subagents
- **[incident-response](workflows/incident-response.md)** - Production incident resolution with ops subagents

## üîß Tools (Single-Purpose Commands)

### AI & Machine Learning
- **[ai-assistant](tools/ai-assistant.md)** - Build production-ready AI assistants and chatbots
- **[ai-review](tools/ai-review.md)** - Specialized review for AI/ML codebases
- **[langchain-agent](tools/langchain-agent.md)** - Create LangChain/LangGraph agents with modern patterns
- **[ml-pipeline](tools/ml-pipeline.md)** - Create end-to-end ML pipelines with MLOps
- **[prompt-optimize](tools/prompt-optimize.md)** - Optimize AI prompts for performance and quality

### Architecture & Code Quality
- **[code-explain](tools/code-explain.md)** - Generate detailed explanations of complex code
- **[code-migrate](tools/code-migrate.md)** - Migrate code between languages, frameworks, or versions
- **[refactor-clean](tools/refactor-clean.md)** - Refactor code for maintainability and performance
- **[tech-debt](tools/tech-debt.md)** - Analyze and prioritize technical debt

### Data & Database
- **[data-pipeline](tools/data-pipeline.md)** - Design scalable data pipeline architectures
- **[data-validation](tools/data-validation.md)** - Implement comprehensive data validation systems
- **[db-migrate](tools/db-migrate.md)** - Advanced database migration strategies

### DevOps & Infrastructure
- **[deploy-checklist](tools/deploy-checklist.md)** - Generate deployment configurations and checklists
- **[docker-optimize](tools/docker-optimize.md)** - Advanced container optimization strategies
- **[k8s-manifest](tools/k8s-manifest.md)** - Production-grade Kubernetes deployments
- **[monitor-setup](tools/monitor-setup.md)** - Set up comprehensive monitoring and observability
- **[slo-implement](tools/slo-implement.md)** - Implement Service Level Objectives (SLOs)
- **[workflow-automate](tools/workflow-automate.md)** - Automate development and operational workflows

### Development & Testing
- **[api-mock](tools/api-mock.md)** - Create realistic API mocks for development and testing
- **[api-scaffold](tools/api-scaffold.md)** - Generate production-ready API endpoints with complete implementation stack
- **[test-harness](tools/test-harness.md)** - Create comprehensive test suites with framework detection

### Security & Compliance
- **[accessibility-audit](tools/accessibility-audit.md)** - Comprehensive accessibility testing and fixes
- **[compliance-check](tools/compliance-check.md)** - Ensure regulatory compliance (GDPR, HIPAA, SOC2)
- **[security-scan](tools/security-scan.md)** - Comprehensive security scanning with automated remediation

### Debugging & Analysis
- **[debug-trace](tools/debug-trace.md)** - Advanced debugging and tracing strategies
- **[error-analysis](tools/error-analysis.md)** - Deep error pattern analysis and resolution strategies
- **[error-trace](tools/error-trace.md)** - Trace and diagnose production errors
- **[issue](tools/issue.md)** - Create well-structured GitHub/GitLab issues

### Dependencies & Configuration
- **[config-validate](tools/config-validate.md)** - Validate and manage application configuration
- **[deps-audit](tools/deps-audit.md)** - Audit dependencies for security and licensing
- **[deps-upgrade](tools/deps-upgrade.md)** - Safely upgrade project dependencies

### Documentation & Collaboration
- **[doc-generate](tools/doc-generate.md)** - Generate comprehensive documentation
- **[git-workflow](tools/git-workflow.md)** - Implement effective Git workflows
- **[pr-enhance](tools/pr-enhance.md)** - Enhance pull requests with quality checks

### Cost Optimization
- **[cost-optimize](tools/cost-optimize.md)** - Optimize cloud and infrastructure costs

### Onboarding & Setup
- **[onboard](tools/onboard.md)** - Set up development environments for new team members

### Subagent Tools
- **[multi-agent-review](tools/multi-agent-review.md)** - Multi-perspective code review with specialized subagents
- **[smart-debug](tools/smart-debug.md)** - Deep debugging with debugger and performance subagents
- **[multi-agent-optimize](tools/multi-agent-optimize.md)** - Full-stack optimization with multiple subagents
- **[context-save](tools/context-save.md)** - Save project context using context-manager subagent
- **[context-restore](tools/context-restore.md)** - Restore saved context for continuity

## Features

- Production-ready implementations
- Framework auto-detection
- Security best practices
- Built-in monitoring and testing
- Commands work together seamlessly

## Command Count

**Total: 52 production-ready slash commands** organized into:

### ü§ñ Workflows (14 commands)

- Feature Development & Review (3 commands) 
- Development Process Automation (6 commands)
- Subagent-Orchestrated Workflows (5 commands)

### üîß Tools (38 commands)

- AI & Machine Learning (5 commands)
- Architecture & Code Quality (4 commands)
- Data & Database (3 commands)
- DevOps & Infrastructure (6 commands)
- Development & Testing (3 commands)
- Security & Compliance (3 commands)
- Debugging & Analysis (4 commands)
- Dependencies & Configuration (3 commands)
- Documentation & Collaboration (1 command)
- Onboarding & Setup (1 command)
- Subagent-Specific Tools (5 commands)

## Usage Examples

### ü§ñ Workflow Examples

```bash
# Implement a complete feature
/feature-development Add user authentication with OAuth2

# Comprehensive code review
/full-review Review the authentication module

# Smart issue resolution
/smart-fix Fix performance degradation in API response times

# Modernize legacy system
/legacy-modernize Migrate monolithic Java app to microservices

# Build comprehensive multi-platform feature
/full-stack-feature User authentication with social login across web and mobile

# Implement security-first architecture
/security-hardening Harden API endpoints and implement zero-trust security model

# Create data-driven ML feature
/data-driven-feature Build recommendation engine with real-time personalization

# Optimize entire application stack
/performance-optimization Improve response times and reduce infrastructure costs

# Respond to production incident
/incident-response High CPU usage causing service degradation in production
```

### üîß Tool Examples (Single-Purpose Commands)

```bash
# Create a user management API
/api-scaffold user CRUD operations with JWT auth and role-based access

# Review microservices architecture
/multi-agent-review analyze our microservices for coupling and scalability issues

# Optimize LLM chat application
/prompt-optimize reduce latency for customer support chatbot while maintaining accuracy

# Create fraud detection pipeline
/data-pipeline real-time fraud detection with feature store and monitoring

# Debug production issue
/error-trace analyze high memory usage in production pods

# Secure container images
/security-scan scan and fix vulnerabilities in Docker images

# Generate API documentation
/doc-generate create OpenAPI docs with examples for REST endpoints

# Onboard new developer
/onboard Setup development environment for React/Node.js project

# Multi-perspective code review
/multi-agent-review Review authentication module

# Deep debugging
/smart-debug Investigate memory leak in production workers

# Full-stack optimization
/multi-agent-optimize Optimize checkout flow for better conversion

# Save project context
/context-save Save current project state and architectural decisions

# Restore project context
/context-restore Load context from last week's sprint
```

## Enhanced Commands

### Security & DevOps

#### [`/security-scan`](tools/security-scan.md)

Comprehensive security scanning with automated remediation.

- **Multi-Tool Scanning**: Bandit, Safety, Trivy, Semgrep, ESLint Security, Snyk
- **Automated Fixes**: Common vulnerabilities auto-remediated
- **CI/CD Integration**: Security gates for GitHub Actions/GitLab CI
- **Container Scanning**: Image vulnerability analysis
- **Secret Detection**: GitLeaks and TruffleHog integration

#### [`/docker-optimize`](tools/docker-optimize.md)

Advanced container optimization strategies.

- **Smart Optimization**: Analyzes and suggests improvements
- **Multi-Stage Builds**: Framework-specific optimized Dockerfiles
- **Modern Tools**: BuildKit, Bun, UV for faster builds
- **Security Hardening**: Distroless images, non-root users
- **Cross-Command Integration**: Works with /api-scaffold outputs

#### [`/k8s-manifest`](tools/k8s-manifest.md)

Production-grade Kubernetes deployments.

- **Advanced Patterns**: Pod Security Standards, Network Policies, OPA
- **GitOps Ready**: FluxCD and ArgoCD configurations
- **Observability**: Prometheus ServiceMonitors, OpenTelemetry
- **Auto-Scaling**: HPA, VPA, and cluster autoscaler configs
- **Service Mesh**: Istio/Linkerd integration

### Frontend & Data

#### [`/db-migrate`](tools/db-migrate.md)

Advanced database migration strategies.

- **Multi-Database**: PostgreSQL, MySQL, MongoDB, DynamoDB
- **Zero-Downtime**: Blue-green deployments, rolling migrations
- **Event Sourcing**: Kafka/Kinesis integration for CDC
- **Cross-Platform**: Handles polyglot persistence
- **Monitoring**: Migration health checks and rollback

## Combining Workflows and Tools

The real power comes from combining workflows and tools for complete development cycles:

### Example: Building a New Feature

```bash
# 1. Use a workflow to implement the feature with multiple subagents
/feature-development Add real-time chat feature with WebSocket support

# 2. Use tools for specific enhancements
/test-harness Add integration tests for WebSocket connections
/security-scan Check for WebSocket vulnerabilities
/docker-optimize Optimize container for WebSocket connections

# 3. Use a workflow for comprehensive review
/full-review Review the entire chat feature implementation
```

### Example: Modernizing Legacy Code

```bash
# 1. Start with the modernization workflow
/legacy-modernize Migrate Express.js v4 app to modern architecture

# 2. Use specific tools for cleanup
/deps-upgrade Update all dependencies to latest versions
/refactor-clean Remove deprecated patterns and dead code
/test-harness Ensure 100% test coverage

# 3. Optimize and deploy
/docker-optimize Create multi-stage production build
/k8s-manifest Deploy with zero-downtime strategy
```

## Command Orchestration Patterns

Commands can be used individually or combined in powerful patterns:

### Sequential Execution
```bash
# Build ‚Üí Test ‚Üí Secure ‚Üí Deploy pipeline
/api-scaffold user management API
/test-harness comprehensive test suite for user API  
/security-scan check user API for vulnerabilities
/k8s-manifest deploy user API to production
```

### Parallel Analysis
```bash
# Multiple perspectives on the same codebase
/multi-agent-review comprehensive architecture and code review
/security-scan audit security posture  
/performance-optimization identify and fix bottlenecks
# Then consolidate findings
```

### Iterative Refinement
```bash
# Start broad, then narrow focus
/feature-development implement payment processing
/security-scan focus on payment security
/compliance-check ensure PCI compliance
/test-harness add payment-specific tests
```

### Cross-Domain Integration
```bash
# Frontend + Backend + Infrastructure
/api-scaffold backend payment API
/multi-agent-optimize optimize payment flow performance
/docker-optimize containerize payment service
/monitor-setup payment metrics and alerts
```

## When to Use Workflows vs Tools

### üîÄ Workflows & Subagent Tools
- **Problem-solving**: Analyze and fix issues adaptively
- **Multiple perspectives**: Coordinate specialist subagents
- **Complex tasks**: Multi-step processes across domains
- **Unknown solutions**: Let subagents determine approach

### üõ†Ô∏è Specialized Tools
- **Infrastructure setup**: Configure environments
- **Code generation**: Create specific implementations
- **Analysis**: Generate reports without fixes
- **Domain tasks**: Highly specialized operations

**Examples:**
- "Implement user authentication system" ‚Üí `/feature-development`
- "Fix performance issues across the stack" ‚Üí `/smart-fix`
- "Modernize legacy monolith" ‚Üí `/legacy-modernize`

### üîß Use Tools When:
- **Specific expertise needed** - Clear, focused task in one domain
- **Precise control desired** - Want to direct specific implementation details
- **Part of manual workflow** - Integrating into existing processes
- **Deep specialization required** - Need expert-level implementation
- **Building on existing work** - Enhancing or refining previous outputs

**Examples:**
- "Create Kubernetes manifests" ‚Üí `/k8s-manifest`
- "Scan for security vulnerabilities" ‚Üí `/security-scan`
- "Generate API documentation" ‚Üí `/doc-generate`

## Command Format

Slash commands are simple markdown files where:
- The filename becomes the command name (e.g., `api-scaffold.md` ‚Üí `/api-scaffold`)
- The file content is the prompt/instructions executed when invoked
- Use `$ARGUMENTS` placeholder for user input

## Best Practices

### Command Selection
- **Let Claude Code suggest automatically** - Analyzes context and selects optimal commands
- **Use workflows for complex tasks** - Subagents coordinate multi-domain implementations
- **Use tools for focused tasks** - Apply specific commands for targeted improvements

### Effective Usage
- **Provide comprehensive context** - Include tech stack, constraints, and requirements
- **Chain commands strategically** - Workflows ‚Üí Tools ‚Üí Refinements
- **Build on previous outputs** - Commands are designed to work together

## Contributing

1. Create `.md` file in `workflows/` or `tools/`
2. Use lowercase-hyphen-names
3. Include `$ARGUMENTS` for user input

## Troubleshooting

**Command not found**: Check files are in `~/.claude/commands/`

**Workflows slow**: Normal - they coordinate multiple subagents

**Generic output**: Add more specific context about your tech stack

**Integration issues**: Verify file paths and command sequence

## Performance Tips

**Command Selection:**
- **Workflows**: Multi-subagent coordination for complex features
- **Tools**: Single-purpose operations for specific tasks
- **Simple edits**: Stay with main agent

**Optimization:**
- Start with tools for known problems
- Provide detailed requirements upfront
- Build on previous command outputs
- Let workflows complete before modifications

### Adding a New Workflow:
- Focus on subagent orchestration and delegation logic
- Specify which specialized subagents to use and in what order
- Define coordination patterns between subagents

### Adding a New Tool:
- Include complete, production-ready implementations
- Structure content with clear sections and actionable outputs
- Include examples, best practices, and integration points

## Learn More

- [Claude Code Documentation](https://docs.anthropic.com/en/docs/claude-code)
- [Slash Commands Documentation](https://docs.anthropic.com/en/docs/claude-code/slash-commands)
- [Subagents Documentation](https://docs.anthropic.com/en/docs/claude-code/sub-agents)
- [Claude Code GitHub](https://github.com/anthropics/claude-code)
- [Claude Code Subagents Collection](https://github.com/wshobson/agents) - Specialized subagents used by these commands
````
